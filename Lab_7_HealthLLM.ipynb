{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPFxW8VHfyCckdla0rmziy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/Lab_7_HealthLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü©∫ Introduction to Health-LLM  \n",
        "\n",
        "## üìå Overview  \n",
        "**Health-LLM** is a specialized large language model designed to **predict and analyze health measures** using **wearable sensor data** and **user demographics**. Developed by **MIT Media Lab**, this model enhances traditional LLMs by integrating physiological signals such as **heart rate, sleep patterns, activity levels, and stress indicators** to provide **personalized health assessments and insights**.  \n",
        "\n",
        "üîó **GitHub Repository**: [Health-LLM on GitHub](https://github.com/mitmedialab/Health-LLM/tree/main)  \n",
        "üìÑ **Research Paper**: [Health-LLM Paper (PDF)](https://github.com/mitmedialab/Health-LLM/blob/main/pdf/paper.pdf)  \n",
        "\n",
        "---\n",
        "\n",
        "## üõ† How Was Health-LLM Trained?  \n",
        "\n",
        "Health-LLM was trained using a **fine-tuning approach** on multiple **public health datasets** to specialize in **consumer health prediction tasks**. The training process involved adapting **general-purpose LLMs** to better interpret **physiological and behavioral data**, improving their ability to generate **health insights and risk assessments**.  \n",
        "\n",
        "### **1Ô∏è‚É£ Base Models Used**\n",
        "Health-LLM fine-tunes existing large language models, including:  \n",
        "- **GPT-3.5 / GPT-4** (OpenAI)  \n",
        "- **Gemini-Pro** (Google)  \n",
        "- **MedAlpaca** (Stanford Medicine)  \n",
        "\n",
        "These models were **not originally trained for health predictions**, so Health-LLM **fine-tunes them on specialized medical datasets** to improve their performance in **health-related tasks**.  \n",
        "\n",
        "### **2Ô∏è‚É£ Training Data**  \n",
        "Health-LLM was trained on **four major health datasets**, each containing **physiological and behavioral data** from real-world users:  \n",
        "\n",
        "| **Dataset** | **Tasks Covered** |\n",
        "|------------|------------------|\n",
        "| **PMData** | Fatigue, Stress, Readiness, Sleep Quality |\n",
        "| **LifeSnaps** | Stress Resilience, Sleep Disorder |\n",
        "| **GLOBEM** | Anxiety, Depression |\n",
        "| **AW FB** | Calories, Activity |\n",
        "\n",
        "These datasets allow the model to **learn human health patterns** and make **more accurate, personalized health predictions**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How to Use Health-LLM  \n",
        "\n",
        "Unlike models hosted on **Hugging Face** or accessible via an **API**, **Health-LLM must be run locally**. To use it:  \n",
        "\n",
        "- **Clone the GitHub repository** and install the required dependencies.  \n",
        "- **Run the inference script locally** to input health-related queries.  \n",
        "- **Ensure you have sufficient GPU memory** for efficient execution, or modify the settings for CPU compatibility.  \n",
        "\n",
        "Health-LLM is designed for **customizable, offline health predictions**, making it ideal for **privacy-focused and research applications**.  \n",
        "\n"
      ],
      "metadata": {
        "id": "jp9OmVW-NJZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Run This Notebook on a GPU!**  \n",
        "\n",
        "Health-LLM **requires a GPU** for efficient execution. Running it on a CPU will be **slow** and may cause **memory issues**.  \n",
        "\n",
        "### **üîß Enable GPU in Google Colab (One-Step Guide)**  \n",
        "üîπ **Runtime** ‚Üí **Change runtime type** ‚Üí **Set Hardware accelerator to GPU** ‚Üí **Save** ‚Üí **Restart runtime** ‚úÖ  \n",
        "\n",
        "To check if GPU is active, run:  \n",
        "```python\n",
        "import torch\n",
        "print(\"üöÄ Device:\", \"CUDA\" if torch.cuda.is_available() else \"CPU\")\n"
      ],
      "metadata": {
        "id": "wHzKkFIWPB8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üè• Install Required Libraries\n",
        "\n",
        "# ‚úÖ Install the necessary libraries\n",
        "# This installs Hugging Face's `transformers` for NLP models and `torch` for running deep learning models.\n",
        "!pip install -q transformers torch\n",
        "\n",
        "# ‚úÖ Verify installation\n",
        "import torch  # PyTorch - Required for handling tensors and running models on GPU\n",
        "import transformers  # Hugging Face Transformers library for loading pre-trained models\n",
        "\n",
        "print(\"‚úÖ Transformers and Torch installed successfully!\")\n"
      ],
      "metadata": {
        "id": "tb1lkciZRxzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üè• Clone Health-LLM Repository & Install Dependencies\n",
        "\n",
        "# ‚úÖ Clone the Health-LLM repository from GitHub\n",
        "# This downloads the entire project folder, including model files and scripts.\n",
        "!git clone https://github.com/mitmedialab/Health-LLM.git\n",
        "\n",
        "# ‚úÖ Navigate into the cloned repository\n",
        "# Moves into the Health-LLM directory so that all commands run within the project.\n",
        "%cd Health-LLM\n",
        "\n",
        "# ‚úÖ Install required dependencies\n",
        "# Reads the `requirements.txt` file and installs all necessary Python libraries.\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "ItC9aXUdNlUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **‚ö° Loading Health-LLM: Running an LLM Locally vs. via API**  \n",
        "\n",
        "Health-LLM fine-tunes **MedAlpaca-7B**, a medical-focused language model built on Meta‚Äôs LLaMA. It is optimized for **clinical reasoning and health-related predictions**, making it well-suited for wearable health monitoring.  \n",
        "\n",
        "The following setup loads the **MedAlpaca model and tokenizer**, ensuring the correct fine-tuned version is used for inference. It also enables **efficient execution** by leveraging FP16 precision and automatic GPU allocation.  \n",
        "\n",
        "---\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.licdn.com/dms/image/D5612AQEERi6rjuXWwA/article-cover_image-shrink_720_1280/0/1716064697000?e=2147483647&v=beta&t=0mDj7CtX0ZfXJ2LTs48ly1F2_IsX4zaK6GuzExadWEI\"\n",
        "  alt=\"Local LLM vs API LLM\" width=\"600\">\n",
        "</p>  \n",
        "The image illustrates how a **local LLM** operates without relying on an external API. The system first retrieves data from a **local vector database**, ensuring all necessary context is available for processing. Then, the query is passed to the **local LLM**, where it is processed directly on the user's device. Unlike API-based models, the response is **generated entirely on the local machine**, eliminating the need for external requests. This method results in **faster inference times** and ensures **data privacy**, as no sensitive information is transmitted to external servers.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîç Local LLM vs. API-based LLM: Key Differences**  \n",
        "\n",
        "| Feature          | **Local LLM (Health-LLM)** | **API-based LLM (e.g., GPT-4, Claude)** |\n",
        "|-----------------|--------------------------|----------------------------------|\n",
        "| **Execution**   | Runs on the local machine | Runs on a cloud server |\n",
        "| **Speed**       | Faster (no API calls) but hardware-dependent | Slightly slower due to network requests |\n",
        "| **Privacy**     | Data stays on the device | Data is sent to external servers |\n",
        "| **Cost**        | Free after downloading (requires GPU) | Pay per API call |\n",
        "| **Flexibility** | Can be fine-tuned/customized | Limited customization |\n",
        "| **Setup**       | Requires downloading & configuring the model | Simple API call with a key |\n",
        "\n",
        "---\n",
        "\n",
        "üîπ Unlike API-based LLMs (e.g., OpenAI‚Äôs GPT models), Health-LLM runs locally, meaning we don‚Äôt need an internet connection to send API requests.\n",
        "\n",
        "üîπ This setup gives full control over the model and avoids API costs but requires more hardware resources (GPU/CPU). AO, instead of calling a remote API, we directly download and execute the model here.  \n"
      ],
      "metadata": {
        "id": "Eu6nujTYRK-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ö° Load Health-LLM (Based on MedAlpaca)\n",
        "\n",
        "# ‚úÖ Import Hugging Face utilities for model & tokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ‚úÖ Define the model name (ensures we use the correct fine-tuned version)\n",
        "model_name = \"medalpaca/medalpaca-7b\"\n",
        "\n",
        "# ‚úÖ Load the tokenizer (converts text into token IDs for model processing)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ‚úÖ Load the causal language model (used for generating text responses)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Uses FP16 precision for efficient execution on GPU\n",
        "    device_map=\"auto\"  # Automatically assigns model to GPU if available\n",
        ")\n",
        "\n",
        "# üîπ Since we are using GPU, the model will run on GPU if detected, improving speed and efficiency.\n",
        "# üîπ If no GPU is available, it will fall back to CPU, but execution may be slower.\n"
      ],
      "metadata": {
        "id": "yfrizx0FQg59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üè• Define the Inference Function for Health-LLM\n",
        "\n",
        "def generate_response(prompt, max_length=200):\n",
        "    \"\"\"\n",
        "    Generates a response from the Health-LLM model based on a given prompt.\n",
        "\n",
        "    Parameters:\n",
        "    - prompt (str): The input question or statement for the model.\n",
        "    - max_length (int): The maximum number of tokens the model should generate.\n",
        "\n",
        "    Returns:\n",
        "    - response (str): The generated text response.\n",
        "    \"\"\"\n",
        "\n",
        "    # ‚úÖ Convert the input text into tokenized format and move it to GPU if available\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ‚úÖ Generate the model's response with sampling enabled\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,  # Pass tokenized input IDs to the model\n",
        "        max_length=max_length,  # Limit response length\n",
        "        do_sample=True  # Enable randomness in text generation\n",
        "    )\n",
        "\n",
        "    # ‚úÖ Decode the generated tokens back into a readable text response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response  # Return the final response\n"
      ],
      "metadata": {
        "id": "0GHAbcDnSlO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage of Health-LLM Inference\n",
        "\n",
        "# Define a personalized test prompt\n",
        "prompt = \"I am a 25-year-old female experiencing frequent dizziness, fatigue, and low blood pressure (90/60 mmHg). What could be the possible causes, and how can I manage this condition?\"\n",
        "\n",
        "# Generate response using the model\n",
        "response = generate_response(prompt)\n",
        "\n",
        "# Print formatted output\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "id": "rI4FsmT0VQsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # Import pandas for table formatting\n",
        "from IPython.display import display  # Import display for showing tables in Colab\n",
        "\n",
        "# üè• Store refined prompts and responses in a list\n",
        "data = []\n",
        "\n",
        "# üèÉ‚Äç‚ôÇÔ∏è Fitness & Health Optimization\n",
        "prompt = \"I am a 35-year-old male with a resting heart rate of 78 bpm. I walk 5,000 steps daily and sleep for 6 hours. My diet consists mainly of processed foods. What are the best strategies to improve my fitness and reduce stress?\"\n",
        "response = generate_response(prompt)\n",
        "data.append([\"üèÉ‚Äç‚ôÇÔ∏è Fitness & Stress\", prompt, response])\n",
        "\n",
        "# ü•ó Nutrition & Weight Management\n",
        "prompt = \"I am a 45-year-old female. My weight is 85 kg, and my height is 165 cm. I have a sedentary lifestyle and consume a high-carb diet. How can I improve my nutrition and achieve a healthier BMI?\"\n",
        "response = generate_response(prompt)\n",
        "data.append([\"ü•ó Nutrition & Weight\", prompt, response])\n",
        "\n",
        "# üò¥ Sleep & Recovery\n",
        "prompt = \"I sleep only 5 hours per night and wake up feeling exhausted. I often drink coffee late at night due to work stress. How can I improve my sleep quality and energy levels throughout the day?\"\n",
        "response = generate_response(prompt)\n",
        "data.append([\"üò¥ Sleep & Energy\", prompt, response])\n",
        "\n",
        "# üìä Convert to DataFrame and Display\n",
        "df = pd.DataFrame(data, columns=[\"Category\", \"Prompt\", \"Response\"])\n",
        "\n",
        "# ‚úÖ Print the table in Colab\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "7fx3YZdxSa_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "96FMYPkwV68z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Install required Hugging Face, LangChain & Wikipedia libraries\n",
        "\n",
        "!pip install -q \\\n",
        "    datasets tokenizers accelerate huggingface_hub \\\n",
        "    langchain langchain-openai langchain-community \\\n",
        "    wikipedia-api wikipedia\n",
        "\n",
        "# üîπ datasets: Provides access to large datasets for training and evaluation\n",
        "# üîπ tokenizers: Efficiently tokenizes text for LLM processing\n",
        "# üîπ accelerate: Optimizes model execution across CPU, GPU, and multi-GPU setups\n",
        "# üîπ huggingface_hub: Enables downloading and managing models from Hugging Face\n",
        "# üîπ langchain: Core framework for integrating LLMs with memory, tools, and agents\n",
        "# üîπ langchain-openai: Official LangChain module for OpenAI models (GPT-4, ChatGPT)\n",
        "# üîπ langchain-community: Community-driven utilities for LangChain\n",
        "# üîπ wikipedia-api: Allows Wikipedia search integration for external knowledge retrieval\n",
        "# üîπ wikipedia: Provides direct API access to Wikipedia for fetching factual information\n",
        "\n",
        "# üîπ Ensure latest version of langchain-openai for OpenAI API integration\n",
        "!pip install -U langchain-openai\n"
      ],
      "metadata": {
        "id": "YE6i6FLrWkoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì• Import necessary libraries for LangChain, Hugging Face & Wikipedia integration\n",
        "\n",
        "import os\n",
        "import pandas as pd  # üîπ For structured table formatting\n",
        "from IPython.display import display  # üîπ Enables table display in Colab\n",
        "\n",
        "# üîπ Hugging Face & Model Execution\n",
        "import datasets  # üîπ Provides access to large datasets for LLM training & evaluation\n",
        "import accelerate  # üîπ Speeds up execution on CPU, GPU, or multi-GPU\n",
        "import huggingface_hub  # üîπ Manages model downloads & integration from Hugging Face\n",
        "\n",
        "# üîπ LangChain - LLM Framework\n",
        "import langchain\n",
        "from langchain_openai import ChatOpenAI  # üîπ Correct import for OpenAI chat models\n",
        "from langchain.agents import initialize_agent, AgentType  # üîπ Enables agent-based interactions\n",
        "from langchain.tools import Tool  # üîπ Defines external tools for agent-based workflows\n",
        "from langchain.memory import ConversationBufferMemory  # üîπ Enables memory for multi-turn interactions\n",
        "from langchain_community.utilities import WikipediaAPIWrapper  # üîπ Allows Wikipedia search integration\n",
        "from langchain.prompts import PromptTemplate  # üîπ Helps in structuring prompts for better responses\n",
        "\n",
        "# ‚úÖ Verify that all libraries are installed and imported correctly\n",
        "print(\"‚úÖ All required libraries installed and imported successfully!\")\n"
      ],
      "metadata": {
        "id": "WHu9WbCeAiUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Set OpenAI API Key from Colab Secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OpenAI_Key\")  # Ensures LangChain uses the correct key\n",
        "\n",
        "# ‚úÖ Load GPT-4 (as a chat model)\n",
        "gpt4_llm = ChatOpenAI(\n",
        "    model=\"gpt-4\",\n",
        "    temperature=0.7,\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")  # Ensures correct API key usage\n",
        ")\n",
        "\n",
        "# ‚úÖ Define conversation memory to store chat history\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# ‚úÖ Define a structured prompt template with chat history\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"query\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI assistant providing accurate answers.\n",
        "    Maintain continuity by considering past conversation history.\n",
        "\n",
        "    Chat History:\n",
        "    {chat_history}\n",
        "\n",
        "    User Question:\n",
        "    {query}\n",
        "\n",
        "    Provide a clear and well-structured response.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Modify GPT-4 tool to use the prompt template with memory\n",
        "def gpt4_with_memory(query):\n",
        "    \"\"\"Formats the query with chat history before sending to GPT-4.\"\"\"\n",
        "    formatted_query = prompt_template.format(\n",
        "        chat_history=memory.load_memory_variables({})[\"chat_history\"], query=query\n",
        "    )\n",
        "    return gpt4_llm(formatted_query)\n",
        "\n",
        "# ‚úÖ Define GPT-4 as a tool\n",
        "gpt4_tool = Tool(\n",
        "    name=\"ChatGPT\",\n",
        "    func=gpt4_with_memory,\n",
        "    description=\"Use this for answering general questions, explanations, and advice.\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Define Wikipedia Search as a tool\n",
        "wiki_tool = Tool(\n",
        "    name=\"Wikipedia Search\",\n",
        "    func=WikipediaAPIWrapper().run,\n",
        "    description=\"Use this to look up general knowledge, historical facts, or scientific topics.\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Initialize the agent with Wikipedia & GPT-4\n",
        "agent = initialize_agent(\n",
        "    tools=[gpt4_tool, wiki_tool],  # Only two tools: ChatGPT & Wikipedia\n",
        "    llm=gpt4_llm,  # Default model if tools aren't explicitly used\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Lets the agent decide the best approach\n",
        "    memory=memory,  # Uses memory to track chat history\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "su6dmjJBV7bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Example Queries & Response Storage\n",
        "data = []\n",
        "\n",
        "# ü©∫ Medical Query - Symptoms of High Blood Pressure\n",
        "prompt = \"What are the symptoms of high blood pressure?\"\n",
        "response = agent.run(prompt)  # ‚úÖ Use LangChain agent to get response\n",
        "data.append([\"ü©∫ Medical - Blood Pressure\", prompt, response])\n",
        "\n",
        "# üåç Wikipedia Lookup - First Person on the Moon\n",
        "prompt = \"Who was the first person to walk on the moon?\"\n",
        "response = agent.run(prompt)  # ‚úÖ Wikipedia should fetch this\n",
        "data.append([\"üåç Wikipedia - Space\", prompt, response])\n",
        "\n",
        "# üò¥ Sleep & Health Query\n",
        "prompt = \"How can I improve my sleep quality?\"\n",
        "response = agent.run(prompt)  # ‚úÖ GPT-4 provides advice\n",
        "data.append([\"üò¥ Sleep & Energy\", prompt, response])\n",
        "\n",
        "# üåè Wikipedia Lookup - Capital of Japan\n",
        "prompt = \"What is the capital of Japan?\"\n",
        "response = agent.run(prompt)  # ‚úÖ Wikipedia should fetch this\n",
        "data.append([\"üåè Wikipedia - Geography\", prompt, response])\n",
        "\n",
        "# üí§ Follow-up Sleep Query\n",
        "prompt = \"Tell me more about sleep quality improvements.\"\n",
        "response = agent.run(prompt)  # ‚úÖ GPT-4 remembers chat history\n",
        "data.append([\"üí§ Sleep Follow-Up\", prompt, response])\n",
        "\n",
        "# üìä Convert to DataFrame and Display\n",
        "df = pd.DataFrame(data, columns=[\"Category\", \"Prompt\", \"Response\"])\n",
        "\n",
        "# ‚úÖ Print the table in Colab\n",
        "display(df)"
      ],
      "metadata": {
        "id": "-9J6tr5jAAXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "id": "rhHM0fDFbkQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr  # üíª Import Gradio to create an interactive web UI\n",
        "\n",
        "# üè• Define sample health-related questions (e.g., predicting health risks)\n",
        "sample_questions = [\n",
        "    \"Predict my health risks: Age 65, Male, BMI 28, Smoker, Blood Pressure 140/90\",\n",
        "    \"Based on my data: Age 45, Female, BMI 22, Resting Heart Rate 80 bpm, how healthy am I?\",\n",
        "    \"What are early signs of heart disease in a 55-year-old male?\",\n",
        "    \"How does sleep deprivation affect long-term health?\",\n",
        "    \"What are the biggest risk factors for diabetes?\"\n",
        "]\n",
        "\n",
        "# üè• Define the Gradio UI for interacting with the LangChain Agent\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üè• AI Health & Knowledge Assistant ü§ñ\")  # üîπ Title of the UI\n",
        "\n",
        "    # üì• User input field (where selected questions will be inserted)\n",
        "    user_input = gr.Textbox(label=\"üîç Ask a Health or General Question\")\n",
        "\n",
        "    # üìå Add sample questions as buttons\n",
        "    with gr.Row():\n",
        "        for question in sample_questions:\n",
        "            gr.Button(question).click(\n",
        "                fn=lambda q=question: q,\n",
        "                inputs=[],\n",
        "                outputs=user_input\n",
        "            )  # üìå Clicking a button fills the textbox\n",
        "\n",
        "    # üî• Temperature slider for GPT-4 (affects randomness in responses)\n",
        "    temperature_slider = gr.Slider(\n",
        "        minimum=0.1, maximum=1.0, value=0.7, step=0.1,\n",
        "        label=\"üå°Ô∏è GPT-4 Temperature (Lower = Precise, Higher = Creative)\"\n",
        "    )\n",
        "\n",
        "    # ‚úÖ Submit button to process the query\n",
        "    submit_button = gr.Button(\"üí° Get AI Response\")\n",
        "\n",
        "    # üì§ Output box to display the AI-generated response\n",
        "    response_output = gr.Textbox(label=\"ü§ñ AI Response\", interactive=False)\n",
        "\n",
        "    # ‚úÖ Define interaction: When the button is clicked, the LangChain agent is triggered\n",
        "    submit_button.click(\n",
        "        fn=lambda user_question: agent.run(user_question),  # üîÑ Uses the LangChain agent\n",
        "        inputs=[user_input],  # üì• Takes user query as input\n",
        "        outputs=response_output  # üì§ Displays AI-generated response\n",
        "    )\n",
        "\n",
        "# üåç Launch the Gradio app\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "hzO3W6_4b29p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1Ô∏è‚É£ Install Required Libraries üöÄ\n"
      ],
      "metadata": {
        "id": "No6r9Ej0TtWR"
      }
    }
  ]
}