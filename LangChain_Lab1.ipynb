{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/LangChain_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b23c44",
      "metadata": {
        "id": "51b23c44"
      },
      "source": [
        "# üß† LangChain Lab 1: Introduction\n",
        "Welcome to the first hands-on lab for LangChain! In this lab, we will:\n",
        "- Set up LangChain in Google Colab\n",
        "- Interact with OpenAI models\n",
        "- Work with open-source models like LLaMA and Falcon\n",
        "- Understand prompt templates and experiment with parameters\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7db95b",
      "metadata": {
        "id": "8e7db95b"
      },
      "source": [
        "## üìå Step 1: Install Required Libraries\n",
        "We need to install LangChain and some additional dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7683f7f",
      "metadata": {
        "id": "c7683f7f"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üìå Installing Required Libraries for LangChain Lab\n",
        "# ==================================================\n",
        "\n",
        "!pip install langchain  # Core framework for working with LLMs\n",
        "!pip install langchain-community # Install the community package containing LLMs\n",
        "!pip install openai==0.28  # OpenAI API package (version 0.28) for GPT models\n",
        "!pip install transformers  # Hugging Face's library for open-source models like LLaMA and Falcon\n",
        "!pip install torch  # PyTorch, required for running deep learning models efficiently\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cDbNlFoDqiET",
      "metadata": {
        "id": "cDbNlFoDqiET"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üìå Importing Required Libraries for LangChain Lab\n",
        "# ==================================================\n",
        "\n",
        "import os  # For setting environment variables, such as API keys\n",
        "import ipywidgets as widgets  # For creating interactive input widgets in Jupyter or Colab\n",
        "from langchain.llms import OpenAI  # LangChain's wrapper for interacting with OpenAI LLMs\n",
        "import transformers  # Hugging Face library for loading and using pre-trained open-source models\n",
        "import torch  # PyTorch library, required for running and training deep learning models\n",
        "from IPython.display import clear_output, display  # For clearing and managing outputs in Jupyter or Colab notebooks\n",
        "\n",
        "# Imports successful if no errors occur\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6843a5c",
      "metadata": {
        "id": "c6843a5c"
      },
      "source": [
        "## üîë Step 2: Set Up OpenAI API Key\n",
        "If you want to use OpenAI models like GPT-4, you need an API key. Run the code below and enter your key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba2e457",
      "metadata": {
        "id": "fba2e457"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üîë OpenAI API Key Setup with Output Clearing\n",
        "# ==================================================\n",
        "\n",
        "import openai\n",
        "\n",
        "# Create an input widget for the OpenAI API key\n",
        "openai_key_input = widgets.Password(\n",
        "    description=\"üîë OpenAI Key:\",\n",
        "    placeholder=\"Enter your OpenAI API Key\",\n",
        ")\n",
        "\n",
        "# Create a button to submit the API key\n",
        "submit_button = widgets.Button(description=\"‚úÖ Set API Key\")\n",
        "\n",
        "# Function to save the OpenAI API key when the button is clicked\n",
        "def set_api_key(b):\n",
        "    # Clear previous outputs\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Display the input widget and button again\n",
        "    display(openai_key_input, submit_button)\n",
        "\n",
        "    # Retrieve and validate the API key\n",
        "    openai_key = openai_key_input.value.strip()\n",
        "\n",
        "    if openai_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        openai.api_key = openai_key\n",
        "        print(\"‚úÖ OpenAI API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"‚ùå Please enter a valid OpenAI API Key.\")\n",
        "\n",
        "# Link button click to the function\n",
        "submit_button.on_click(set_api_key)\n",
        "\n",
        "# Display the API key input field and button\n",
        "display(openai_key_input, submit_button)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27zoTSlqMFr",
      "metadata": {
        "id": "e27zoTSlqMFr"
      },
      "source": [
        "# üìå **Testing LangChain Connection (OpenAI GPT-4)**  \n",
        "\n",
        "Now that we've set up our OpenAI API key, let's test if **LangChain** is properly connected.  \n",
        "Unlike directly calling OpenAI‚Äôs API, this approach leverages **LangChain‚Äôs framework** to manage interactions efficiently.  \n",
        "\n",
        "### üîπ **What This Code Does:**  \n",
        "1Ô∏è‚É£ **Imports Required Libraries** ‚Üí `ChatOpenAI` for OpenAI chat models and `HumanMessage` for structured inputs.  \n",
        "2Ô∏è‚É£ **Initializes an OpenAI LLM Instance** ‚Üí Uses `gpt-4` via LangChain.  \n",
        "3Ô∏è‚É£ **Sends a Test Prompt** ‚Üí Verifies that LangChain can process requests through OpenAI.  \n",
        "4Ô∏è‚É£ **Uses `.invoke()` to Generate a Response** ‚Üí  \n",
        "   - `.invoke()` is the recommended method in LangChain for calling an LLM.  \n",
        "   - It ensures the request is formatted correctly and optimizes processing within the framework.  \n",
        "\n",
        "5Ô∏è‚É£ **Checks for a Successful Connection** ‚Üí If LangChain is set up correctly, the model will return a valid response.  \n",
        "\n",
        "üöÄ **Next Step:** If you receive a valid response, your LangChain integration is working as expected!  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HpUBxiUZqLqU",
      "metadata": {
        "id": "HpUBxiUZqLqU"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test: OpenAI (GPT-4)\n",
        "# üìå Import Libraries for OpenAI\n",
        "## Imports\n",
        "### - ChatOpenAI: For interacting specifically with OpenAI's GPT chat models.\n",
        "### - HumanMessage: Used to format user inputs as part of structured conversations.\n",
        "# =========================================================\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI's GPT chat models\n",
        "from langchain.schema import HumanMessage  # Message schema for chat models\n",
        "\n",
        "# ‚úÖ Step 1: Define the prompt\n",
        "prompt = \"What is LangChain in a 1 short sentence?\"  # Simple test question\n",
        "\n",
        "# ‚úÖ Step 2: Initialize the OpenAI LLM instance\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")  # GPT-4 model via OpenAI\n",
        "\n",
        "# ‚úÖ Step 3: Run the query for OpenAI\n",
        "response = llm_openai.invoke([HumanMessage(content=prompt)])\n",
        "print(\"üîπ OpenAI Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49B7fPC_sM6g",
      "metadata": {
        "id": "49B7fPC_sM6g"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Reading the LLM's Response**\n",
        "# Replace '-----' in the placeholders with the correct method or key to retrieve the requested information.\n",
        "\n",
        "# Task 1: Get the main content from the LLM response\n",
        "response_text = response.-----  # Extract the main response text (e.g., \"content\" for ChatOpenAI)\n",
        "\n",
        "# Task 2: Get the number of prompt tokens used\n",
        "prompt_tokens = response.-----  # Extract the number of tokens used in the input prompt\n",
        "\n",
        "# Task 3: Get the number of response tokens generated\n",
        "response_tokens = response.-----  # Extract the number of tokens used in the generated response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0kwgZutv3RoF",
      "metadata": {
        "id": "0kwgZutv3RoF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6MMOLIqdwO3x",
      "metadata": {
        "id": "6MMOLIqdwO3x"
      },
      "source": [
        "# üìù Understanding Roles in LLMs: System, Assistant, and Human\n",
        "\n",
        "LLMs like GPT-4 use **three roles** to structure conversations:  \n",
        "\n",
        "1Ô∏è‚É£ **System** ‚Üí Defines AI behavior and tone (e.g., `\"You are a financial expert.\"`)  \n",
        "2Ô∏è‚É£ **Human (User)** ‚Üí Represents user input (e.g., `\"What are safe investments for 2024?\"`)  \n",
        "3Ô∏è‚É£ **Assistant** ‚Üí The AI‚Äôs response (e.g., `\"Government bonds and index funds are good low-risk options.\"`)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SaZnAErHwNN3",
      "metadata": {
        "id": "SaZnAErHwNN3"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Using Roles in LangChain\n",
        "# ==================================================\n",
        "\n",
        "# üìå Importing Required Classes from LangChain\n",
        "# - SystemMessage: Defines the AI's role and behavior.\n",
        "# - HumanMessage: Represents user input in the conversation.\n",
        "# - AIMessage: (Not used in this example but represents AI responses when needed).\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# ‚úÖ Step 1: Define the conversation structure\n",
        "# - SystemMessage sets the AI‚Äôs persona (financial advisor).\n",
        "# - HumanMessage represents a user asking a financial question.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a financial advisor.\"),  # Guides AI behavior\n",
        "    HumanMessage(content=\"What are top 3 good passive income investments?\"),  # User query\n",
        "]\n",
        "\n",
        "# ‚úÖ Step 2: Pass messages to the LLM for processing\n",
        "# - 'llm' should be an initialized model instance before running this.\n",
        "# - The LLM will generate a response based on the provided messages.\n",
        "response = llm.invoke(messages)\n",
        "print(\"üîπ Response:\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Using Roles in LangChain (Travel Assistant)**\n",
        "# Replace the placeholders to:\n",
        "# 1Ô∏è‚É£ Define a structured conversation using SystemMessage and HumanMessage.\n",
        "# 2Ô∏è‚É£ Use the correct method to call the LLM.\n",
        "# 3Ô∏è‚É£ Extract and print the response.\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage  # üîß Add one more import if needed\n",
        "\n",
        "# ‚úÖ Step 1: Define the conversation structure\n",
        "messages = [\n",
        "    ----- (content=\"You are a travel assistant helping users plan their trips.\"),  # üîß Replace '-----' with the correct role\n",
        "    HumanMessage(content=\"What are the top 3 destinations for solo travelers?\"),  # User query\n",
        "]\n",
        "\n",
        "# ‚úÖ Step 2: Pass messages to the LLM for processing\n",
        "response = llm.----- (messages)  # üîß Replace '-----' with the correct method to call the model\n",
        "print(\"üîπ Travel Assistant Response:\", response.-----)  # üîß Replace '-----' with the correct way to extract content\n"
      ],
      "metadata": {
        "id": "4qIS6ujaAe0p"
      },
      "id": "4qIS6ujaAe0p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Alternative: Getting OpenAI Response Without `.invoke()`\n",
        "response = llm([\n",
        "    SystemMessage(content=\"You are a financial advisor.\"),\n",
        "    HumanMessage(content=\"What are top 3 good passive income investments?\")\n",
        "])\n",
        "print(\"üîπ OpenAI Response:\", response.content)  # Extract and print response text\n"
      ],
      "metadata": {
        "id": "tKlCEM_09f8g"
      },
      "id": "tKlCEM_09f8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Ng1KX4E_xXqH",
      "metadata": {
        "id": "Ng1KX4E_xXqH"
      },
      "source": [
        "# üîÑ **Multi-Turn Conversation in LangChain**\n",
        "\n",
        "A **multi-turn conversation** allows an AI to retain context across multiple exchanges, making interactions more natural and intelligent. Instead of treating each query independently, the AI builds on previous inputs, improving coherence and accuracy.\n",
        "\n",
        "### ‚úÖ **Why Use Multi-Turn Conversations?**\n",
        "- üß† **Context Retention** ‚Äì AI remembers past interactions, leading to more relevant responses.  \n",
        "- üí¨ **Realistic Dialogue** ‚Äì Mimics natural human conversations, making chatbots more interactive.  \n",
        "- üéØ **Better Accuracy** ‚Äì Responses are refined based on previous exchanges.  \n",
        "- üìà **Scalable Design** ‚Äì Supports long-form AI discussions without losing meaning.  \n",
        "\n",
        "This structure is useful for **financial advisors, chatbots, research assistants, and other AI-driven applications** that require ongoing, dynamic conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VPScknqNr28z",
      "metadata": {
        "id": "VPScknqNr28z"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Using Multiple Messages in LangChain\n",
        "# ==================================================\n",
        "\n",
        "# üìå This code demonstrates a multi-turn conversation where the AI retains context.\n",
        "# - The SystemMessage sets the AI‚Äôs role as a financial advisor.\n",
        "# - HumanMessage represents the user‚Äôs input in a conversation.\n",
        "# - AIMessage contains the AI‚Äôs responses based on previous interactions.\n",
        "# - Multiple HumanMessage and AIMessage instances create a back-and-forth exchange.\n",
        "# - The conversation now ends with a HumanMessage, allowing the AI to respond dynamically.\n",
        "# - LangChain processes this structured dialogue, allowing the AI to generate responses informed by earlier messages.\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# ‚úÖ Define a conversation with 6 messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a financial advisor specializing in investment strategies.\"),  # Sets AI's role\n",
        "    HumanMessage(content=\"What are good passive income investments?\"),  # User asks first question\n",
        "    AIMessage(content=\"Some good passive income investments include dividend stocks, real estate rentals, and REITs.\"),  # AI responds\n",
        "    HumanMessage(content=\"Which one has the lowest risk?\"),  # User follows up\n",
        "    AIMessage(content=\"Government bonds and high-yield savings accounts are considered the lowest-risk options.\"),  # AI provides more details\n",
        "    HumanMessage(content=\"Are there any tax benefits to these options?\")  # User asks another question\n",
        "]\n",
        "\n",
        "# ‚úÖ Example of passing messages to an LLM\n",
        "response = llm(messages)  # Replace 'llm' with your initialized model\n",
        "print(response.content)  # Print AI's response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jhpvSXq7yCv_",
      "metadata": {
        "id": "jhpvSXq7yCv_"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Completing a Multi-Turn Conversation (Travel Assistant)**\n",
        "\n",
        "# üìå Task Instructions:\n",
        "# - Below is a conversation with a **travel assistant** AI.\n",
        "# - Fill in the last `HumanMessage` with a relevant travel-related question.\n",
        "# - Complete the placeholder `response = ----(----)` to correctly call the LLM.\n",
        "\n",
        "# Define a conversation with missing parts\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful travel assistant, providing recommendations for destinations and travel tips.\"),\n",
        "    HumanMessage(content=\"What are some must-visit places in Japan?\"),\n",
        "    AIMessage(content=\"Some must-visit places in Japan include Tokyo, Kyoto, and Osaka. Each city offers unique cultural and historical experiences.\"),\n",
        "    HumanMessage(content=\"-----\")  #  Task: Fill in a relevant follow-up question\n",
        "]\n",
        "\n",
        "# üîß Task: Complete the function to generate a response from the LLM\n",
        "response = ----(----)  # Call the LLM correctly using the messages\n",
        "\n",
        "print(response)  # Display the AI's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IY4M6c06zgqR",
      "metadata": {
        "id": "IY4M6c06zgqR"
      },
      "source": [
        "# ü§ñ **Multi-LLM Comparison: Understanding Model Differences**  \n",
        "\n",
        "As AI-powered language models continue to evolve, different models offer **unique perspectives** based on their training data and architecture. This task explores how multiple LLMs respond to the same prompt, helping us understand their **strengths, biases, and response styles**.  \n",
        "\n",
        "### **Why Compare Multiple LLMs?**  \n",
        "‚úÖ **Varied insights** ‚Äì Each model generates responses based on different datasets and training techniques.  \n",
        "‚úÖ **Performance differences** ‚Äì Some models prioritize factual accuracy, while others focus on creativity or conciseness.  \n",
        "‚úÖ **Task specialization** ‚Äì While some models excel in general knowledge, others are optimized for specific domains.  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "JoAeXeGcnAB4"
      },
      "id": "JoAeXeGcnAB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîë Hugging Face API Key Setup with Output Clearing\n",
        "# ==================================================\n",
        "\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# Create an input widget for the Hugging Face API key\n",
        "huggingface_key_input = widgets.Password(\n",
        "    description=\"ü§ó HF Key:\",\n",
        "    placeholder=\"Enter your Hugging Face API Key\",\n",
        ")\n",
        "\n",
        "# Create a button to submit the API key\n",
        "submit_button = widgets.Button(description=\"‚úÖ Set API Key\")\n",
        "\n",
        "# Function to save the Hugging Face API key when the button is clicked\n",
        "def set_hf_api_key(b):\n",
        "    # Clear previous outputs\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Display the input widget and button again\n",
        "    display(huggingface_key_input, submit_button)\n",
        "\n",
        "    # Retrieve and validate the API key\n",
        "    hf_key = huggingface_key_input.value.strip()\n",
        "\n",
        "    if hf_key:\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key\n",
        "        print(\"‚úÖ Hugging Face API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"‚ùå Please enter a valid Hugging Face API Key.\")\n",
        "\n",
        "# Link button click to the function\n",
        "submit_button.on_click(set_hf_api_key)\n",
        "\n",
        "# Display the API key input field and button\n",
        "display(huggingface_key_input, submit_button)\n"
      ],
      "metadata": {
        "id": "VR90KJx1BX5J"
      },
      "id": "VR90KJx1BX5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test with HuggingFaceEndpoint\n",
        "# =========================================================\n",
        "\n",
        "# üìå Import Libraries\n",
        "# - HuggingFaceEndpoint: Allows direct interaction with Hugging Face models via API.\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Initialize the Hugging Face endpoint with a specific model\n",
        "# - Falcon-7B-Instruct: A powerful, open-source language model optimized for instruction-based tasks.\n",
        "# - Developed by TII UAE, it is designed for high-quality text generation and reasoning.\n",
        "llm_falcon = HuggingFaceEndpoint(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\"  # Falcon-7B Instruct model\n",
        ")\n",
        "\n",
        "# ‚úÖ Define a question\n",
        "question = \"What are the key challenges of the AI boom?\"\n",
        "\n",
        "# ‚úÖ Query the model\n",
        "response_falcon = llm_falcon.invoke(question)  # Store the response\n",
        "\n",
        "# ‚úÖ Display the output\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "_W4Jn6aJ8gQa"
      },
      "id": "_W4Jn6aJ8gQa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Querying Falcon-7B with LangChain**\n",
        "# Replace the placeholders to:\n",
        "# 1Ô∏è‚É£ Initialize the Hugging Face model correctly.\n",
        "# 2Ô∏è‚É£ Use the correct method to query the model.\n",
        "# 3Ô∏è‚É£ Store the response in the right variable.\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Step 1: Initialize the Falcon model\n",
        "llm_falcon = ----- (repo_id=\"tiiuae/falcon-7b-instruct\")  # üîß Replace '-----' with the correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define the question\n",
        "question = \"How will AI impact the job market in the next decade?\"\n",
        "\n",
        "# ‚úÖ Step 3: Query the model\n",
        "response_falcon = llm_falcon.----- (question)  # üîß Replace '-----' with the correct method\n",
        "\n",
        "# ‚úÖ Display the response\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "NcHL_zVXDoA0"
      },
      "id": "NcHL_zVXDoA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecf8384",
      "metadata": {
        "id": "9ecf8384"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Call OpenAI's GPT-4 model\n",
        "response = openai.----(\n",
        "    model=\"gpt-4\",\n",
        "    prompt=\"Hello, how can I use LangChain?\",\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Handling Responses in LangChain (OpenAI vs. Hugging Face)**  \n",
        "\n",
        "üîπ **For OpenAI (`ChatOpenAI`)** ‚Üí The response contains structured data, so you need to extract `.content`.  \n",
        "üîπ **For Hugging Face (`HuggingFaceEndpoint`)** ‚Üí The response is plain text, so no `.content` is needed.  \n",
        "\n",
        "### ‚úÖ **Examples**\n",
        "\n",
        "#### **OpenAI (Requires `.content`)**\n",
        "```python\n",
        "response_openai = llm_openai.invoke([HumanMessage(content=\"What is AI?\")])\n",
        "print(response_openai.content)  # ‚úÖ Required for OpenAI\n",
        "response_falcon = llm_falcon.invoke(\"What is AI?\")\n",
        "print(response_falcon)  # ‚úÖ Directly usable\n"
      ],
      "metadata": {
        "id": "1WfMw6NyD_GV"
      },
      "id": "1WfMw6NyD_GV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tezuyOcJz7zG",
      "metadata": {
        "id": "tezuyOcJz7zG"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Multi-LLM Comparison: AI Boom (Updated)\n",
        "# ==================================================\n",
        "\n",
        "# üìå This code compares multiple LLMs by asking them the same question.\n",
        "# - A SystemMessage sets a common AI behavior (AI expert on the AI boom).\n",
        "# - A HumanMessage simulates a user query about AI challenges.\n",
        "# - Different LLMs (GPT-4, Falcon, Zephyr) process the same input.\n",
        "# - Their responses are stored and displayed for comparison.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "from langchain.schema import SystemMessage, HumanMessage  # Defines structured messages for AI interaction\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI chat models (e.g., GPT-4)\n",
        "from langchain_huggingface import HuggingFaceEndpoint  # Correct Hugging Face integration\n",
        "\n",
        "# ‚úÖ Define a system message (sets behavior of the AI)\n",
        "system_message = SystemMessage(content=\"You are an AI expert. Explain the impact of the AI boom on society.\")\n",
        "\n",
        "# ‚úÖ Define a human message (user's input)\n",
        "human_message = HumanMessage(content=\"What are the 2 biggest challenges of the AI boom?\")\n",
        "\n",
        "# ‚úÖ Dictionary of models to compare (each entry is an LLM instance)\n",
        "llms = {\n",
        "    \"GPT-4\": ChatOpenAI(model_name=\"gpt-4\"),  # OpenAI's GPT-4 model\n",
        "    \"Falcon\": HuggingFaceEndpoint(repo_id=\"tiiuae/falcon-7b-instruct\"),  # Falcon-7B Instruct model\n",
        "    \"Zephyr\": HuggingFaceEndpoint(\n",
        "        repo_id=\"HuggingFaceH4/zephyr-7b-alpha\"  # Zephyr-7B model\n",
        "    ),  # Zephyr is a lightweight, general-purpose LLM optimized for efficiency and speed.\n",
        "}\n",
        "\n",
        "# ‚úÖ Query each model and store responses\n",
        "responses = {}\n",
        "for model_name, llm in llms.items():\n",
        "    if isinstance(llm, ChatOpenAI):\n",
        "        # OpenAI models expect structured messages (extract `.content` from the response)\n",
        "        response = llm.invoke([system_message, human_message])\n",
        "        responses[model_name] = response.content  # Extract and store the response content\n",
        "    elif isinstance(llm, HuggingFaceEndpoint):\n",
        "        # Hugging Face models expect a plain text prompt\n",
        "        prompt = f\"{system_message.content} {human_message.content}\"\n",
        "        responses[model_name] = llm.invoke(prompt)  # Hugging Face models return plain text directly\n",
        "\n",
        "# ‚úÖ Display responses for comparison\n",
        "for model, response in responses.items():\n",
        "    print(f\"üîπ {model} Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Multi-LLM Comparison with OpenAI and Hugging Face**\n",
        "# Replace the placeholders (`-----`) to:\n",
        "# 1Ô∏è‚É£ Initialize the OpenAI and Hugging Face models properly.\n",
        "# 2Ô∏è‚É£ Format the messages correctly.\n",
        "# 3Ô∏è‚É£ Extract the OpenAI response content and handle Hugging Face response directly.\n",
        "# 4Ô∏è‚É£ Print all the model responses for comparison.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "from langchain.schema import -----, -----  # Define the correct classes for AI messages\n",
        "from langchain.chat_models import -----  # Import OpenAI's chat model class\n",
        "from langchain_huggingface import -----  # Import Hugging Face endpoint class\n",
        "\n",
        "# ‚úÖ Step 1: Define a system message (AI's role/behavior)\n",
        "system_message = ----- (content=\"You are a healthcare consultant. Provide insights on improving hospital efficiency.\")  # Correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define a human message (user's question)\n",
        "human_message = ----- (content=\"What strategies can hospitals implement to reduce patient wait times?\")  # Correct class\n",
        "\n",
        "# ‚úÖ Step 3: Initialize models\n",
        "llms = {\n",
        "    \"GPT-4\": ----- (model_name=\"gpt-4\"),  # Correct model initialization for OpenAI\n",
        "    \"Falcon\": ----- (repo_id=\"tiiuae/falcon-7b-instruct\"),  # Hugging Face model for Falcon\n",
        "    \"Zephyr\": ----- (repo_id=\"HuggingFaceH4/zephyr-7b-alpha\"),  # Hugging Face model for Zephyr\n",
        "}\n",
        "\n",
        "# ‚úÖ Step 4: Query each model and store responses\n",
        "responses = {}\n",
        "for model_name, llm in llms.items():\n",
        "    if isinstance(llm, -----):  # Replace with correct class to identify OpenAI models\n",
        "        response = llm.----- ([system_message, human_message])  # Replace with correct method\n",
        "        responses[model_name] = response.-----  # Extract the content for OpenAI models\n",
        "    elif isinstance(llm, -----):  # Replace with correct class to identify Hugging Face models\n",
        "        prompt = f\"{system_message.content} {human_message.content}\"\n",
        "        responses[model_name] = llm.----- (prompt)  # Replace with correct method for Hugging Face\n",
        "\n",
        "# ‚úÖ Step 5: Display all model responses\n",
        "for model, response in responses.items():\n",
        "    print(f\"üîπ {model} Response:\", response)\n"
      ],
      "metadata": {
        "id": "YvS3hJQSMRX1"
      },
      "id": "YvS3hJQSMRX1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Why is Prompting Different in OpenAI vs. Hugging Face?**  \n",
        "\n",
        "### üîπ **OpenAI (`ChatOpenAI`) Uses Structured Messages**\n",
        "- Requires **role-based prompts** (`SystemMessage`, `HumanMessage`).\n",
        "- Designed for **multi-turn conversations** with memory.\n",
        "```python\n",
        "response = llm.invoke([\n",
        "    SystemMessage(content=\"You are a healthcare consultant.\"),\n",
        "    HumanMessage(content=\"How can hospitals reduce patient wait times?\")\n",
        "])\n",
        "print(response.content)  # Extract response\n",
        "\n",
        "### üîπ **Hugging Face (`HuggingFaceEndpoint`) Uses Plain Text**\n",
        "- Models expect a **single text prompt**, no structured roles.\n",
        "- Typically **stateless**, meaning no conversation history.\n",
        "\n",
        "```python\n",
        "prompt = \"You are a healthcare consultant. How can hospitals reduce wait times?\"\n",
        "response = llm.invoke(prompt)  # Direct plain text input\n",
        "print(response)  # No `.content` needed\n"
      ],
      "metadata": {
        "id": "Kw7h8DV5M9v4"
      },
      "id": "Kw7h8DV5M9v4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You have successfully completed the **Introduction to LangChain** lab. üöÄ  \n",
        "We hope you found it insightful and are excited to explore more!  \n",
        "\n",
        "üí° **Next Steps:**  \n",
        "- Try using different LLMs in LangChain.  \n",
        "- Experiment with structured vs. plain text prompts.  \n",
        "- Explore advanced features like memory and chains.\n",
        "\n",
        "Happy Coding! üíª‚ú®  \n"
      ],
      "metadata": {
        "id": "BqzZxacjPK6k"
      },
      "id": "BqzZxacjPK6k"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}