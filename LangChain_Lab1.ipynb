{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/LangChain_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b23c44",
      "metadata": {
        "id": "51b23c44"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">üß† LangChain Lab 1: Introduction</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">Welcome to the first hands-on lab for LangChain! In this lab, we will:</p>\n",
        "  <p style=\"margin-top: 10px; font-size: 18px; font-weight: bold;\">Instructor: Dr. Dehghani</p>\n",
        "  <p style=\"margin-top: 5px; font-size: 14px;\">\n",
        "    Learn more at <a href=\"https://langchain.com\" target=\"_blank\" style=\"color: #ffdd57; text-decoration: underline;\">LangChain Website</a>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 20px; border: 1px solid #0055d4;\">\n",
        "  <p style=\"line-height: 1.6; font-size: 16px; margin-bottom: 15px;\">\n",
        "    LangChain is a framework for building applications powered by large language models (LLMs).  \n",
        "    It provides modular components‚Äîsuch as prompt templates, memory, and chains‚Äîthat make it easy to develop, test, and deploy LLM-based solutions.\n",
        "  </p>\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">Lab Objectives</h2>\n",
        "  <ul style=\"line-height: 1.8; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li>Set up LangChain in Google Colab</li>\n",
        "    <li>Interact with OpenAI models</li>\n",
        "    <li>Work with open-source models like LLaMA and Falcon</li>\n",
        "    <li>Understand prompt templates and experiment with parameters</li>\n",
        "  </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c7683f7f",
      "metadata": {
        "id": "c7683f7f"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è LangChain Lab Dependencies\n",
        "# ================================\n",
        "!pip install -q langchain            # core LLM framework\n",
        "!pip install -q langchain-community # extra integrations & utils\n",
        "!pip install -q openai==0.28        # OpenAI GPT client (v0.28)\n",
        "!pip install -q transformers        # Hugging Face models (LLaMA, Falcon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cDbNlFoDqiET",
      "metadata": {
        "id": "cDbNlFoDqiET",
        "outputId": "14d5b813-33d2-451b-bf0c-d8194ddfed0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# üìå Importing Required Libraries for LangChain Lab\n",
        "# ==================================================\n",
        "\n",
        "import os                                  # For environment variables (API keys)\n",
        "from google.colab import userdata          # For loading secrets in Colab\n",
        "\n",
        "# LangChain chat utilities\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI‚Äôs GPT chat models\n",
        "from langchain.schema import HumanMessage     # Schema for structured chat messages\n",
        "\n",
        "# Open-source model support\n",
        "import transformers                         # Hugging Face models (e.g., LLaMA, Falcon)\n",
        "import torch                                # PyTorch for deep learning execution\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6843a5c",
      "metadata": {
        "id": "c6843a5c"
      },
      "source": [
        "## üîë Step 2: Set Up OpenAI API Key\n",
        "If you want to use OpenAI models like GPT-4, you need an API key. Run the code below and enter your key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fba2e457",
      "metadata": {
        "id": "fba2e457",
        "outputId": "52755de7-22ed-4354-c97d-c1f90f0d5c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded successfully!\n",
            "‚úÖ Hugging Face API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è Load API Keys from Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve your stored secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')  # OpenAI API key\n",
        "HF_API_KEY     = userdata.get('HF_API_KEY')      # Hugging Face API key\n",
        "\n",
        "# Feedback messages with emoji\n",
        "if OPENAI_API_KEY:\n",
        "    print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API key not found. Please set 'OPENAI_API_KEY' in Colab secrets.\")\n",
        "\n",
        "if HF_API_KEY:\n",
        "    print(\"‚úÖ Hugging Face API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Hugging Face API key not found. Please set 'HF_API_KEY' in Colab secrets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27zoTSlqMFr",
      "metadata": {
        "id": "e27zoTSlqMFr"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">üìå Testing LangChain Connection (OpenAI GPT-4)</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">\n",
        "    Now that we've set up our OpenAI API key, let's test if <strong>LangChain</strong> is properly connected.<br>\n",
        "    Unlike directly calling OpenAI‚Äôs API, this approach leverages <strong>LangChain‚Äôs framework</strong> to manage interactions efficiently.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 20px; margin-bottom: 25px; border: 1px solid #0055d4;\">\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">üîπ What This Code Does:</h2>\n",
        "  <ol style=\"line-height: 1.8; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li><strong>Imports Required Libraries</strong> ‚Üí <code>ChatOpenAI</code> for OpenAI chat models and <code>HumanMessage</code> for structured inputs.</li>\n",
        "    <li><strong>Initializes an OpenAI LLM Instance</strong> ‚Üí Uses <code>gpt-4</code> via LangChain.</li>\n",
        "    <li><strong>Sends a Test Prompt</strong> ‚Üí Verifies that LangChain can process requests through OpenAI.</li>\n",
        "    <li><strong>Uses <code>.invoke()</code> to Generate a Response</strong> ‚Üí<br>\n",
        "      &nbsp;&nbsp;‚Äì <code>.invoke()</code> is the recommended method in LangChain for calling an LLM.<br>\n",
        "      &nbsp;&nbsp;‚Äì It ensures the request is formatted correctly and optimizes processing within the framework.\n",
        "    </li>\n",
        "    <li><strong>Checks for a Successful Connection</strong> ‚Üí If LangChain is set up correctly, the model will return a valid response.</li>\n",
        "  </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3f3; border-radius: 12px; padding: 15px; border: 1px solid #ff6b6b;\">\n",
        "  <p style=\"margin: 0; line-height: 1.6; font-size: 16px; color: #d32f2f;\">\n",
        "    üöÄ <strong>Next Step:</strong> If you receive a valid response, your LangChain integration is working as expected!\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "HpUBxiUZqLqU",
      "metadata": {
        "id": "HpUBxiUZqLqU",
        "outputId": "b1edd316-4758-4b4d-e3dd-884801d98a88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ OpenAI Response: LangChain is a decentralized translation platform powered by artificial intelligence and blockchain technology.\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test: OpenAI (GPT-4)\n",
        "# üìå Import Libraries & Load API Key\n",
        "# =========================================================\n",
        "\n",
        "# üîë Load your OpenAI API key from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# ‚úÖ Step 1: Initialize the LLM with your API key\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 2: Define the prompt\n",
        "prompt = \"What is LangChain in one short sentence?\"\n",
        "\n",
        "# ‚úÖ Step 3: Invoke the model\n",
        "response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "# ‚úÖ Step 4: Display the response\n",
        "print(\"üîπ OpenAI Response:\", response.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49B7fPC_sM6g",
      "metadata": {
        "id": "49B7fPC_sM6g"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Reading the LLM's Response**\n",
        "# Replace '-----' in the placeholders with the correct method or key to retrieve the requested information.\n",
        "\n",
        "# Task 1: Get the main content from the LLM response\n",
        "response_text = response.-----  # Extract the main response text (e.g., \"content\" for ChatOpenAI)\n",
        "\n",
        "# Task 2: Get the number of prompt tokens used\n",
        "prompt_tokens = response.-----  # Extract the number of tokens used in the input prompt\n",
        "\n",
        "# Task 3: Get the number of response tokens generated\n",
        "response_tokens = response.-----  # Extract the number of tokens used in the generated response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6MMOLIqdwO3x",
      "metadata": {
        "id": "6MMOLIqdwO3x"
      },
      "source": [
        "# üìù Understanding Roles in LLMs: System, Assistant, and Human\n",
        "\n",
        "LLMs like GPT-4 use **three roles** to structure conversations:  \n",
        "\n",
        "1Ô∏è‚É£ **System** ‚Üí Defines AI behavior and tone (e.g., `\"You are a financial expert.\"`)  \n",
        "2Ô∏è‚É£ **Human (User)** ‚Üí Represents user input (e.g., `\"What are safe investments for 2024?\"`)  \n",
        "3Ô∏è‚É£ **Assistant** ‚Üí The AI‚Äôs response (e.g., `\"Government bonds and index funds are good low-risk options.\"`)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SaZnAErHwNN3",
      "metadata": {
        "id": "SaZnAErHwNN3"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Using Roles in LangChain\n",
        "# ==================================================\n",
        "\n",
        "# üìå Importing Required Classes from LangChain\n",
        "# - SystemMessage: Defines the AI's role and behavior.\n",
        "# - HumanMessage: Represents user input in the conversation.\n",
        "# - AIMessage: (Not used in this example but represents AI responses when needed).\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# ‚úÖ Step 1: Define the conversation structure\n",
        "# - SystemMessage sets the AI‚Äôs persona (financial advisor).\n",
        "# - HumanMessage represents a user asking a financial question.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a financial advisor.\"),  # Guides AI behavior\n",
        "    HumanMessage(content=\"What are top 3 good passive income investments?\"),  # User query\n",
        "]\n",
        "\n",
        "# ‚úÖ Step 2: Pass messages to the LLM for processing\n",
        "# - 'llm' should be an initialized model instance before running this.\n",
        "# - The LLM will generate a response based on the provided messages.\n",
        "response = llm.invoke(messages)\n",
        "print(\"üîπ Response:\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Using Roles in LangChain (Travel Assistant)**\n",
        "# Replace the placeholders to:\n",
        "# 1Ô∏è‚É£ Define a structured conversation using SystemMessage and HumanMessage.\n",
        "# 2Ô∏è‚É£ Use the correct method to call the LLM.\n",
        "# 3Ô∏è‚É£ Extract and print the response.\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage  # üîß Add one more import if needed\n",
        "\n",
        "# ‚úÖ Step 1: Define the conversation structure\n",
        "messages = [\n",
        "    ----- (content=\"You are a travel assistant helping users plan their trips.\"),  # üîß Replace '-----' with the correct role\n",
        "    HumanMessage(content=\"What are the top 3 destinations for solo travelers?\"),  # User query\n",
        "]\n",
        "\n",
        "# ‚úÖ Step 2: Pass messages to the LLM for processing\n",
        "response = llm.----- (messages)  # üîß Replace '-----' with the correct method to call the model\n",
        "print(\"üîπ Travel Assistant Response:\", response.-----)  # üîß Replace '-----' with the correct way to extract content\n"
      ],
      "metadata": {
        "id": "4qIS6ujaAe0p"
      },
      "id": "4qIS6ujaAe0p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Alternative: Getting OpenAI Response Without `.invoke()`\n",
        "response = llm([\n",
        "    SystemMessage(content=\"You are a financial advisor.\"),\n",
        "    HumanMessage(content=\"What are top 3 good passive income investments?\")\n",
        "])\n",
        "print(\"üîπ OpenAI Response:\", response.content)  # Extract and print response text\n"
      ],
      "metadata": {
        "id": "tKlCEM_09f8g"
      },
      "id": "tKlCEM_09f8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Ng1KX4E_xXqH",
      "metadata": {
        "id": "Ng1KX4E_xXqH"
      },
      "source": [
        "# üîÑ **Multi-Turn Conversation in LangChain**\n",
        "\n",
        "A **multi-turn conversation** allows an AI to retain context across multiple exchanges, making interactions more natural and intelligent. Instead of treating each query independently, the AI builds on previous inputs, improving coherence and accuracy.\n",
        "\n",
        "### ‚úÖ **Why Use Multi-Turn Conversations?**\n",
        "- üß† **Context Retention** ‚Äì AI remembers past interactions, leading to more relevant responses.  \n",
        "- üí¨ **Realistic Dialogue** ‚Äì Mimics natural human conversations, making chatbots more interactive.  \n",
        "- üéØ **Better Accuracy** ‚Äì Responses are refined based on previous exchanges.  \n",
        "- üìà **Scalable Design** ‚Äì Supports long-form AI discussions without losing meaning.  \n",
        "\n",
        "This structure is useful for **financial advisors, chatbots, research assistants, and other AI-driven applications** that require ongoing, dynamic conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VPScknqNr28z",
      "metadata": {
        "id": "VPScknqNr28z"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Using Multiple Messages in LangChain\n",
        "# ==================================================\n",
        "\n",
        "# üìå This code demonstrates a multi-turn conversation where the AI retains context.\n",
        "# - The SystemMessage sets the AI‚Äôs role as a financial advisor.\n",
        "# - HumanMessage represents the user‚Äôs input in a conversation.\n",
        "# - AIMessage contains the AI‚Äôs responses based on previous interactions.\n",
        "# - Multiple HumanMessage and AIMessage instances create a back-and-forth exchange.\n",
        "# - The conversation now ends with a HumanMessage, allowing the AI to respond dynamically.\n",
        "# - LangChain processes this structured dialogue, allowing the AI to generate responses informed by earlier messages.\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# ‚úÖ Define a conversation with 6 messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a financial advisor specializing in investment strategies.\"),  # Sets AI's role\n",
        "    HumanMessage(content=\"What are good passive income investments?\"),  # User asks first question\n",
        "    AIMessage(content=\"Some good passive income investments include dividend stocks, real estate rentals, and REITs.\"),  # AI responds\n",
        "    HumanMessage(content=\"Which one has the lowest risk?\"),  # User follows up\n",
        "    AIMessage(content=\"Government bonds and high-yield savings accounts are considered the lowest-risk options.\"),  # AI provides more details\n",
        "    HumanMessage(content=\"Are there any tax benefits to these options?\")  # User asks another question\n",
        "]\n",
        "\n",
        "# ‚úÖ Example of passing messages to an LLM\n",
        "response = llm(messages)  # Replace 'llm' with your initialized model\n",
        "print(response.content)  # Print AI's response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jhpvSXq7yCv_",
      "metadata": {
        "id": "jhpvSXq7yCv_"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Completing a Multi-Turn Conversation (Travel Assistant)**\n",
        "\n",
        "# üìå Task Instructions:\n",
        "# - Below is a conversation with a **travel assistant** AI.\n",
        "# - Fill in the last `HumanMessage` with a relevant travel-related question.\n",
        "# - Complete the placeholder `response = ----(----)` to correctly call the LLM.\n",
        "\n",
        "# Define a conversation with missing parts\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful travel assistant, providing recommendations for destinations and travel tips.\"),\n",
        "    HumanMessage(content=\"What are some must-visit places in Japan?\"),\n",
        "    AIMessage(content=\"Some must-visit places in Japan include Tokyo, Kyoto, and Osaka. Each city offers unique cultural and historical experiences.\"),\n",
        "    HumanMessage(content=\"-----\")  #  Task: Fill in a relevant follow-up question\n",
        "]\n",
        "\n",
        "# üîß Task: Complete the function to generate a response from the LLM\n",
        "response = ----(----)  # Call the LLM correctly using the messages\n",
        "\n",
        "print(response)  # Display the AI's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IY4M6c06zgqR",
      "metadata": {
        "id": "IY4M6c06zgqR"
      },
      "source": [
        "# ü§ñ **Multi-LLM Comparison: Understanding Model Differences**  \n",
        "\n",
        "As AI-powered language models continue to evolve, different models offer **unique perspectives** based on their training data and architecture. This task explores how multiple LLMs respond to the same prompt, helping us understand their **strengths, biases, and response styles**.  \n",
        "\n",
        "### **Why Compare Multiple LLMs?**  \n",
        "‚úÖ **Varied insights** ‚Äì Each model generates responses based on different datasets and training techniques.  \n",
        "‚úÖ **Performance differences** ‚Äì Some models prioritize factual accuracy, while others focus on creativity or conciseness.  \n",
        "‚úÖ **Task specialization** ‚Äì While some models excel in general knowledge, others are optimized for specific domains.  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "JoAeXeGcnAB4"
      },
      "id": "JoAeXeGcnAB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîë Hugging Face API Key Setup with Output Clearing\n",
        "# ==================================================\n",
        "\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# Create an input widget for the Hugging Face API key\n",
        "huggingface_key_input = widgets.Password(\n",
        "    description=\"ü§ó HF Key:\",\n",
        "    placeholder=\"Enter your Hugging Face API Key\",\n",
        ")\n",
        "\n",
        "# Create a button to submit the API key\n",
        "submit_button = widgets.Button(description=\"‚úÖ Set API Key\")\n",
        "\n",
        "# Function to save the Hugging Face API key when the button is clicked\n",
        "def set_hf_api_key(b):\n",
        "    # Clear previous outputs\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Display the input widget and button again\n",
        "    display(huggingface_key_input, submit_button)\n",
        "\n",
        "    # Retrieve and validate the API key\n",
        "    hf_key = huggingface_key_input.value.strip()\n",
        "\n",
        "    if hf_key:\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key\n",
        "        print(\"‚úÖ Hugging Face API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"‚ùå Please enter a valid Hugging Face API Key.\")\n",
        "\n",
        "# Link button click to the function\n",
        "submit_button.on_click(set_hf_api_key)\n",
        "\n",
        "# Display the API key input field and button\n",
        "display(huggingface_key_input, submit_button)\n"
      ],
      "metadata": {
        "id": "VR90KJx1BX5J"
      },
      "id": "VR90KJx1BX5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test with HuggingFaceEndpoint\n",
        "# =========================================================\n",
        "\n",
        "# üìå Import Libraries\n",
        "# - HuggingFaceEndpoint: Allows direct interaction with Hugging Face models via API.\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Initialize the Hugging Face endpoint with a specific model\n",
        "# - Falcon-7B-Instruct: A powerful, open-source language model optimized for instruction-based tasks.\n",
        "# - Developed by TII UAE, it is designed for high-quality text generation and reasoning.\n",
        "llm_falcon = HuggingFaceEndpoint(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\"  # Falcon-7B Instruct model\n",
        ")\n",
        "\n",
        "# ‚úÖ Define a question\n",
        "question = \"What are the key challenges of the AI boom?\"\n",
        "\n",
        "# ‚úÖ Query the model\n",
        "response_falcon = llm_falcon.invoke(question)  # Store the response\n",
        "\n",
        "# ‚úÖ Display the output\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "_W4Jn6aJ8gQa"
      },
      "id": "_W4Jn6aJ8gQa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Querying Falcon-7B with LangChain**\n",
        "# Replace the placeholders to:\n",
        "# 1Ô∏è‚É£ Initialize the Hugging Face model correctly.\n",
        "# 2Ô∏è‚É£ Use the correct method to query the model.\n",
        "# 3Ô∏è‚É£ Store the response in the right variable.\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Step 1: Initialize the Falcon model\n",
        "llm_falcon = ----- (repo_id=\"tiiuae/falcon-7b-instruct\")  # üîß Replace '-----' with the correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define the question\n",
        "question = \"How will AI impact the job market in the next decade?\"\n",
        "\n",
        "# ‚úÖ Step 3: Query the model\n",
        "response_falcon = llm_falcon.----- (question)  # üîß Replace '-----' with the correct method\n",
        "\n",
        "# ‚úÖ Display the response\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "NcHL_zVXDoA0"
      },
      "id": "NcHL_zVXDoA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecf8384",
      "metadata": {
        "id": "9ecf8384"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Call OpenAI's GPT-4 model\n",
        "response = openai.----(\n",
        "    model=\"gpt-4\",\n",
        "    prompt=\"Hello, how can I use LangChain?\",\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Handling Responses in LangChain (OpenAI vs. Hugging Face)**  \n",
        "\n",
        "üîπ **For OpenAI (`ChatOpenAI`)** ‚Üí The response contains structured data, so you need to extract `.content`.  \n",
        "üîπ **For Hugging Face (`HuggingFaceEndpoint`)** ‚Üí The response is plain text, so no `.content` is needed.  \n",
        "\n",
        "### ‚úÖ **Examples**\n",
        "\n",
        "#### **OpenAI (Requires `.content`)**\n",
        "```python\n",
        "response_openai = llm_openai.invoke([HumanMessage(content=\"What is AI?\")])\n",
        "print(response_openai.content)  # ‚úÖ Required for OpenAI\n",
        "response_falcon = llm_falcon.invoke(\"What is AI?\")\n",
        "print(response_falcon)  # ‚úÖ Directly usable\n"
      ],
      "metadata": {
        "id": "1WfMw6NyD_GV"
      },
      "id": "1WfMw6NyD_GV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tezuyOcJz7zG",
      "metadata": {
        "id": "tezuyOcJz7zG"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Multi-LLM Comparison: AI Boom (Updated)\n",
        "# ==================================================\n",
        "\n",
        "# üìå This code compares multiple LLMs by asking them the same question.\n",
        "# - A SystemMessage sets a common AI behavior (AI expert on the AI boom).\n",
        "# - A HumanMessage simulates a user query about AI challenges.\n",
        "# - Different LLMs (GPT-4, Falcon, Zephyr) process the same input.\n",
        "# - Their responses are stored and displayed for comparison.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "from langchain.schema import SystemMessage, HumanMessage  # Defines structured messages for AI interaction\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI chat models (e.g., GPT-4)\n",
        "from langchain_huggingface import HuggingFaceEndpoint  # Correct Hugging Face integration\n",
        "\n",
        "# ‚úÖ Define a system message (sets behavior of the AI)\n",
        "system_message = SystemMessage(content=\"You are an AI expert. Explain the impact of the AI boom on society.\")\n",
        "\n",
        "# ‚úÖ Define a human message (user's input)\n",
        "human_message = HumanMessage(content=\"What are the 2 biggest challenges of the AI boom?\")\n",
        "\n",
        "# ‚úÖ Dictionary of models to compare (each entry is an LLM instance)\n",
        "llms = {\n",
        "    \"GPT-4\": ChatOpenAI(model_name=\"gpt-4\"),  # OpenAI's GPT-4 model\n",
        "    \"Falcon\": HuggingFaceEndpoint(repo_id=\"tiiuae/falcon-7b-instruct\"),  # Falcon-7B Instruct model\n",
        "    \"Zephyr\": HuggingFaceEndpoint(\n",
        "        repo_id=\"HuggingFaceH4/zephyr-7b-alpha\"  # Zephyr-7B model\n",
        "    ),  # Zephyr is a lightweight, general-purpose LLM optimized for efficiency and speed.\n",
        "}\n",
        "\n",
        "# ‚úÖ Query each model and store responses\n",
        "responses = {}\n",
        "for model_name, llm in llms.items():\n",
        "    if isinstance(llm, ChatOpenAI):\n",
        "        # OpenAI models expect structured messages (extract `.content` from the response)\n",
        "        response = llm.invoke([system_message, human_message])\n",
        "        responses[model_name] = response.content  # Extract and store the response content\n",
        "    elif isinstance(llm, HuggingFaceEndpoint):\n",
        "        # Hugging Face models expect a plain text prompt\n",
        "        prompt = f\"{system_message.content} {human_message.content}\"\n",
        "        responses[model_name] = llm.invoke(prompt)  # Hugging Face models return plain text directly\n",
        "\n",
        "# ‚úÖ Display responses for comparison\n",
        "for model, response in responses.items():\n",
        "    print(f\"üîπ {model} Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Multi-LLM Comparison with OpenAI and Hugging Face**\n",
        "# Replace the placeholders (`-----`) to:\n",
        "# 1Ô∏è‚É£ Initialize the OpenAI and Hugging Face models properly.\n",
        "# 2Ô∏è‚É£ Format the messages correctly.\n",
        "# 3Ô∏è‚É£ Extract the OpenAI response content and handle Hugging Face response directly.\n",
        "# 4Ô∏è‚É£ Print all the model responses for comparison.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "from langchain.schema import -----, -----  # Define the correct classes for AI messages\n",
        "from langchain.chat_models import -----  # Import OpenAI's chat model class\n",
        "from langchain_huggingface import -----  # Import Hugging Face endpoint class\n",
        "\n",
        "# ‚úÖ Step 1: Define a system message (AI's role/behavior)\n",
        "system_message = ----- (content=\"You are a healthcare consultant. Provide insights on improving hospital efficiency.\")  # Correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define a human message (user's question)\n",
        "human_message = ----- (content=\"What strategies can hospitals implement to reduce patient wait times?\")  # Correct class\n",
        "\n",
        "# ‚úÖ Step 3: Initialize models\n",
        "llms = {\n",
        "    \"GPT-4\": ----- (model_name=\"gpt-4\"),  # Correct model initialization for OpenAI\n",
        "    \"Falcon\": ----- (repo_id=\"tiiuae/falcon-7b-instruct\"),  # Hugging Face model for Falcon\n",
        "    \"Zephyr\": ----- (repo_id=\"HuggingFaceH4/zephyr-7b-alpha\"),  # Hugging Face model for Zephyr\n",
        "}\n",
        "\n",
        "# ‚úÖ Step 4: Query each model and store responses\n",
        "responses = {}\n",
        "for model_name, llm in llms.items():\n",
        "    if isinstance(llm, -----):  # Replace with correct class to identify OpenAI models\n",
        "        response = llm.----- ([system_message, human_message])  # Replace with correct method\n",
        "        responses[model_name] = response.-----  # Extract the content for OpenAI models\n",
        "    elif isinstance(llm, -----):  # Replace with correct class to identify Hugging Face models\n",
        "        prompt = f\"{system_message.content} {human_message.content}\"\n",
        "        responses[model_name] = llm.----- (prompt)  # Replace with correct method for Hugging Face\n",
        "\n",
        "# ‚úÖ Step 5: Display all model responses\n",
        "for model, response in responses.items():\n",
        "    print(f\"üîπ {model} Response:\", response)\n"
      ],
      "metadata": {
        "id": "YvS3hJQSMRX1"
      },
      "id": "YvS3hJQSMRX1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Why is Prompting Different in OpenAI vs. Hugging Face?**  \n",
        "\n",
        "### üîπ **OpenAI (`ChatOpenAI`) Uses Structured Messages**\n",
        "- Requires **role-based prompts** (`SystemMessage`, `HumanMessage`).\n",
        "- Designed for **multi-turn conversations** with memory.\n",
        "```python\n",
        "response = llm.invoke([\n",
        "    SystemMessage(content=\"You are a healthcare consultant.\"),\n",
        "    HumanMessage(content=\"How can hospitals reduce patient wait times?\")\n",
        "])\n",
        "print(response.content)  # Extract response\n",
        "\n",
        "### üîπ **Hugging Face (`HuggingFaceEndpoint`) Uses Plain Text**\n",
        "- Models expect a **single text prompt**, no structured roles.\n",
        "- Typically **stateless**, meaning no conversation history.\n",
        "\n",
        "```python\n",
        "prompt = \"You are a healthcare consultant. How can hospitals reduce wait times?\"\n",
        "response = llm.invoke(prompt)  # Direct plain text input\n",
        "print(response)  # No `.content` needed\n"
      ],
      "metadata": {
        "id": "Kw7h8DV5M9v4"
      },
      "id": "Kw7h8DV5M9v4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You have successfully completed the **Introduction to LangChain** lab. üöÄ  \n",
        "We hope you found it insightful and are excited to explore more!  \n",
        "\n",
        "üí° **Next Steps:**  \n",
        "- Try using different LLMs in LangChain.  \n",
        "- Experiment with structured vs. plain text prompts.  \n",
        "- Explore advanced features like memory and chains.\n",
        "\n",
        "Happy Coding! üíª‚ú®  \n"
      ],
      "metadata": {
        "id": "BqzZxacjPK6k"
      },
      "id": "BqzZxacjPK6k"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}