{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/LangChain_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "# 🧠 **LangChain Lab 2: Prompt Templates & Memory**  \n",
        "- **Prof. Dehghani (m.dehghani@northeastern.edu)**  \n",
        "\n",
        "## ** Lab Overview**  \n",
        "This lab focuses on **enhancing interactions with LLMs** by leveraging **Prompt Templates** and **Memory** in LangChain.  \n",
        "You'll learn how to create **structured prompts dynamically** and maintain **conversation history** across multiple interactions.  \n",
        "\n",
        "---\n",
        "\n",
        "## **🎯 What You'll Learn in This Lab**\n",
        "In this session, you'll explore:  \n",
        "- 🔹 **Prompt Templates** → How to format inputs dynamically for LLMs.  \n",
        "- 🔹 **Memory in LangChain** → How to maintain context in multi-turn conversations.  \n",
        "- 🔹 **Hands-on exercises** → Reinforce concepts with practical coding tasks.  \n",
        "\n",
        "By the end of this lab, you’ll be able to structure prompts effectively and implement **conversational memory** in LangChain applications. 🚀  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# 📌 Installing Required Libraries\n",
        "# ==================================================\n",
        "\n",
        "!pip install langchain  # Core framework for working with LLMs\n",
        "!pip install langchain-community  # Install the community package containing LLMs\n",
        "!pip install openai==0.28  # OpenAI API package (version 0.28) for GPT models\n",
        "!pip install langchain-huggingface  # Hugging Face LLM wrapper\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# 📌 Importing Required Libraries for LangChain Lab\n",
        "# ==================================================\n",
        "\n",
        "import os  # For setting environment variables, such as API keys\n",
        "import ipywidgets as widgets  # For creating interactive input widgets in Jupyter or Colab\n",
        "from langchain.llms import OpenAI  # LangChain's wrapper for interacting with OpenAI LLMs\n",
        "import transformers  # Hugging Face library for loading and using pre-trained open-source models\n",
        "import torch  # PyTorch library, required for running and training deep learning models\n",
        "from IPython.display import clear_output, display  # For clearing and managing outputs in Jupyter or Colab notebooks\n",
        "\n",
        "\n",
        "# Imports successful if no errors occur\n",
        "print(\"✅ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4e47a7",
      "metadata": {
        "id": "2d4e47a7"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# 🔑 OpenAI & Hugging Face API Key Setup with Output Clearing\n",
        "# ==================================================\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# ✅ Create input widgets for both API keys\n",
        "openai_key_input = widgets.Password(\n",
        "    description=\"🔑 OpenAI Key:\",\n",
        "    placeholder=\"Enter your OpenAI API Key\",\n",
        ")\n",
        "\n",
        "huggingface_key_input = widgets.Password(\n",
        "    description=\"🤗 HF Key:\",\n",
        "    placeholder=\"Enter your Hugging Face API Key\",\n",
        ")\n",
        "\n",
        "# ✅ Create a button to submit both API keys\n",
        "submit_button = widgets.Button(description=\"✅ Set API Keys\")\n",
        "\n",
        "# ✅ Function to save both API keys when the button is clicked\n",
        "def set_api_keys(b):\n",
        "    # Clear previous outputs\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Display the input fields and button again\n",
        "    display(openai_key_input, huggingface_key_input, submit_button)\n",
        "\n",
        "    # Retrieve and validate API keys\n",
        "    openai_key = openai_key_input.value.strip()\n",
        "    hf_key = huggingface_key_input.value.strip()\n",
        "\n",
        "    # ✅ Set OpenAI API Key\n",
        "    if openai_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        openai.api_key = openai_key\n",
        "        print(\"✅ OpenAI API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"❌ Please enter a valid OpenAI API Key.\")\n",
        "\n",
        "    # ✅ Set Hugging Face API Key\n",
        "    if hf_key:\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key\n",
        "        print(\"✅ Hugging Face API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"❌ Please enter a valid Hugging Face API Key.\")\n",
        "\n",
        "# ✅ Link button click to the function\n",
        "submit_button.on_click(set_api_keys)\n",
        "\n",
        "# ✅ Display the input fields and button\n",
        "display(openai_key_input, huggingface_key_input, submit_button)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5047273f",
      "metadata": {
        "id": "5047273f"
      },
      "source": [
        "# 📝 **Prompt Templates in LangChain**  \n",
        "\n",
        "### 🔹 **What are Prompt Templates?**  \n",
        "Prompt Templates allow us to **dynamically format prompts** by inserting variables, making interactions with LLMs **more flexible and reusable**. Instead of hardcoding static text, we can use placeholders that get replaced with actual values at runtime.  \n",
        "\n",
        "### 🔹 **Why Use Prompt Templates?**  \n",
        "- ✅ **Reusability** → Avoid writing repetitive prompts.  \n",
        "- ✅ **Dynamic Inputs** → Easily customize prompts with different user inputs.  \n",
        "- ✅ **Consistency** → Ensures a structured approach to formatting queries.  \n",
        "\n",
        "### **📌 Example Usage**  \n",
        "A **static prompt**:  \n",
        "> `\"What are the benefits of AI in healthcare?\"`  \n",
        "\n",
        "A **dynamic prompt using a template**:  \n",
        "> `\"What are the benefits of {technology} in {industry}?\"`  \n",
        "- If `{technology} = \"AI\"` and `{industry} = \"healthcare\"`, the generated prompt becomes:  \n",
        "  `\"What are the benefits of AI in healthcare?\"`  \n",
        "\n",
        "🚀 **Let's get started with the first example!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da3cc1a",
      "metadata": {
        "id": "7da3cc1a"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# 🎯 Using Prompt Templates with OpenAI (GPT-4)\n",
        "# ==================================================\n",
        "\n",
        "# ✅ Import required classes\n",
        "# 'PromptTemplate' Allows dynamic formatting of prompts by inserting variables, enabling reusable and structured interactions with LLMs.\n",
        "from langchain.prompts import PromptTemplate  # Template system for formatting prompts\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI LLM wrapper\n",
        "\n",
        "# ✅ Step 1: Define a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],  # Variables to be replaced dynamically\n",
        "    template=\"What are the benefits of {technology} in {industry} in 3 short bullets?\"  # Defines the structure of the prompt\n",
        ")\n",
        "\n",
        "# ✅ Step 2: Format the prompt dynamically\n",
        "formatted_prompt = prompt_template.format(technology=\"AI\", industry=\"healthcare\")\n",
        "\n",
        "# ✅ Step 3: Initialize the OpenAI model (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")  # Using OpenAI's GPT-4 model\n",
        "\n",
        "# ✅ Step 4: Generate the response\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)  # Sends the formatted prompt to GPT-4\n",
        "\n",
        "# ✅ Step 5: Display results\n",
        "print(\"🔹 **Generated Prompt:**\", formatted_prompt)\n",
        "print(\"🔹 **LLM Response:**\", response_ChatGPT.content)  # Extract `.content` for OpenAI responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbb5ec1",
      "metadata": {
        "id": "6cbb5ec1"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# ✋ **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# ==================================================\n",
        "\n",
        "# 📌 **Task Instructions:**\n",
        "# 1️⃣ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2️⃣ Ensure the Prompt Template correctly replaces {topic} and {context}.\n",
        "# 3️⃣ Run the code and verify GPT-4 generates a response.\n",
        "\n",
        "# ✅ Step 1: Define a Prompt Template\n",
        "prompt_template = -----(\n",
        "    input_variables=[\"-----\", \"context\"],  # Fill in the missing variable name\n",
        "    template=\"How does {topic} impact {context} in a few words?\"  # Structure of the prompt\n",
        ")\n",
        "\n",
        "# ✅ Step 2: Format the prompt with actual values\n",
        "formatted_prompt = prompt_template.----- (topic=\"Machine Learning\", -----=\"business analytics\")\n",
        "\n",
        "# ✅ Step 3: Generate a response using OpenAI (GPT-4)\n",
        "llm_ChatGPT = llm.----- (model_name=\"gpt-4\")  # Initialize the ChatGPT model\n",
        "response_ChatGPT = llm_ChatGPT.invoke(-----)  # Fill in the correct variable for invoke\n",
        "\n",
        "# ✅ Step 4: Display results\n",
        "print(\"🔹 **Generated Prompt:**\", formatted_prompt)\n",
        "print(\"🔹 **LLM Response:**\", response_ChatGPT.-----)  # Extract response content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 🔄 **Using Prompt Templates in a Loop**\n",
        "# ==================================================\n",
        "\n",
        "# ✅ Step 1: Define a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],  # Variables for dynamic input\n",
        "    template=\"In one sentence, how does {technology} impact {industry}?\"  # Keeps response short\n",
        ")\n",
        "\n",
        "# ✅ Step 2: Initialize the OpenAI model\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")  # Using OpenAI's GPT-4 model\n",
        "\n",
        "# ✅ Step 3: Define multiple input values for the loop\n",
        "input_data = [\n",
        "    {\"technology\": \"AI\", \"industry\": \"education\"},\n",
        "    {\"technology\": \"Blockchain\", \"industry\": \"finance\"},\n",
        "    {\"technology\": \"5G\", \"industry\": \"telecommunications\"},\n",
        "]\n",
        "\n",
        "# ✅ Step 4: Loop through different input values and generate responses\n",
        "for data in input_data:\n",
        "    formatted_prompt = prompt_template.format(**data)  # Dynamically format the prompt\n",
        "    response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)  # Generate response\n",
        "\n",
        "    # ✅ Step 5: Display results\n",
        "    print(f\"🔹 **Prompt:** {formatted_prompt}\")\n",
        "    print(f\"💡 **Response:** {response_ChatGPT.content}\")  # Extract and display response\n",
        "    print(\"-\" * 60)  # Separator for readability\n"
      ],
      "metadata": {
        "id": "MQE2mEke-6_w"
      },
      "id": "MQE2mEke-6_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3c9bdf18",
      "metadata": {
        "id": "3c9bdf18"
      },
      "source": [
        "# 🧠 **Understanding Memory in LangChain**\n",
        "\n",
        "## 🔹 **What is Memory in LangChain?**\n",
        "By default, LLMs **do not remember past interactions**.  \n",
        "LangChain **Memory** allows an AI model to **retain context** across multiple turns, enabling more natural, conversational interactions.  \n",
        "\n",
        "## 🔹 **Why Use Memory?**\n",
        "✅ **Maintains conversation history** → AI can recall previous exchanges.  \n",
        "✅ **Improves response coherence** → Reduces redundant user re-explanations.  \n",
        "✅ **Essential for chatbots & agents** → Allows multi-turn dialogue without loss of context.  \n",
        "\n",
        "## 🔹 **Types of Memory in LangChain**\n",
        "LangChain provides **various types of memory**, including:  \n",
        "1️⃣ **ConversationBufferMemory** → Stores messages in a buffer (basic memory).  \n",
        "2️⃣ **ConversationSummaryMemory** → Summarizes past interactions instead of storing all messages.  \n",
        "3️⃣ **ConversationBufferWindowMemory** → Retains only the last N interactions for efficiency.  \n",
        "4️⃣ **Vector-based Memory** → Uses embeddings for advanced retrieval of past conversations.  \n",
        "\n",
        "## **🚀 What We'll Do in This Lab**\n",
        "We’ll explore **ConversationBufferMemory** first, which allows an LLM to **recall past messages** and interact in a more natural, memory-enhanced way.  \n",
        "\n",
        "Let's get started! 👇  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225bb6e6",
      "metadata": {
        "id": "225bb6e6"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# 🔢 **Using Memory in LangChain: Math Example**\n",
        "# ==================================================\n",
        "#\n",
        "# This script demonstrates how to use memory in LangChain for multi-turn conversations.\n",
        "# It initializes a conversation with memory and interacts with GPT-4 to show that it\n",
        "# can recall previous user inputs and continue reasoning based on past responses.\n",
        "\n",
        "\n",
        "# ✅ Import required classes\n",
        "from langchain.memory import ConversationBufferMemory  # Memory system for storing conversation history\n",
        "from langchain.chains import ConversationChain  # Handles multi-turn conversations\n",
        "from langchain.chat_models import ChatOpenAI  # Import ChatOpenAI to use GPT models\n",
        "\n",
        "# ✅ Step 1: Initialize Memory\n",
        "memory = ConversationBufferMemory()  # Stores conversation history\n",
        "\n",
        "# ✅ Step 2: Initialize ChatGPT with Memory\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")  # Using GPT-4 model\n",
        "\n",
        "# ✅ Step 3: Initialize Conversation Chain\n",
        "# This step is crucial as it links the language model with memory, enabling it to retain\n",
        "# and recall past interactions, creating a coherent multi-turn conversation.\n",
        "conversation = ConversationChain(llm=llm, memory=memory)  # Attaching memory\n",
        "\n",
        "# ✅ Step 4: Run Multiple Interactions\n",
        "# The `conversation.predict()` method is crucial because it retrieves responses\n",
        "# while maintaining context from previous exchanges stored in memory.\n",
        "# This enables the model to remember past interactions and generate relevant answers.\n",
        "\n",
        "print(\"\\n💬 **User:** What is 15 + 27?\")\n",
        "response = conversation.predict(input=\"What is 15 + 27?\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n",
        "\n",
        "print(\"\\n💬 **User:** Now multiply that result by 3.\")\n",
        "response = conversation.predict(input=\"Now multiply that result by 3.\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n",
        "\n",
        "print(\"\\n💬 **User:** What was my first question?\")\n",
        "response = conversation.predict(input=\"What was my first question?\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔢 **Using Memory in LangChain: Math Example (Hugging Face)**\n"
      ],
      "metadata": {
        "id": "AwCIzy1lE7lh"
      },
      "id": "AwCIzy1lE7lh"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 🔢 **Using Memory in LangChain: Math Example (Hugging Face)**\n",
        "# ==================================================\n",
        "\n",
        "# ✅ Import required libraries\n",
        "import os\n",
        "from langchain.memory import ConversationBufferMemory  # Stores conversation history\n",
        "from langchain.chains import ConversationChain  # Handles multi-turn conversations\n",
        "from langchain.chat_models import ChatOpenAI  # Chat model integration\n",
        "from langchain.llms import HuggingFaceHub  # Hugging Face Model integration\n",
        "\n",
        "# ✅ Step 1: Set Hugging Face API Token (Ensure this is set securely beforehand)\n",
        "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "    raise ValueError(\"Please set your Hugging Face API token as an environment variable.\")\n",
        "\n",
        "# ✅ Step 2: Initialize the Hugging Face model\n",
        "llm_HF = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"temperature\": 0.5})  # Falcon model\n",
        "\n",
        "# ✅ Step 3: Initialize Memory\n",
        "memory = ConversationBufferMemory()  # Stores conversation history\n",
        "\n",
        "# ✅ Step 4: Attach memory to a conversation chain\n",
        "conversation = ConversationChain(llm=llm_HF, memory=memory)  # Attaching memory\n",
        "\n",
        "# ✅ Step 5: Run Multiple Interactions\n",
        "print(\"\\n💬 **User:** What is 12 + 8?\")\n",
        "response_HF = conversation.predict(input=\"What is 12 + 8?\")\n",
        "print(\"🤖 **Falcon Model:**\", response_HF)\n",
        "\n",
        "print(\"\\n💬 **User:** Now subtract 5 from that result.\")\n",
        "response_HF = conversation.predict(input=\"Now subtract 5 from that result.\")\n",
        "print(\"🤖 **Falcon Model:**\", response_HF)\n",
        "\n",
        "print(\"\\n💬 **User:** What was my first question?\")\n",
        "response_HF = conversation.predict(input=\"What was my first question?\")\n",
        "print(\"🤖 **Falcon Model:**\", response_HF)\n"
      ],
      "metadata": {
        "id": "_k2Td8K7Bvlf"
      },
      "id": "_k2Td8K7Bvlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 🔎 **Search for Hugging Face Models**\n",
        "# ==================================================\n",
        "\n",
        "import requests  # To send API requests\n",
        "import json  # For handling responses\n",
        "\n",
        "# ✅ Step 1: Define the Hugging Face API URL\n",
        "HF_API_URL = \"https://huggingface.co/api/models\"\n",
        "\n",
        "# ✅ Step 2: Define search parameters\n",
        "query = \"chat\"  # Change this to search for different models (e.g., \"math\", \"finance\", \"healthcare\")\n",
        "params = {\"search\": query, \"limit\": 10}  # Adjust limit as needed\n",
        "\n",
        "# ✅ Step 3: Send a request to Hugging Face Model Hub\n",
        "response = requests.get(HF_API_URL, params=params)\n",
        "\n",
        "# ✅ Step 4: Display results\n",
        "if response.status_code == 200:\n",
        "    models = response.json()\n",
        "    print(f\"🔍 **Top {len(models)} Models Matching '{query}':**\")\n",
        "    for model in models:\n",
        "        print(f\"🔹 {model['modelId']} ➝ {model['pipeline_tag']}\")  # Show model ID and type\n",
        "else:\n",
        "    print(\"❌ Failed to retrieve models. Check your internet connection.\")\n"
      ],
      "metadata": {
        "id": "xn4wf7jTIBXN"
      },
      "id": "xn4wf7jTIBXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "181b747e",
      "metadata": {
        "id": "181b747e"
      },
      "source": [
        "✋ Hands-On 1: Memory with ChatGPT (OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc20167",
      "metadata": {
        "id": "9dc20167"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# ✋ **Hands-On: Using Memory with OpenAI (Beer Game - Supply Chain Predictions)**\n",
        "# ==================================================\n",
        "#\n",
        "# 📌 **Task Instructions:**\n",
        "# 1️⃣ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2️⃣ Ensure the AI remembers previous demand data and predicts future order quantities.\n",
        "# 3️⃣ Run the code and check if ChatGPT maintains context for supply chain decisions.\n",
        "\n",
        "# ✅ Step 1: Initialize Memory\n",
        "memory = -----()  # Initialize the correct memory class\n",
        "\n",
        "# ✅ Step 2: Initialize ChatGPT with Memory\n",
        "llm_ChatGPT = ----- (model_name=\"gpt-4\")  # Initialize ChatGPT model\n",
        "conversation = ----- (llm=llm_ChatGPT, memory=memory)  # Attach memory to conversation\n",
        "\n",
        "# ✅ Step 3: Run Multiple Interactions\n",
        "print(\"\\n💬 **Retailer:** Last week, the customer demand was 200 units. What should I order this week?\")\n",
        "response_ChatGPT = conversation.----- (input=\"Last week, the customer demand was 200 units. What should I order this week?\")  # Call the correct method\n",
        "print(\"🤖 **ChatGPT:**\", response_ChatGPT)\n",
        "\n",
        "print(\"\\n💬 **Retailer:** If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "response_ChatGPT = conversation.----- (input=\"If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "print(\"🤖 **ChatGPT:**\", response_ChatGPT)\n",
        "\n",
        "print(\"\\n💬 **Retailer:** What was the demand I mentioned last week?\")\n",
        "response_ChatGPT = conversation.----- (input=\"What was the demand I mentioned last week?\")\n",
        "print(\"🤖 **ChatGPT:**\", response_ChatGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using 'Summarized Conversation' Example\n"
      ],
      "metadata": {
        "id": "3ihDbUh4E5Sn"
      },
      "id": "3ihDbUh4E5Sn"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 🎤 **Using Memory in LangChain: Job Interview Prep**\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a job interview practice session.\n",
        "# It uses ConversationSummaryMemory to retain key points from previous exchanges\n",
        "# rather than storing the full conversation history.\n",
        "\n",
        "# ✅ Import required classes\n",
        "from langchain.memory import ConversationSummaryMemory  # Summarized conversation memory\n",
        "\n",
        "# ✅ Step 1: Initialize Memory\n",
        "# This memory will maintain a **summarized** version of the conversation.\n",
        "memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=\"gpt-4\"))\n",
        "\n",
        "# ✅ Step 2: Initialize ChatGPT with Memory\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")  # Using GPT-4 model\n",
        "\n",
        "# ✅ Step 3: Initialize Conversation Chain\n",
        "# The model will summarize key details from the job interview practice.\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "# ✅ Step 4: Conduct the Interview Simulation\n",
        "\n",
        "print(\"\\n💬 **User:** Can you ask me a common interview question?\")\n",
        "response = conversation.predict(input=\"Can you ask me a common interview question?\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n",
        "\n",
        "# ✅ Check memory after first interaction\n",
        "print(\"\\n📜 **Memory Summary After 1st Question:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\n💬 **User:** My biggest strength is adaptability and problem-solving.\")\n",
        "response = conversation.predict(input=\"My biggest strength is adaptability and problem-solving.\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n",
        "\n",
        "# ✅ Check memory after user shares strength\n",
        "print(\"\\n📜 **Memory Summary After Strength Response:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\n💬 **User:** My biggest weakness is that I sometimes overthink decisions.\")\n",
        "response = conversation.predict(input=\"My biggest weakness is that I sometimes overthink decisions.\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n",
        "\n",
        "# ✅ Check memory after user shares weakness\n",
        "print(\"\\n📜 **Memory Summary After Weakness Response:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\n💬 **User:** Can you summarize what we discussed so far?\")\n",
        "response = conversation.predict(input=\"Can you summarize what we discussed so far?\")\n",
        "print(\"🤖 **ChatGPT:**\", response)\n",
        "\n",
        "# ✅ Final Memory Check\n",
        "print(\"\\n📜 **Final Memory Summary:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n"
      ],
      "metadata": {
        "id": "PGhKAmB-E3vm"
      },
      "id": "PGhKAmB-E3vm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2S8V5RarFUQy"
      },
      "id": "2S8V5RarFUQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 🍺 **LangChain Beer Game: Comparing Memory Types (Fixed)**\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a Beer Game ordering process over 6 weeks.\n",
        "# It uses:\n",
        "# 1️⃣ ConversationBufferMemory (Tracks full history)\n",
        "# 2️⃣ ConversationBufferWindowMemory (Tracks only last 3 orders)\n",
        "#\n",
        "# The AI predicts the next order quantity based on past interactions.\n",
        "\n",
        "# ✅ Import required libraries\n",
        "import pandas as pd\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ✅ Step 1: Initialize Memory Types\n",
        "buffer_memory = ConversationBufferMemory(return_messages=True)  # Stores entire history\n",
        "window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)  # Stores last 3 interactions\n",
        "\n",
        "# ✅ Step 2: Initialize Chat Model\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# ✅ Step 3: Define a Prompt Template\n",
        "beer_game_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are managing a supply chain for a beer distribution system.\n",
        "    Orders fluctuate at first but stabilize later.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on past orders, what should be the next order quantity?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ✅ Step 4: Define Processing Pipelines\n",
        "buffer_chain = beer_game_template | llm\n",
        "window_chain = beer_game_template | llm\n",
        "\n",
        "# ✅ Step 5: Define Order Fluctuations (First 3 weeks volatile, last 3 weeks stable)\n",
        "weekly_orders = [20, 50, 10, 25, 30, 30]  # Example fluctuations\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# ✅ Step 6: Run the Simulation\n",
        "for week in range(1, len(weekly_orders) + 1):\n",
        "    prev_orders = \", \".join(map(str, weekly_orders[:week]))  # Orders seen so far\n",
        "    context = f\"Week {week}: The previous orders were {prev_orders}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "    window_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "\n",
        "    # Get AI predictions using RunnableSequence\n",
        "    buffer_prediction = buffer_chain.invoke({\"context\": context})\n",
        "    window_prediction = window_chain.invoke({\"context\": context})\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# ✅ Step 7: Display Results in a Table\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(weekly_orders) + 1)),\n",
        "    \"Actual Orders\": weekly_orders,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n"
      ],
      "metadata": {
        "id": "uFTXdRyVFUNC"
      },
      "id": "uFTXdRyVFUNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Save the table to an Excel file\n",
        "df.to_excel(\"beer_game_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# ✅ Print confirmation message\n",
        "print(\"Data saved to 'beer_game_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "tlLyE0DYB6Qt"
      },
      "id": "tlLyE0DYB6Qt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 **Assignment: AI Stock Market Trend Prediction with Memory**\n",
        "\n",
        "## **Objective**\n",
        "In this assignment, you will use AI to predict stock market trends based on historical stock prices. You will compare how different memory types affect AI's ability to track and predict future trends.\n",
        "\n",
        "## **Tasks**\n",
        "1. **Initialize memory types** (`ConversationBufferMemory` and `ConversationBufferWindowMemory`).\n",
        "2. **Define the AI model** (GPT-4 or another suitable model).\n",
        "3. **Complete the prompt template** to guide AI predictions.\n",
        "4. **Process stock price data** and use memory to store past trends.\n",
        "5. **Retrieve and analyze stored memory** after each step.\n",
        "6. **Invoke the AI model correctly** to generate predictions.\n",
        "7. **Save results to an Excel file** for analysis.\n",
        "\n",
        "## **Expected Outcome**\n",
        "You will observe how AI predictions change when it has full history vs. limited memory. This will help you understand the impact of memory in AI-based forecasting.\n",
        "\n",
        "🚀 **Complete the placeholders and run the script to generate insights!** 🚀\n"
      ],
      "metadata": {
        "id": "r9Y518a4MeVJ"
      },
      "id": "r9Y518a4MeVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 📈 **AI Assignment: Stock Market Trend Prediction with Memory**\n",
        "# ==================================================\n",
        "#\n",
        "# 🔹 In this assignment, you will use AI to predict stock market trends.\n",
        "# 🔹 You will compare how different memory types affect AI's ability to track stock price movements.\n",
        "# 🔹 Complete the placeholders (----) to make the script functional.\n",
        "#\n",
        "# 📌 **Your Tasks:**\n",
        "# 1️⃣ Initialize the correct memory types.\n",
        "# 2️⃣ Define the AI model.\n",
        "# 3️⃣ Complete the template prompt.\n",
        "# 4️⃣ Use memory correctly when processing stock data.\n",
        "# 5️⃣ Ensure correct invocation of AI for predictions.\n",
        "# 6️⃣ Retrieve and analyze stored memory.\n",
        "# 7️⃣ Save results in an Excel file.\n",
        "\n",
        "# ✅ Import required libraries\n",
        "import pandas as pd\n",
        "from langchain.memory import ----  # Import appropriate memory classes\n",
        "from langchain.chat_models import ----  # Import ChatGPT model\n",
        "from langchain.prompts import ----  # Import PromptTemplate\n",
        "\n",
        "# ✅ Step 1: Initialize Memory Types\n",
        "buffer_memory = ----  # Stores full stock history\n",
        "window_memory = ----  # Stores last 3 stock movements\n",
        "\n",
        "# ✅ Step 2: Initialize Chat Model\n",
        "llm = ----  # Define the AI model (GPT-4 or another model)\n",
        "\n",
        "# ✅ Step 3: Define a Prompt Template\n",
        "stock_prediction_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI financial analyst predicting stock market trends.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on this stock price history, what will be the next trend (Up, Down, or Stable)?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ✅ Step 4: Define Processing Pipelines\n",
        "buffer_chain = ----  # Define how memory connects to AI\n",
        "window_chain = ----  # Define how memory connects to AI with windowed memory\n",
        "\n",
        "# ✅ Step 5: Define Stock Price Data (Fluctuations in the first weeks, then stabilizing)\n",
        "stock_prices = [120, 125, 110, 130, 128, 129]  # Example price movements\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# ✅ Step 6: Run the Prediction Simulation\n",
        "for week in range(1, len(stock_prices) + 1):\n",
        "    prev_prices = \", \".join(map(str, stock_prices[:week]))  # Stocks seen so far\n",
        "    context = f\"Week {week}: The previous stock prices were {prev_prices}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.----  # Store context in buffer memory\n",
        "    window_memory.----  # Store context in windowed memory\n",
        "\n",
        "    # Get AI predictions\n",
        "    buffer_prediction = buffer_chain.----  # Invoke AI for buffer memory\n",
        "    window_prediction = window_chain.----  # Invoke AI for window memory\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# ✅ Step 7: Save Results in an Excel File\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(stock_prices) + 1)),\n",
        "    \"Stock Price\": stock_prices,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n",
        "\n",
        "df.to_excel(\"stock_market_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# ✅ Print confirmation message\n",
        "print(\"Assignment completed! Data saved to 'stock_market_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "TGapDKV0HJWg"
      },
      "id": "TGapDKV0HJWg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}