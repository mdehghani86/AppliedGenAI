{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/LangChain_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "# üîó **LangChain Lab 4 Retrieval-Augmented Generation (RAG)s**  \n",
        "- **Prof. Dehghani (m.dehghani@northeastern.edu)**  \n",
        "\n",
        "## üìñ Introduction to RAG\n",
        "\n",
        "### üîπ What is Retrieval-Augmented Generation (RAG)?\n",
        "Retrieval-Augmented Generation (**RAG**) is a technique that enhances Large Language Models (LLMs) by retrieving external knowledge before generating a response. Instead of relying solely on a model's pretrained knowledge, RAG fetches relevant **documents, database entries, or structured data** to improve accuracy.\n",
        "\n",
        "### üöÄ **Why Use RAG?**\n",
        "Traditional LLMs have limitations:\n",
        "‚úÖ **Limited Knowledge** ‚Äì LLMs can‚Äôt update their training data dynamically.  \n",
        "‚úÖ **Hallucinations** ‚Äì Models sometimes generate incorrect or fabricated information.  \n",
        "‚úÖ **Domain-Specific Needs** ‚Äì For specialized fields like **finance, law, or medicine**, retrieval ensures better accuracy.\n",
        "\n",
        "**RAG solves these issues by combining retrieval and generation**, allowing models to fetch relevant knowledge on demand.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è **How Does RAG Work?**\n",
        "RAG consists of two main steps:\n",
        "\n",
        "1Ô∏è‚É£ **Retrieval:** The system **searches for relevant information** in a knowledge source (e.g., vector database, documents).  \n",
        "2Ô∏è‚É£ **Generation:** The retrieved information is **passed as context** to an LLM, which generates a response based on both its knowledge and the retrieved data.\n",
        "\n",
        "üìå **Example Use Case**: A chatbot answering questions about **company policies** can use RAG to pull up official policy documents instead of relying only on pre-trained responses.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¨ **Comparison: Traditional LLM vs. RAG**\n",
        "| Feature | Traditional LLM | RAG-Enhanced LLM |\n",
        "|---------|----------------|------------------|\n",
        "| **Knowledge Source** | Fixed (Training Data) | Dynamic (Retrieval + LLM) |\n",
        "| **Updates** | Requires Retraining | Can Fetch New Information |\n",
        "| **Risk of Hallucinations** | High | Reduced |\n",
        "| **Domain-Specific Adaptability** | Limited | Highly Adaptable |\n",
        "\n",
        "---\n",
        "### üèóÔ∏è **Next Step: Setting Up the Environment**\n",
        "In the next section, we will **install required libraries** and set up our workspace for building a **RAG pipeline in Google Colab**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üìå Installing Required Libraries\n",
        "# ==================================================\n",
        "!pip install langchain langchain-community  # Core LangChain framework & community package\n",
        "!pip install openai==0.28  # OpenAI API package (version 0.28) for GPT models\n",
        "\n",
        "# Additional libraries for RAG (FAISS, ChromaDB, Tokenization, and Unstructured Data Processing)\n",
        "!pip install faiss-cpu chromadb tiktoken unstructured\n",
        "\n",
        "!pip install \"unstructured[pdf]\" pypdf pdfminer.six\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üìå Importing Required Libraries for LangChain RAG Lab\n",
        "# ==================================================\n",
        "\n",
        "# ‚úÖ System & Environment Setup\n",
        "import os  # For setting environment variables, such as API keys\n",
        "\n",
        "# ‚úÖ Jupyter & Colab Utilities\n",
        "import ipywidgets as widgets  # For creating interactive input widgets\n",
        "from IPython.display import clear_output, display  # For managing notebook outputs\n",
        "\n",
        "# ‚úÖ OpenAI API\n",
        "import openai  # Direct interaction with OpenAI API (useful for API-based calls)\n",
        "\n",
        "# ‚úÖ LangChain Core Components\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI chat models (GPT)\n",
        "from langchain.llms import OpenAI  # OpenAI LLM wrapper\n",
        "from langchain.prompts import PromptTemplate  # Structured prompt templates\n",
        "from langchain.memory import ConversationBufferMemory  # Maintaining conversation history\n",
        "\n",
        "# ‚úÖ RAG-Specific LangChain Imports\n",
        "from langchain.vectorstores import FAISS  # FAISS for fast retrieval\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings  # OpenAI embeddings for vector search\n",
        "from langchain.chains import RetrievalQA  # Prebuilt RAG pipeline in LangChain\n",
        "from langchain.document_loaders import TextLoader  # Loading documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splitting text into chunks\n",
        "\n",
        "# ‚úÖ Alternative Embeddings (Hugging Face)\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # HF embeddings for local models\n",
        "import transformers  # Hugging Face Transformers\n",
        "\n",
        "# ‚úÖ Confirmation message\n",
        "print(\"‚úÖ All required libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4e47a7",
      "metadata": {
        "id": "2d4e47a7"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üîë OpenAI API Key Setup from Colab Secrets\n",
        "# ==================================================\n",
        "\n",
        "# ‚úÖ Retrieve OpenAI API Key from Colab's Secret Storage\n",
        "try:\n",
        "    from google.colab import userdata  # Import Colab's secret storage\n",
        "    openai_key = userdata.get('OpenAI_Key')  # Retrieve key from Colab Secrets\n",
        "\n",
        "    if openai_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        print(\"‚úÖ OpenAI API Key has been set successfully from Colab Secrets!\")\n",
        "    else:\n",
        "        print(\"‚ùå OpenAI API Key not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving OpenAI API Key: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Case Study: OpenAI's Marketing Strategy ‚Äì RAG vs. Non-RAG\n",
        "\n",
        "## üéØ Objective\n",
        "This case study evaluates how **Retrieval-Augmented Generation (RAG)** improves AI-generated responses for a business use case. We analyze OpenAI's **marketing strategy**, first using a standard LLM (**without RAG**) and then incorporating **retrieved external data (RAG)** to enhance the answer.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Approach\n",
        "We structured our experiment in **four key steps**:\n",
        "\n",
        "1. **Non-RAG Query:**  \n",
        "   - Asked OpenAI's GPT model: *\"What is OpenAI's marketing strategy?\"*  \n",
        "   - The model relied **only on its pretrained knowledge**, potentially outdated.  \n",
        "\n",
        "2. **Loading External Knowledge:**  \n",
        "   - Uploaded a **marketing-related PDF from Dropbox** to provide fresh, structured data.  \n",
        "   - Split the document into **smaller retrievable text chunks** using LangChain.  \n",
        "\n",
        "3. **Embedding & Retrieval:**  \n",
        "   - Converted document chunks into **vector embeddings** (FAISS).  \n",
        "   - Set up a **retriever** to fetch relevant context dynamically.  \n",
        "\n",
        "4. **RAG-Based Query:**  \n",
        "   - Asked the same question, but now the model **retrieved** relevant document excerpts.  \n",
        "   - AI generated a **more informed and factually grounded** response.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìä Comparison: Non-RAG vs. RAG Responses\n",
        "\n",
        "| Feature              | ‚ùå **Without RAG** (LLM Only) | ‚úÖ **With RAG** (Retrieved Data) |\n",
        "|----------------------|-----------------------------|----------------------------------|\n",
        "| **Knowledge Source** | Trained model (static)      | External documents (dynamic)    |\n",
        "| **Response Quality** | General & vague            | Specific & data-backed          |\n",
        "| **Up-to-date Info**  | Limited                     | Can retrieve recent data        |\n",
        "| **Risk of Hallucination** | Higher                | Reduced                         |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "O-6iyAWy1t02"
      },
      "id": "O-6iyAWy1t02"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚ùå No RAG: Ask OpenAI About Its Marketing Strategy\n",
        "# ==================================================\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# ‚úÖ Initialize OpenAI Model\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# ‚úÖ Test Query Without RAG\n",
        "query = \"What is OpenAI's marketing strategy?\"\n",
        "response = llm.invoke(query)  # Corrected to use .invoke()\n",
        "\n",
        "# ‚úÖ Display Result\n",
        "print(\"ü§ñ OpenAI's Marketing Strategy (No RAG):\")\n",
        "print(response.content)  # Corrected to use .content\n"
      ],
      "metadata": {
        "id": "QZewsGvez13I"
      },
      "id": "QZewsGvez13I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==================================================\n",
        "# üìå Comparing `RecursiveCharacterTextSplitter` vs. `CharacterTextSplitter`\n",
        "# ==================================================\n",
        "\n",
        "\"\"\"\n",
        "# üìå Comparing `RecursiveCharacterTextSplitter` vs. `CharacterTextSplitter`\n",
        "\n",
        "## üîç Why Use a Text Splitter?\n",
        "Large documents must be broken into smaller, manageable chunks for efficient **retrieval in RAG pipelines**. LangChain provides different text splitters for this purpose.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° `CharacterTextSplitter`\n",
        "**Basic, fast, but limited control.**  \n",
        "‚úÖ Splits text based on a **fixed character limit** (e.g., 500 characters).  \n",
        "‚úÖ Doesn't consider **logical sentence breaks**‚Äîmay split words in half.  \n",
        "‚úÖ **Good for simple text division** without deep structure.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ `RecursiveCharacterTextSplitter`\n",
        "**More advanced & structure-aware.**  \n",
        "‚úÖ Attempts to **split text at logical breakpoints** (e.g., paragraphs, sentences).  \n",
        "‚úÖ Uses a **fallback mechanism**: tries to split by **paragraphs > sentences > words** if possible.  \n",
        "‚úÖ **Better for structured documents** like PDFs, articles, or books.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Summary Table\n",
        "\n",
        "| Feature                        | `CharacterTextSplitter` | `RecursiveCharacterTextSplitter` |\n",
        "|--------------------------------|-------------------------|----------------------------------|\n",
        "| **Splitting Logic**             | Fixed character count   | Tries paragraphs ‚Üí sentences ‚Üí words |\n",
        "| **Maintains Logical Flow?**     | ‚ùå No                   | ‚úÖ Yes |\n",
        "| **Best for PDFs & Long Texts?** | ‚ùå No                   | ‚úÖ Yes |\n",
        "| **Computational Efficiency**    | ‚úÖ Faster               | ‚ö†Ô∏è Slightly slower |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Which One Should You Use?**\n",
        "- **For simple splitting** (e.g., short plain text) ‚Üí Use `CharacterTextSplitter`.  \n",
        "- **For structured documents** (PDFs, articles, books) ‚Üí Use `RecursiveCharacterTextSplitter`.  \n",
        "- **If unsure** ‚Üí Default to `RecursiveCharacterTextSplitter` for better retrieval quality.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "1Hc73ijx_Nrz"
      },
      "id": "1Hc73ijx_Nrz"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üìÇ Download & Load PDF from Dropbox for RAG\n",
        "# ==================================================\n",
        "# This cell fixes the issue with loading PDFs from a Dropbox URL.\n",
        "# Instead of using `UnstructuredURLLoader` (which requires extra dependencies),\n",
        "# we:\n",
        "# ‚úÖ Step 1: Download the PDF from Dropbox and save it locally.\n",
        "# ‚úÖ Step 2: Use `PyPDFLoader` to extract text from the PDF.\n",
        "# ‚úÖ Step 3: Split the extracted text into small chunks for retrieval.\n",
        "# ‚úÖ Step 4: Preview the first few chunks to verify the content.\n",
        "\n",
        "import requests  # Library for downloading files\n",
        "from langchain.document_loaders import PyPDFLoader  # Stable PDF loader for LangChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into smaller parts\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 1: Download the PDF from Dropbox and Save Locally\n",
        "# ==================================================\n",
        "dropbox_url = \"https://www.dropbox.com/scl/fi/wvvef7qxrq36czo4poquc/pdf.pdf?rlkey=yp0sn2f60bjumn7m943hh3o4u&dl=1\"\n",
        "pdf_path = \"/content/document.pdf\"  # Local path to save the downloaded file\n",
        "\n",
        "# Download the file from Dropbox\n",
        "response = requests.get(dropbox_url)\n",
        "with open(pdf_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"‚úÖ PDF Downloaded Successfully!\")\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 2: Load the PDF Using `PyPDFLoader`\n",
        "# ==================================================\n",
        "# PyPDFLoader extracts the text content from the entire PDF document.\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()  # Loads the text into a LangChain-compatible format\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 3: Split Text into Chunks for Better Retrieval\n",
        "# ==================================================\n",
        "# Large text blocks make retrieval inefficient, so we split the text into smaller pieces.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)  # Splits the document into multiple parts\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 4: Preview the First Few Chunks to Ensure Proper Splitting\n",
        "# ==================================================\n",
        "print(f\"‚úÖ Total Chunks: {len(docs)}\")  # Shows how many text chunks were created\n",
        "\n",
        "# Display the first few chunks to verify extraction\n",
        "for i in range(min(3, len(docs))):  # Avoid index error if the document is too short\n",
        "    print(f\"\\nüìú Chunk {i+1}: {docs[i].page_content[:300]}...\")  # Display first 300 chars of each chunk\n"
      ],
      "metadata": {
        "id": "UTtajJuA0FCb"
      },
      "id": "UTtajJuA0FCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Vector Databases & FAISS: Efficient Retrieval in RAG\n",
        "\n",
        "\"\"\"\n",
        "# üîç What Are Vector Databases?\n",
        "Vector databases store and search **high-dimensional embeddings**, allowing AI to find **similar text chunks efficiently**. They are essential for **Retrieval-Augmented Generation (RAG)**, where AI retrieves relevant context before generating responses.\n",
        "\n",
        "## üöÄ Why Use a Vector Database?\n",
        "- üîé **Fast similarity search** for large datasets.\n",
        "- üìñ **Improves accuracy** in AI-generated responses.\n",
        "- ‚ö° **Optimized for large-scale AI applications** (chatbots, search engines, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ FAISS: A Popular Choice\n",
        "FAISS (**Facebook AI Similarity Search**) is an **open-source, fast, and efficient** vector database optimized for **local similarity search**. It‚Äôs widely used for:\n",
        "‚úÖ **Low-latency text retrieval**  \n",
        "‚úÖ **Handling millions of vectors efficiently**  \n",
        "‚úÖ **Offline or on-device AI applications**  \n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Other Vector Database Options\n",
        "| Vector DB  | Best For | Key Features |\n",
        "|------------|---------|--------------|\n",
        "| **FAISS**  | Local, Fast Search | ‚úÖ No server required, efficient indexing |\n",
        "| **ChromaDB** | Simple RAG Pipelines | ‚úÖ Lightweight, native LangChain support |\n",
        "| **Pinecone** | Scalable Cloud Search | ‚úÖ Fully managed, real-time retrieval |\n",
        "| **Weaviate** | Hybrid Search (Text + Metadata) | ‚úÖ Graph-based, AI-powered filtering |\n",
        "\n",
        "### ‚úÖ Choosing the Right One:\n",
        "- **For local, fast retrieval** ‚Üí Use **FAISS**.  \n",
        "- **For easy cloud-based search** ‚Üí Use **Pinecone**.  \n",
        "- **For AI-driven search & metadata filtering** ‚Üí Use **Weaviate**.  \n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "i6sGnvE9AH3_"
      },
      "id": "i6sGnvE9AH3_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîç Step 3: Convert Text Chunks to Embeddings (FAISS Vector Database)\n",
        "# ==================================================\n",
        "# This step:\n",
        "# ‚úÖ Converts each text chunk into vector embeddings using OpenAI Embeddings.\n",
        "# ‚úÖ Stores the embeddings in FAISS, a fast and efficient vector search database.\n",
        "# ‚úÖ Confirms successful embedding of document chunks.\n",
        "\n",
        "from langchain.vectorstores import FAISS  # FAISS for fast similarity search\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings  # OpenAI's embedding model\n",
        "\n",
        "# ‚úÖ Step 1: Initialize OpenAI Embeddings\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# ‚úÖ Step 2: Convert Text Chunks into Vector Embeddings and Store in FAISS\n",
        "vector_db = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "# ‚úÖ Step 3: Confirm database is ready\n",
        "print(f\"‚úÖ {len(docs)} document chunks embedded successfully!\")\n"
      ],
      "metadata": {
        "id": "6OnpkpGe1NHB"
      },
      "id": "6OnpkpGe1NHB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîç Step 4: Querying the Vector Database (Retrieval + Generation)\n",
        "# ==================================================\n",
        "# This step:\n",
        "# ‚úÖ Creates a retriever to fetch relevant document chunks.\n",
        "# ‚úÖ Uses OpenAI's LLM to generate an answer based on retrieved content.\n",
        "# ‚úÖ Compares RAG-based response with the non-RAG response.\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ‚úÖ Step 1: Create a Retriever (Finds Relevant Chunks)\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "# ‚úÖ Step 2: Create a Retrieval-Augmented Generation (RAG) Chain\n",
        "rag_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "\n",
        "# ‚úÖ Step 3: Ask the same question, but now with RAG retrieval\n",
        "query = \"What is OpenAI's marketing strategy in 3 bullets?\"\n",
        "response_rag = rag_chain.run(query)\n",
        "\n",
        "# ‚úÖ Step 4: Display RAG-Based Response\n",
        "print(\"\\nüîç RAG-Based Response (With Retrieval):\")\n",
        "print(response_rag)\n"
      ],
      "metadata": {
        "id": "M75QIXhX1X5J"
      },
      "id": "M75QIXhX1X5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Loading External Data: HTML & CSV in LangChain\n",
        "\n",
        "\"\"\"\n",
        "# üîç Loading External Data in LangChain\n",
        "LangChain allows us to **ingest external data** from sources like **HTML (web pages)** and **CSV files (structured data)** for retrieval-based AI applications.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Loading HTML Data (Web Scraping)\n",
        "We can extract text from websites using `HTMLLoader`:\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import HTMLLoader\n",
        "\n",
        "# Load a Wikipedia page (example)\n",
        "loader = HTMLLoader(\"https://en.wikipedia.org/wiki/Renewable_energy\")\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "cTX-PTnGAtwj"
      },
      "id": "cTX-PTnGAtwj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úã Hands-On:"
      ],
      "metadata": {
        "id": "jS5bBlCKjE4H"
      },
      "id": "jS5bBlCKjE4H"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **üìå Hands-On Task: Load Wikipedia Data on Renewable Energy**\n",
        "```python\n",
        "# ==================================================\n",
        "# ‚úã **Hands-On: Load & Retrieve Renewable Energy Info from Wikipedia**\n",
        "# ==================================================\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (`-----`) to complete the process.\n",
        "# 2Ô∏è‚É£ Use `HTMLLoader` to load Wikipedia data.\n",
        "# 3Ô∏è‚É£ Split text into retrievable chunks.\n",
        "# 4Ô∏è‚É£ Convert chunks into vector embeddings using FAISS.\n",
        "# 5Ô∏è‚É£ Use retrieval to answer a question about renewable energy.\n",
        "\n",
        "from langchain.document_loaders import HTMLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 1: Load Wikipedia Page on Renewable Energy\n",
        "# ==================================================\n",
        "wiki_url = \"https://en.wikipedia.org/wiki/Renewable_energy\"\n",
        "loader = -----  # Load HTML from Wikipedia\n",
        "documents = -----  # Extract text from the page\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 2: Split Text into Chunks\n",
        "# ==================================================\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = -----  # Split extracted text into smaller chunks\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 3: Convert Chunks to Embeddings & Store in FAISS\n",
        "# ==================================================\n",
        "embedding_model = -----  # Use OpenAIEmbeddings or another model\n",
        "vector_db = -----  # Convert docs into vector embeddings and store in FAISS\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 4: Create a Retriever to Fetch Relevant Information\n",
        "# ==================================================\n",
        "retriever = -----  # Convert FAISS vector store into a retriever\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 5: Ask AI a Question About Renewable Energy\n",
        "# ==================================================\n",
        "rag_chain = RetrievalQA.from_chain_type(-----, retriever=retriever)  # Define the RAG pipeline\n",
        "\n",
        "query = \"What are the main types of renewable energy sources?\"\n",
        "response_rag = rag_chain.run(query)\n",
        "\n",
        "# ‚úÖ Step 6: Display Retrieved Answer\n",
        "print(\"\\nüåç üîã AI Answer on Renewable Energy:\")\n",
        "print(response_rag)\n"
      ],
      "metadata": {
        "id": "wMfwf0xUz1Zq"
      },
      "id": "wMfwf0xUz1Zq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3WLVBwwAi6Iu"
      },
      "id": "3WLVBwwAi6Iu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}