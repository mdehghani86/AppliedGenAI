{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORXYUr2+/mW0Jgw7hfDhZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/M3_Lab1_Prompting_Strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Intro Section -->\n",
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 30px; border-radius: 12px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.1);\">\n",
        "    <h1 style=\"margin-bottom: 10px; font-size: 32px;\">Introduction to Prompting Strategies</h1>\n",
        "    <p style=\"font-size: 18px; margin: 0;\">Instructor: <strong>Dr. Dehghani</strong></p>\n",
        "</div>\n",
        "\n",
        "<!-- Spacer -->\n",
        "<div style=\"height: 30px;\"></div>\n",
        "\n",
        "<!-- Why It Matters Section -->\n",
        "<div style=\"background: #ffffff; padding: 25px; border-radius: 10px; border-left: 6px solid #0055d4; box-shadow: 0 4px 8px rgba(0,0,0,0.05);\">\n",
        "    <h2 style=\"margin-top: 0; color: #001a70;\">Why Prompting Strategies Matter</h2>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Imagine you‚Äôre working with a junior engineer. You say:  \n",
        "        <em>‚ÄúOptimize the system.‚Äù</em><br>\n",
        "        They‚Äôll probably ask: <em>‚ÄúWhich system? Optimize for cost, speed, or energy? Any constraints?‚Äù</em> üßê\n",
        "    </p>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Now try this instead:  \n",
        "        <em>‚ÄúAnalyze the HVAC system and minimize energy consumption while keeping temperatures between 22-24¬∞C. Provide a cost breakdown.‚Äù</em>  \n",
        "    </p>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        That‚Äôs not just a prompt‚Äîit‚Äôs a <strong>clear strategy</strong> with defined objectives and boundaries.\n",
        "        And that‚Äôs exactly what AI models need to perform at their best.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<!-- Tip Section -->\n",
        "<div style=\"background: #f5faff; padding: 20px; border-radius: 8px; border-left: 5px solid #0055d4; margin-top: 30px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #0055d4;\">üí° Pro Tip</h3>\n",
        "    <p style=\"margin: 0; font-size: 16px; line-height: 1.6;\">\n",
        "        AI models appreciate well-structured instructions just like engineers appreciate complete design specs.\n",
        "        Be specific, set clear goals, and watch the results improve!\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<!-- Upcoming Topics -->\n",
        "<div style=\"margin-top: 40px; text-align: center;\">\n",
        "    <h3 style=\"color: #001a70;\">What‚Äôs Ahead</h3>\n",
        "    <ul style=\"list-style: none; padding: 0; font-size: 16px; line-height: 1.8;\">\n",
        "        <li>üìö Basic Prompting Types</li>\n",
        "        <li>üß© Advanced Strategies</li>\n",
        "        <li>üìä Application-Specific Techniques</li>\n",
        "    </ul>\n",
        "    <p style=\"font-size: 16px; color: #333;\">Let‚Äôs engineer some powerful AI conversations! üõ†Ô∏è</p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "VuSW9V7pZvnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Section Header -->\n",
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; border-radius: 12px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.1);\">\n",
        "    <h1 style=\"margin-bottom: 10px; font-size: 30px;\">üìö Basic Prompting Types</h1>\n",
        "</div>\n",
        "\n",
        "<!-- Spacer -->\n",
        "<div style=\"height: 25px;\"></div>\n",
        "\n",
        "<!-- Zero-Shot Prompting -->\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #001a70;\">1Ô∏è‚É£ Zero-Shot Prompting</h3>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Provide only the task without any examples.  \n",
        "        <strong>Use When:</strong> The task is simple and well-known by the model.  \n",
        "        <em>Example:</em> ‚ÄúTranslate 'Hello' to French.‚Äù\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "cjZ5fTTNbfyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Set Up LLM and OpenAI API\n",
        "# ==========================\n",
        "# Import required libraries\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Load the OpenAI API key securely from Colab secrets\n",
        "api_key = userdata.get('OpenAI_Key')\n",
        "\n",
        "# Check that the API key was found\n",
        "if api_key is None:\n",
        "    raise ValueError(\"‚ùå API Key not found. Please store your OpenAI API key using Colab secrets.\")\n",
        "\n",
        "# Set API key as environment variable for OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "print(\"‚úÖ OpenAI API Key successfully loaded and environment is ready!\")\n",
        "\n",
        "# ==========================\n",
        "# üìå Set LLM Model to GPT-3.5\n",
        "# ==========================\n",
        "# Define which LLM model to use\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "print(f\"‚úÖ LLM model set to: {model_name}\")\n"
      ],
      "metadata": {
        "id": "dB3jBmICZwED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Zero-Shot Test: Hidden Formula Sequence\n",
        "# ==========================\n",
        "\n",
        "hard_sequence_prompt_zero = (\n",
        "    \"The sequence is: 3, 12, 27, 48, 75, ___. What‚Äôs next?\"\n",
        ")\n",
        "\n",
        "response_zero_hard = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": hard_sequence_prompt_zero}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(\"üîπ LLM Response (Zero-Shot - Hard Sequence):\\n\")\n",
        "print(response_zero_hard.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "AO4UhSJxb0Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<!-- One-Shot Prompting -->\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #001a70;\">2Ô∏è‚É£ One-Shot Prompting</h3>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Provide one clear example along with the instruction.  \n",
        "        <strong>Use When:</strong> You want to guide the model‚Äôs behavior with a single example.  \n",
        "        <em>Example:</em> ‚ÄúTranslate 'Hello' to French: Bonjour. Now translate 'Goodbye'.‚Äù\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "zN2CjeJunlV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Zero-Shot vs One-Shot Comparison: Alternating Pattern Sequence (Correct One-Shot)\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Zero-Shot Prompt (No Example)\n",
        "zero_shot_prompt = (\n",
        "    \"The sequence is: 1, 4, 2, 9, 3, 16, 4, ___. What number should replace the blank?\"\n",
        ")\n",
        "\n",
        "# One-Shot Prompt (One Example + New Question)\n",
        "one_shot_prompt = (\n",
        "    \"Example:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 3, 9, ___. What‚Äôs next?\\n\"\n",
        "    \"Answer: 4.\\n\\n\"\n",
        "    \"Now solve this one:\\n\"\n",
        "    \"The sequence is: 1, 4, 2, 9, 3, 16, 4, ___. What number should replace the blank?\"\n",
        ")\n",
        "\n",
        "# Run Zero-Shot\n",
        "response_zero = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Run One-Shot\n",
        "response_one = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": one_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Display Results\n",
        "print(\"üîπ Zero-Shot Response:\\n\" + \"-\"*40)\n",
        "print(response_zero.choices[0].message.content.strip())\n",
        "\n",
        "print(\"\\n\\nüîπ One-Shot Response:\\n\" + \"-\"*40)\n",
        "print(response_one.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "AWI0m9_eciJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<!-- Few-Shot Prompting -->\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #001a70;\">3Ô∏è‚É£ Few-Shot Prompting</h3>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Provide multiple examples to clearly demonstrate the pattern.  \n",
        "        <strong>Use When:</strong> The task is complex or requires understanding a specific format.  \n",
        "        <em>Example:</em>  \n",
        "        - ‚ÄúTranslate 'Hello' to French: Bonjour.‚Äù  \n",
        "        - ‚ÄúTranslate 'Goodbye' to French: Au revoir.‚Äù  \n",
        "        - ‚ÄúTranslate 'Thank you' to French: Merci.‚Äù  \n",
        "        Now translate 'Good night'.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<!-- Spacer -->\n",
        "<div style=\"height: 30px;\"></div>\n",
        "\n",
        "<!-- Closing Tip -->\n",
        "<div style=\"background: #f5faff; padding: 20px; border-radius: 8px; border-left: 5px solid #0055d4;\">\n",
        "    <h3 style=\"margin-top: 0; color: #0055d4;\">üí° Quick Reminder</h3>\n",
        "    <p style=\"margin: 0; font-size: 16px; line-height: 1.6;\">\n",
        "        The more complex the task, the more examples you should provide. But remember, too many examples can make prompts bulky and inefficient.\n",
        "    </p>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "DdXDGKuAnawA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Few-Shot Prompting Example: Ultra-Hard Pattern (3 Hidden Rules)\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-4-turbo\"  # Best for complex reasoning\n",
        "\n",
        "# Few-Shot Prompt with 2 Examples\n",
        "few_shot_prompt = (\n",
        "    \"Example 1:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 3, 9, ___. What‚Äôs next?\\n\"\n",
        "    \"Answer: 4.\\n\\n\"\n",
        "    \"Example 2:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 4, 9, 7, 16, ___. What‚Äôs next?\\n\"\n",
        "    \"Answer: 11.\\n\\n\"\n",
        "    \"Now try this one:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 4, 9, 7, 16, 11, ___, 16, 36. What number should replace the blank?\"\n",
        ")\n",
        "\n",
        "# Run Few-Shot Prompt\n",
        "response_few = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": few_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Display Result\n",
        "print(\"üîπ Few-Shot Prompting (Two Examples Provided):\")\n",
        "print(\"-\" * 40)\n",
        "print(response_few.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "3FLfQaF8mwB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m0ICbL-NnaGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Advanced Prompting Techniques  \n",
        "\n",
        "Moving beyond basic prompting methods like zero-shot and few-shot, advanced strategies help enhance the reasoning and adaptability of large language models (LLMs). These techniques guide the model's thought process to handle complex tasks more effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### üîó Chain-of-Thought (CoT) Prompting  \n",
        "\n",
        "Chain-of-Thought prompting encourages models to **explain their intermediate reasoning steps**, leading to more transparent and accurate conclusions. By structuring prompts to include logical steps, CoT improves the model‚Äôs ability to solve complex reasoning tasks.\n",
        "\n",
        "**Why is CoT Important?**  \n",
        "- ‚úîÔ∏è Improves performance on multi-step reasoning tasks.  \n",
        "- ‚úîÔ∏è Helps produce logically structured and coherent responses.  \n",
        "- ‚úîÔ∏è Breaks down complex problems into manageable steps.\n",
        "\n",
        "üìñ **Reference:** [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
        "\n",
        "---\n",
        "\n",
        "*Next, explore practical examples of Chain-of-Thought prompting.*\n"
      ],
      "metadata": {
        "id": "cJYU0rOaTD5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Chain-of-Thought Demonstration: Make 110 with Five 5's\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-4-turbo\"\n",
        "\n",
        "# Zero-Shot Prompt (No Reasoning Encouraged)\n",
        "zero_shot_prompt = (\n",
        "    \"Use exactly five 5‚Äôs and only four operations (+, -, *, /) and parentheses to make 110.\"\n",
        ")\n",
        "\n",
        "# Chain-of-Thought Prompt (Encourages Step-by-Step Reasoning)\n",
        "cot_prompt = (\n",
        "    \"Let's solve this step by step.\\n\"\n",
        "    \"We need to use exactly five 5‚Äôs and only four operations (+, -, *, /) and parentheses to make 110.\\n\"\n",
        "    \"Step 1: Think about how we can combine the 5's to form larger numbers (e.g., 55).\\n\"\n",
        "    \"Step 2: Try to combine them logically to reach 110.\\n\"\n",
        "    \"Now, provide the final equation and the answer.\"\n",
        ")\n",
        "\n",
        "# Run Zero-Shot\n",
        "response_zero = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Run Chain-of-Thought\n",
        "response_cot = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": cot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Display Results\n",
        "print(\"üîπ Zero-Shot Response (No Reasoning Encouraged):\\n\" + \"-\" * 50)\n",
        "print(response_zero.choices[0].message.content.strip())\n",
        "\n",
        "print(\"\\nüîπ Chain-of-Thought Response (Reasoning Encouraged):\\n\" + \"-\" * 50)\n",
        "print(response_cot.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "rNdqf6qGnJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úã Hands-On Experiment: Observations  \n",
        "\n",
        "üìå **Instructions:**  \n",
        "- Run your experiments by changing the model type (e.g., `gpt-3.5-turbo`, `gpt-4-turbo`, `gpt-o3`), temperature, and prompt style.  \n",
        "- You can **either attach a screenshot/image of your results** or **write a brief summary of your observations (max half a page)**.\n",
        "\n",
        "---\n",
        "\n",
        "- **Model Used:**  \n",
        "  _[Enter the model name you tried, e.g., gpt-3.5-turbo, gpt-4-turbo, or gpt-o3]_\n",
        "\n",
        "- **Temperature Setting:**  \n",
        "  _[Enter the temperature you used, e.g., 0.0, 0.5, 0.7]_\n",
        "\n",
        "- **Zero-Shot Result:**  \n",
        "  _[Did Zero-Shot solve the problem correctly? Yes/No. Add a short explanation or attach an image.]_\n",
        "\n",
        "- **Chain-of-Thought Result:**  \n",
        "  _[Did Chain-of-Thought solve the problem better? Yes/No. Add a short explanation or attach an image.]_\n",
        "\n",
        "- **Key Takeaways (Max Half Page or Screenshot):**  \n",
        "  _[Summarize what you observed. Did a specific model perform better? How did temperature affect the results? What worked best? Attach image or write here.]_\n",
        "\n",
        "---\n",
        "\n",
        "‚úçÔ∏è *Try at least two models and different temperatures. Compare the results and reflect on how prompting strategies influence performance!*\n"
      ],
      "metadata": {
        "id": "LQ6og568laaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Self-Consistency Prompting\n",
        "\n",
        "While Chain-of-Thought (CoT) improves reasoning by encouraging step-by-step thinking, it may still produce **inconsistent or incorrect** answers, especially in complex scenarios.  \n",
        "**Self-Consistency Prompting** enhances CoT by asking the model to **generate multiple reasoning paths** and then select the most common or consistent final answer.\n",
        "\n",
        "### Why is Self-Consistency Useful?\n",
        "\n",
        "- ‚úÖ Reduces random reasoning errors.\n",
        "- ‚úÖ Boosts reliability on ambiguous or multi-path problems.\n",
        "- ‚úÖ Often improves performance on mathematical, logical, and symbolic tasks.\n",
        "\n",
        "üìñ **Reference**: [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)\n",
        "\n",
        "---\n",
        "\n",
        "*Next, we‚Äôll see how Self-Consistency works in action using a complex reasoning example.*\n"
      ],
      "metadata": {
        "id": "RN2Af6nAkKi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Comparing Chain-of-Thought vs. Self-Consistency Prompting\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-4-turbo\"  # Using GPT-4 for better reasoning\n",
        "\n",
        "# Define the problem prompt\n",
        "problem_prompt = (\n",
        "    \"If a train travels at 60 miles per hour and leaves at 2 PM, and another train leaves \"\n",
        "    \"the same station at 3 PM traveling at 90 miles per hour, when will the second train catch up to the first?\"\n",
        ")\n",
        "\n",
        "# Chain-of-Thought Prompt (Standard)\n",
        "cot_prompt = (\n",
        "    \"Let's solve this step by step.\\n\"\n",
        "    + problem_prompt\n",
        ")\n",
        "\n",
        "# Self-Consistency Prompt: Ask the model to produce multiple reasoning paths\n",
        "def run_self_consistency(prompt, num_attempts=5):\n",
        "    answers = []\n",
        "    for _ in range(num_attempts):\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7  # Add randomness to explore different reasoning paths\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip()\n",
        "        answers.append(answer)\n",
        "    return answers\n",
        "\n",
        "# Run Chain-of-Thought (Single Attempt)\n",
        "response_cot = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": cot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "cot_answer = response_cot.choices[0].message.content.strip()\n",
        "\n",
        "# Run Self-Consistency (Multiple Attempts)\n",
        "sc_answers = run_self_consistency(cot_prompt, num_attempts=5)\n",
        "\n",
        "# Simple Majority Vote to Find Most Consistent Answer\n",
        "from collections import Counter\n",
        "most_common_answer = Counter(sc_answers).most_common(1)[0]\n",
        "\n",
        "# Display Results\n",
        "print(\"üîπ Chain-of-Thought Response (Single Attempt):\\n\" + \"-\" * 50)\n",
        "print(cot_answer)\n",
        "\n",
        "print(\"\\nüîπ Self-Consistency Responses (Multiple Attempts):\\n\" + \"-\" * 50)\n",
        "for idx, ans in enumerate(sc_answers, 1):\n",
        "    print(f\"Attempt {idx}: {ans}\")\n",
        "\n",
        "print(\"\\nüîπ Final Self-Consistency Selected Answer:\\n\" + \"-\" * 50)\n",
        "print(f\"Most Common Answer: {most_common_answer[0]}\\nAppeared {most_common_answer[1]} times.\")\n"
      ],
      "metadata": {
        "id": "yNWwXIahdaOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Exploring More Advanced Prompting Strategies  \n",
        "\n",
        "Beyond Chain-of-Thought and Self-Consistency, several other powerful prompting methods help LLMs tackle complex reasoning tasks more effectively:\n",
        "\n",
        "- **üß© Tree-of-Thought (ToT) Prompting**:  \n",
        "  Explores multiple reasoning paths like a decision tree, helping the model evaluate and compare various solutions before choosing the best one.\n",
        "\n",
        "- **ü§ñ ReAct (Reasoning and Acting) Prompting**:  \n",
        "  Combines reasoning steps with actions, including API calls or external tool usage. Ideal for interactive agents and dynamic decision-making tasks.\n",
        "\n",
        "- **üîÑ Reflexion Prompting**:  \n",
        "  Encourages the model to critique its own responses and iteratively improve them, simulating self-correction and learning.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úã Hands-On Task: Compare Prompting Strategies  \n",
        "\n",
        "üìå **Task Instructions:**  \n",
        "- Experiment with **Self-Consistency**, **Tree-of-Thought**, and **ReAct** prompting methods.  \n",
        "- Try to solve the following problem using each method and compare the results.\n",
        "\n",
        "### üß† **Challenge Problem:**  \n",
        "*A farmer has chickens and rabbits in a cage. There are 35 heads and 94 legs. How many chickens and rabbits are there?*  \n",
        "\n",
        "---\n",
        "\n",
        "- Try different models (e.g., `gpt-3.5-turbo`, `gpt-4-turbo`, `gpt-o3`).\n",
        "- Experiment with different temperatures (e.g., `0.0`, `0.5`, `0.7`).\n",
        "- Use both direct prompts and advanced strategies like CoT, Self-Consistency, or ReAct.\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Observations  \n",
        "\n",
        "- **Model and Strategy Used:**  \n",
        "  _[Enter the model and prompting strategy you tried]_\n",
        "\n",
        "- **Was the Correct Answer Found?**  \n",
        "  _[Yes/No. Explain briefly or attach a screenshot]_  \n",
        "\n",
        "- **Key Takeaways (Max Half Page or Screenshot):**  \n",
        "  _[Summarize how different strategies performed. What worked best? Why?]_\n",
        "\n",
        "---\n",
        "\n",
        "‚úçÔ∏è *Hint: Try breaking down the problem into equations or ask the model to explain its steps before giving the final answer. Notice which strategies lead to faster and more accurate results!*\n"
      ],
      "metadata": {
        "id": "Hfw5kDf5l_o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ‚úã Hands-On Code: Try Different Prompting Strategies and Models\n",
        "# ==========================\n",
        "\n",
        "# üìù Instructions:\n",
        "# - Change 'model_name' to try different models (e.g., \"gpt-3.5-turbo\", \"gpt-4-turbo\", \"gpt-o3\").\n",
        "# - Adjust 'temperature' to test how creativity affects reasoning.\n",
        "# - Try Self-Consistency by sampling multiple outputs and comparing answers.\n",
        "# - Optionally, explore Tree-of-Thought and ReAct patterns by modifying prompts.\n",
        "# ‚úÖ Your Experiment Starts Here üëá\n"
      ],
      "metadata": {
        "id": "5kYqO4FgkJd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Conclusion  \n",
        "\n",
        "In this hands-on exploration, different advanced prompting strategies were tested to solve reasoning-based challenges. Through experimenting with **Chain-of-Thought (CoT)**, **Self-Consistency**, and other methods, the following key insights were observed:\n",
        "\n",
        "- Advanced prompting techniques significantly improve model performance, especially on complex, multi-step problems.\n",
        "- Changing the **model type** and **temperature** can drastically affect reasoning quality and creativity.\n",
        "- Some strategies, like **Self-Consistency**, help reduce random errors by exploring multiple reasoning paths.\n",
        "- For ambiguous or challenging problems, combining strategies (e.g., CoT + Self-Consistency) often leads to the most reliable results.\n",
        "\n",
        "üìñ *Remember: Prompt engineering is both an art and a science. The more you experiment, the better you understand how to guide LLMs effectively!*\n",
        "\n",
        "---\n",
        "\n",
        "‚úçÔ∏è *Final Reflection:*  \n",
        "_[Write 2-3 sentences summarizing what you personally learned about prompting strategies and how model selection or temperature influenced the results.]_\n"
      ],
      "metadata": {
        "id": "sivjpWXsmlUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WJRwTsOmlrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}