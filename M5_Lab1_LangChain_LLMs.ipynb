{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/LangChain_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b23c44",
      "metadata": {
        "id": "51b23c44"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">üß† LangChain Lab 1: Introduction</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">Welcome to the first hands-on lab for LangChain! In this lab, we will:</p>\n",
        "  <p style=\"margin-top: 10px; font-size: 18px; font-weight: bold;\">Instructor: Dr. Dehghani</p>\n",
        "  <p style=\"margin-top: 5px; font-size: 14px;\">\n",
        "    Learn more at <a href=\"https://langchain.com\" target=\"_blank\" style=\"color: #ffdd57; text-decoration: underline;\">LangChain Website</a>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 20px; border: 1px solid #0055d4;\">\n",
        "  <p style=\"line-height: 1.6; font-size: 16px; margin-bottom: 15px;\">\n",
        "    LangChain is a framework for building applications powered by large language models (LLMs).  \n",
        "    It provides modular components‚Äîsuch as prompt templates, memory, and chains‚Äîthat make it easy to develop, test, and deploy LLM-based solutions.\n",
        "  </p>\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">Lab Objectives</h2>\n",
        "  <ul style=\"line-height: 1.8; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li>Set up LangChain in Google Colab</li>\n",
        "    <li>Interact with OpenAI models</li>\n",
        "    <li>Work with open-source models like LLaMA and Falcon</li>\n",
        "    <li>Understand prompt templates and experiment with parameters</li>\n",
        "  </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c7683f7f",
      "metadata": {
        "id": "c7683f7f"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è LangChain Lab Dependencies\n",
        "# ================================\n",
        "!pip install -q langchain            # core LLM framework\n",
        "!pip install -q langchain-community # extra integrations & utils\n",
        "!pip install -q openai==0.28        # OpenAI GPT client (v0.28)\n",
        "!pip install -q transformers        # Hugging Face models (LLaMA, Falcon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cDbNlFoDqiET",
      "metadata": {
        "id": "cDbNlFoDqiET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d5b813-33d2-451b-bf0c-d8194ddfed0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# üìå Importing Required Libraries for LangChain Lab\n",
        "# ==================================================\n",
        "\n",
        "import os                                  # For environment variables (API keys)\n",
        "from google.colab import userdata          # For loading secrets in Colab\n",
        "\n",
        "# LangChain chat utilities\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI‚Äôs GPT chat models\n",
        "from langchain.schema import HumanMessage     # Schema for structured chat messages\n",
        "\n",
        "# Open-source model support\n",
        "import transformers                         # Hugging Face models (e.g., LLaMA, Falcon)\n",
        "import torch                                # PyTorch for deep learning execution\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6843a5c",
      "metadata": {
        "id": "c6843a5c"
      },
      "source": [
        "## üîë Step 2: Set Up OpenAI API Key\n",
        "If you want to use OpenAI models like GPT-4, you need an API key. Run the code below and enter your key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fba2e457",
      "metadata": {
        "id": "fba2e457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52755de7-22ed-4354-c97d-c1f90f0d5c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded successfully!\n",
            "‚úÖ Hugging Face API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è Load API Keys from Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve your stored secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')  # OpenAI API key\n",
        "HF_API_KEY     = userdata.get('HF_API_KEY')      # Hugging Face API key\n",
        "\n",
        "# Feedback messages with emoji\n",
        "if OPENAI_API_KEY:\n",
        "    print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API key not found. Please set 'OPENAI_API_KEY' in Colab secrets.\")\n",
        "\n",
        "if HF_API_KEY:\n",
        "    print(\"‚úÖ Hugging Face API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Hugging Face API key not found. Please set 'HF_API_KEY' in Colab secrets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27zoTSlqMFr",
      "metadata": {
        "id": "e27zoTSlqMFr"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">üìå Testing LangChain Connection (OpenAI GPT-4)</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">\n",
        "    Now that we've set up our OpenAI API key, let's test if <strong>LangChain</strong> is properly connected.<br>\n",
        "    Unlike directly calling OpenAI‚Äôs API, this approach leverages <strong>LangChain‚Äôs framework</strong> to manage interactions efficiently.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 20px; margin-bottom: 25px; border: 1px solid #0055d4;\">\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">üîπ What This Code Does:</h2>\n",
        "  <ol style=\"line-height: 1.8; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li><strong>Imports Required Libraries</strong> ‚Üí <code>ChatOpenAI</code> for OpenAI chat models and <code>HumanMessage</code> for structured inputs.</li>\n",
        "    <li><strong>Initializes an OpenAI LLM Instance</strong> ‚Üí Uses <code>gpt-4</code> via LangChain.</li>\n",
        "    <li><strong>Sends a Test Prompt</strong> ‚Üí Verifies that LangChain can process requests through OpenAI.</li>\n",
        "    <li><strong>Uses <code>.invoke()</code> to Generate a Response</strong> ‚Üí<br>\n",
        "      &nbsp;&nbsp;‚Äì <code>.invoke()</code> is the recommended method in LangChain for calling an LLM.<br>\n",
        "      &nbsp;&nbsp;‚Äì It ensures the request is formatted correctly and optimizes processing within the framework.\n",
        "    </li>\n",
        "    <li><strong>Checks for a Successful Connection</strong> ‚Üí If LangChain is set up correctly, the model will return a valid response.</li>\n",
        "  </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3f3; border-radius: 12px; padding: 15px; border: 1px solid #ff6b6b;\">\n",
        "  <p style=\"margin: 0; line-height: 1.6; font-size: 16px; color: #d32f2f;\">\n",
        "    üöÄ <strong>Next Step:</strong> If you receive a valid response, your LangChain integration is working as expected!\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "HpUBxiUZqLqU",
      "metadata": {
        "id": "HpUBxiUZqLqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1edd316-4758-4b4d-e3dd-884801d98a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ OpenAI Response: LangChain is a decentralized translation platform powered by artificial intelligence and blockchain technology.\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test: OpenAI (GPT-4)\n",
        "# üìå Import Libraries & Load API Key\n",
        "# =========================================================\n",
        "\n",
        "# üîë Load your OpenAI API key from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# ‚úÖ Step 1: Initialize the LLM with your API key\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 2: Define the prompt\n",
        "prompt = \"What is LangChain in one short sentence?\"\n",
        "\n",
        "# ‚úÖ Step 3: Invoke the model\n",
        "response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "# ‚úÖ Step 4: Display the response\n",
        "print(\"üîπ OpenAI Response:\", response.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49B7fPC_sM6g",
      "metadata": {
        "id": "49B7fPC_sM6g"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Reading the LLM's Response**\n",
        "# Replace '-----' in the placeholders with the correct method or key to retrieve the requested information.\n",
        "\n",
        "# Task 1: Get the main content from the LLM response\n",
        "response_text = response.-----  # Extract the main response text (e.g., \"content\" for ChatOpenAI)\n",
        "\n",
        "# Task 2: Get the number of prompt tokens used\n",
        "prompt_tokens = response.-----  # Extract the number of tokens used in the input prompt\n",
        "\n",
        "# Task 3: Get the number of response tokens generated\n",
        "response_tokens = response.-----  # Extract the number of tokens used in the generated response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ng1KX4E_xXqH",
      "metadata": {
        "id": "Ng1KX4E_xXqH"
      },
      "source": [
        "# Multi-Turn Conversation in LangChain\n",
        "\n",
        "A **multi-turn conversation** allows an AI to retain context across multiple exchanges, making interactions more natural and intelligent. Instead of treating each query independently, the AI builds on previous inputs, improving coherence and accuracy.\n",
        "\n",
        "## Why Use Multi-Turn Conversations?\n",
        "- **Context Retention** ‚Äì AI remembers past interactions, leading to more relevant responses.  \n",
        "- **Realistic Dialogue** ‚Äì Mimics human conversations, making chatbots more engaging.  \n",
        "- **Improved Accuracy** ‚Äì Responses are refined based on earlier exchanges.  \n",
        "- **Scalable Design** ‚Äì Supports long-form discussions without losing context.\n",
        "\n",
        "This approach is ideal for applications like **financial advisors, chatbots, research assistants**, and other AI-driven services that require ongoing, dynamic conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "VPScknqNr28z",
      "metadata": {
        "id": "VPScknqNr28z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38c8ad0-9c33-4721-b519-838e6137416c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-348eb891876d>:26: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(messages)  # Replace 'llm' with your initialized model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, there can be tax benefits associated with certain types of passive income investments:\n",
            "\n",
            "1. Dividend Stocks: Qualified dividends are taxed at a lower rate than ordinary income. \n",
            "\n",
            "2. Real Estate Rentals: Rental properties can provide tax benefits through depreciation, mortgage interest deductions, and other real estate expenses. \n",
            "\n",
            "3. REITs: Real Estate Investment Trusts (REITs) distribute at least 90% of their taxable income to shareholders annually in the form of dividends. While these dividends are taxed as ordinary income, they might be a tax-efficient choice for certain investors.\n",
            "\n",
            "4. Government Bonds: Interest earned on certain types of government-issued securities, such as municipal bonds, are often exempt from federal income tax and in some cases, state and local taxes as well.\n",
            "\n",
            "5. High-Yield Savings Accounts: While the interest you earn is taxable, individuals can open an account in an Individual Retirement Account (IRA) or other retirement accounts to defer or possibly avoid paying taxes.\n",
            "\n",
            "Remember, tax laws are complex and subject to change, so it's always a good idea to consult with a tax advisor or certified public accountant.\n"
          ]
        }
      ],
      "source": [
        "# üåü Multi-Turn Conversation Example: Coffee History (Concise Responses)\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Instruct the AI to act as a coffee historian and reply in one short sentence\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a coffee historian. Provide concise, one-sentence answers.\"),\n",
        "    HumanMessage(content=\"What is the origin of coffee?\"),\n",
        "    AIMessage(content=\"Coffee was first discovered in the Kaffa region of Ethiopia in the 9th century.\"),\n",
        "    HumanMessage(content=\"How did coffee spread beyond Ethiopia?\"),\n",
        "    AIMessage(content=\"It traveled via trade routes into Yemen and then throughout the Ottoman Empire.\"),\n",
        "    HumanMessage(content=\"When did coffee reach Europe?\")\n",
        "]\n",
        "\n",
        "# Send the structured messages to your initialized LangChain model\n",
        "response = llm(messages)\n",
        "print(response.content)  # Expect a one-sentence response on coffee reaching Europe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jhpvSXq7yCv_",
      "metadata": {
        "id": "jhpvSXq7yCv_"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Completing a Multi-Turn Conversation (Travel Assistant)**\n",
        "\n",
        "# üìå Task Instructions:\n",
        "# - Below is a conversation with a **travel assistant** AI.\n",
        "# - Fill in the last `HumanMessage` with a relevant travel-related question.\n",
        "# - Complete the placeholder `response = ----(----)` to correctly call the LLM.\n",
        "\n",
        "# Define a conversation with missing parts\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful travel assistant, providing recommendations for destinations and travel tips.\"),\n",
        "    HumanMessage(content=\"What are some must-visit places in Japan?\"),\n",
        "    AIMessage(content=\"Some must-visit places in Japan include Tokyo, Kyoto, and Osaka. Each city offers unique cultural and historical experiences.\"),\n",
        "    HumanMessage(content=\"-----\")  #  Task: Fill in a relevant follow-up question\n",
        "]\n",
        "\n",
        "# üîß Task: Complete the function to generate a response from the LLM\n",
        "response = ----(----)  # Call the LLM correctly using the messages\n",
        "\n",
        "print(response)  # Display the AI's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IY4M6c06zgqR",
      "metadata": {
        "id": "IY4M6c06zgqR"
      },
      "source": [
        "# ü§ñ **Multi-LLM Comparison: Understanding Model Differences**  \n",
        "\n",
        "As AI-powered language models continue to evolve, different models offer **unique perspectives** based on their training data and architecture. This task explores how multiple LLMs respond to the same prompt, helping us understand their **strengths, biases, and response styles**.  \n",
        "\n",
        "### **Why Compare Multiple LLMs?**  \n",
        "‚úÖ **Varied insights** ‚Äì Each model generates responses based on different datasets and training techniques.  \n",
        "‚úÖ **Performance differences** ‚Äì Some models prioritize factual accuracy, while others focus on creativity or conciseness.  \n",
        "‚úÖ **Task specialization** ‚Äì While some models excel in general knowledge, others are optimized for specific domains.  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "JoAeXeGcnAB4"
      },
      "id": "JoAeXeGcnAB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîë Hugging Face API Key Setup with Output Clearing\n",
        "# ==================================================\n",
        "\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# Create an input widget for the Hugging Face API key\n",
        "huggingface_key_input = widgets.Password(\n",
        "    description=\"ü§ó HF Key:\",\n",
        "    placeholder=\"Enter your Hugging Face API Key\",\n",
        ")\n",
        "\n",
        "# Create a button to submit the API key\n",
        "submit_button = widgets.Button(description=\"‚úÖ Set API Key\")\n",
        "\n",
        "# Function to save the Hugging Face API key when the button is clicked\n",
        "def set_hf_api_key(b):\n",
        "    # Clear previous outputs\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Display the input widget and button again\n",
        "    display(huggingface_key_input, submit_button)\n",
        "\n",
        "    # Retrieve and validate the API key\n",
        "    hf_key = huggingface_key_input.value.strip()\n",
        "\n",
        "    if hf_key:\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key\n",
        "        print(\"‚úÖ Hugging Face API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"‚ùå Please enter a valid Hugging Face API Key.\")\n",
        "\n",
        "# Link button click to the function\n",
        "submit_button.on_click(set_hf_api_key)\n",
        "\n",
        "# Display the API key input field and button\n",
        "display(huggingface_key_input, submit_button)\n"
      ],
      "metadata": {
        "id": "VR90KJx1BX5J"
      },
      "id": "VR90KJx1BX5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test with HuggingFaceEndpoint\n",
        "# =========================================================\n",
        "\n",
        "# üìå Import Libraries\n",
        "# - HuggingFaceEndpoint: Allows direct interaction with Hugging Face models via API.\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Initialize the Hugging Face endpoint with a specific model\n",
        "# - Falcon-7B-Instruct: A powerful, open-source language model optimized for instruction-based tasks.\n",
        "# - Developed by TII UAE, it is designed for high-quality text generation and reasoning.\n",
        "llm_falcon = HuggingFaceEndpoint(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\"  # Falcon-7B Instruct model\n",
        ")\n",
        "\n",
        "# ‚úÖ Define a question\n",
        "question = \"What are the key challenges of the AI boom?\"\n",
        "\n",
        "# ‚úÖ Query the model\n",
        "response_falcon = llm_falcon.invoke(question)  # Store the response\n",
        "\n",
        "# ‚úÖ Display the output\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "_W4Jn6aJ8gQa"
      },
      "id": "_W4Jn6aJ8gQa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Querying Falcon-7B with LangChain**\n",
        "# Replace the placeholders to:\n",
        "# 1Ô∏è‚É£ Initialize the Hugging Face model correctly.\n",
        "# 2Ô∏è‚É£ Use the correct method to query the model.\n",
        "# 3Ô∏è‚É£ Store the response in the right variable.\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Step 1: Initialize the Falcon model\n",
        "llm_falcon = ----- (repo_id=\"tiiuae/falcon-7b-instruct\")  # üîß Replace '-----' with the correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define the question\n",
        "question = \"How will AI impact the job market in the next decade?\"\n",
        "\n",
        "# ‚úÖ Step 3: Query the model\n",
        "response_falcon = llm_falcon.----- (question)  # üîß Replace '-----' with the correct method\n",
        "\n",
        "# ‚úÖ Display the response\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "NcHL_zVXDoA0"
      },
      "id": "NcHL_zVXDoA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecf8384",
      "metadata": {
        "id": "9ecf8384"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Call OpenAI's GPT-4 model\n",
        "response = openai.----(\n",
        "    model=\"gpt-4\",\n",
        "    prompt=\"Hello, how can I use LangChain?\",\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Handling Responses in LangChain (OpenAI vs. Hugging Face)**  \n",
        "\n",
        "üîπ **For OpenAI (`ChatOpenAI`)** ‚Üí The response contains structured data, so you need to extract `.content`.  \n",
        "üîπ **For Hugging Face (`HuggingFaceEndpoint`)** ‚Üí The response is plain text, so no `.content` is needed.  \n",
        "\n",
        "### ‚úÖ **Examples**\n",
        "\n",
        "#### **OpenAI (Requires `.content`)**\n",
        "```python\n",
        "response_openai = llm_openai.invoke([HumanMessage(content=\"What is AI?\")])\n",
        "print(response_openai.content)  # ‚úÖ Required for OpenAI\n",
        "response_falcon = llm_falcon.invoke(\"What is AI?\")\n",
        "print(response_falcon)  # ‚úÖ Directly usable\n"
      ],
      "metadata": {
        "id": "1WfMw6NyD_GV"
      },
      "id": "1WfMw6NyD_GV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tezuyOcJz7zG",
      "metadata": {
        "id": "tezuyOcJz7zG"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üåü Multi-LLM Comparison: AI Boom (Updated)\n",
        "# ==================================================\n",
        "\n",
        "# üìå This code compares multiple LLMs by asking them the same question.\n",
        "# - A SystemMessage sets a common AI behavior (AI expert on the AI boom).\n",
        "# - A HumanMessage simulates a user query about AI challenges.\n",
        "# - Different LLMs (GPT-4, Falcon, Zephyr) process the same input.\n",
        "# - Their responses are stored and displayed for comparison.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "from langchain.schema import SystemMessage, HumanMessage  # Defines structured messages for AI interaction\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI chat models (e.g., GPT-4)\n",
        "from langchain_huggingface import HuggingFaceEndpoint  # Correct Hugging Face integration\n",
        "\n",
        "# ‚úÖ Define a system message (sets behavior of the AI)\n",
        "system_message = SystemMessage(content=\"You are an AI expert. Explain the impact of the AI boom on society.\")\n",
        "\n",
        "# ‚úÖ Define a human message (user's input)\n",
        "human_message = HumanMessage(content=\"What are the 2 biggest challenges of the AI boom?\")\n",
        "\n",
        "# ‚úÖ Dictionary of models to compare (each entry is an LLM instance)\n",
        "llms = {\n",
        "    \"GPT-4\": ChatOpenAI(model_name=\"gpt-4\"),  # OpenAI's GPT-4 model\n",
        "    \"Falcon\": HuggingFaceEndpoint(repo_id=\"tiiuae/falcon-7b-instruct\"),  # Falcon-7B Instruct model\n",
        "    \"Zephyr\": HuggingFaceEndpoint(\n",
        "        repo_id=\"HuggingFaceH4/zephyr-7b-alpha\"  # Zephyr-7B model\n",
        "    ),  # Zephyr is a lightweight, general-purpose LLM optimized for efficiency and speed.\n",
        "}\n",
        "\n",
        "# ‚úÖ Query each model and store responses\n",
        "responses = {}\n",
        "for model_name, llm in llms.items():\n",
        "    if isinstance(llm, ChatOpenAI):\n",
        "        # OpenAI models expect structured messages (extract `.content` from the response)\n",
        "        response = llm.invoke([system_message, human_message])\n",
        "        responses[model_name] = response.content  # Extract and store the response content\n",
        "    elif isinstance(llm, HuggingFaceEndpoint):\n",
        "        # Hugging Face models expect a plain text prompt\n",
        "        prompt = f\"{system_message.content} {human_message.content}\"\n",
        "        responses[model_name] = llm.invoke(prompt)  # Hugging Face models return plain text directly\n",
        "\n",
        "# ‚úÖ Display responses for comparison\n",
        "for model, response in responses.items():\n",
        "    print(f\"üîπ {model} Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Multi-LLM Comparison with OpenAI and Hugging Face**\n",
        "# Replace the placeholders (`-----`) to:\n",
        "# 1Ô∏è‚É£ Initialize the OpenAI and Hugging Face models properly.\n",
        "# 2Ô∏è‚É£ Format the messages correctly.\n",
        "# 3Ô∏è‚É£ Extract the OpenAI response content and handle Hugging Face response directly.\n",
        "# 4Ô∏è‚É£ Print all the model responses for comparison.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "from langchain.schema import -----, -----  # Define the correct classes for AI messages\n",
        "from langchain.chat_models import -----  # Import OpenAI's chat model class\n",
        "from langchain_huggingface import -----  # Import Hugging Face endpoint class\n",
        "\n",
        "# ‚úÖ Step 1: Define a system message (AI's role/behavior)\n",
        "system_message = ----- (content=\"You are a healthcare consultant. Provide insights on improving hospital efficiency.\")  # Correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define a human message (user's question)\n",
        "human_message = ----- (content=\"What strategies can hospitals implement to reduce patient wait times?\")  # Correct class\n",
        "\n",
        "# ‚úÖ Step 3: Initialize models\n",
        "llms = {\n",
        "    \"GPT-4\": ----- (model_name=\"gpt-4\"),  # Correct model initialization for OpenAI\n",
        "    \"Falcon\": ----- (repo_id=\"tiiuae/falcon-7b-instruct\"),  # Hugging Face model for Falcon\n",
        "    \"Zephyr\": ----- (repo_id=\"HuggingFaceH4/zephyr-7b-alpha\"),  # Hugging Face model for Zephyr\n",
        "}\n",
        "\n",
        "# ‚úÖ Step 4: Query each model and store responses\n",
        "responses = {}\n",
        "for model_name, llm in llms.items():\n",
        "    if isinstance(llm, -----):  # Replace with correct class to identify OpenAI models\n",
        "        response = llm.----- ([system_message, human_message])  # Replace with correct method\n",
        "        responses[model_name] = response.-----  # Extract the content for OpenAI models\n",
        "    elif isinstance(llm, -----):  # Replace with correct class to identify Hugging Face models\n",
        "        prompt = f\"{system_message.content} {human_message.content}\"\n",
        "        responses[model_name] = llm.----- (prompt)  # Replace with correct method for Hugging Face\n",
        "\n",
        "# ‚úÖ Step 5: Display all model responses\n",
        "for model, response in responses.items():\n",
        "    print(f\"üîπ {model} Response:\", response)\n"
      ],
      "metadata": {
        "id": "YvS3hJQSMRX1"
      },
      "id": "YvS3hJQSMRX1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù **Note: Why is Prompting Different in OpenAI vs. Hugging Face?**  \n",
        "\n",
        "### üîπ **OpenAI (`ChatOpenAI`) Uses Structured Messages**\n",
        "- Requires **role-based prompts** (`SystemMessage`, `HumanMessage`).\n",
        "- Designed for **multi-turn conversations** with memory.\n",
        "```python\n",
        "response = llm.invoke([\n",
        "    SystemMessage(content=\"You are a healthcare consultant.\"),\n",
        "    HumanMessage(content=\"How can hospitals reduce patient wait times?\")\n",
        "])\n",
        "print(response.content)  # Extract response\n",
        "\n",
        "### üîπ **Hugging Face (`HuggingFaceEndpoint`) Uses Plain Text**\n",
        "- Models expect a **single text prompt**, no structured roles.\n",
        "- Typically **stateless**, meaning no conversation history.\n",
        "\n",
        "```python\n",
        "prompt = \"You are a healthcare consultant. How can hospitals reduce wait times?\"\n",
        "response = llm.invoke(prompt)  # Direct plain text input\n",
        "print(response)  # No `.content` needed\n"
      ],
      "metadata": {
        "id": "Kw7h8DV5M9v4"
      },
      "id": "Kw7h8DV5M9v4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You have successfully completed the **Introduction to LangChain** lab. üöÄ  \n",
        "We hope you found it insightful and are excited to explore more!  \n",
        "\n",
        "üí° **Next Steps:**  \n",
        "- Try using different LLMs in LangChain.  \n",
        "- Experiment with structured vs. plain text prompts.  \n",
        "- Explore advanced features like memory and chains.\n",
        "\n",
        "Happy Coding! üíª‚ú®  \n"
      ],
      "metadata": {
        "id": "BqzZxacjPK6k"
      },
      "id": "BqzZxacjPK6k"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
