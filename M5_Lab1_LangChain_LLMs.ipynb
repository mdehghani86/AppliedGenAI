{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/M5_Lab1_LangChain_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b23c44",
      "metadata": {
        "id": "51b23c44"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">üß† LangChain Lab 1: Introduction</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">Welcome to the first hands-on lab for LangChain! In this lab, we will:</p>\n",
        "  <p style=\"margin-top: 10px; font-size: 18px; font-weight: bold;\">Instructor: Dr. Dehghani</p>\n",
        "  <p style=\"margin-top: 5px; font-size: 14px;\">\n",
        "    Learn more at <a href=\"https://langchain.com\" target=\"_blank\" style=\"color: #ffdd57; text-decoration: underline;\">LangChain Website</a>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 20px; border: 1px solid #0055d4;\">\n",
        "  <p style=\"line-height: 1.6; font-size: 16px; margin-bottom: 15px;\">\n",
        "    LangChain is a framework for building applications powered by large language models (LLMs).  \n",
        "    It provides modular components‚Äîsuch as prompt templates, memory, and chains‚Äîthat make it easy to develop, test, and deploy LLM-based solutions.\n",
        "  </p>\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">Lab Objectives</h2>\n",
        "  <ul style=\"line-height: 1.8; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li>Set up LangChain in Google Colab</li>\n",
        "    <li>Interact with OpenAI models</li>\n",
        "    <li>Work with open-source models like Falcon</li>\n",
        "  </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7683f7f",
      "metadata": {
        "id": "c7683f7f"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Install only the essential packages for LangChain with OpenAI & Gemini support\n",
        "\n",
        "!pip install -q --upgrade langchain                # Core LangChain framework for building LLM workflows\n",
        "!pip install -q --upgrade langchain-community      # Community integrations (still useful for many non-OpenAI/Gemini models)\n",
        "!pip install -q --upgrade langchain-openai         # ‚úÖ NEW: Dedicated package for OpenAI integrations\n",
        "!pip install -q --upgrade langchain-google-genai   # Integration for Google's Gemini models\n",
        "!pip install -q --upgrade openai                   # OpenAI SDK for native API calls (not strictly needed for LangChain, but often useful)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6843a5c",
      "metadata": {
        "id": "c6843a5c"
      },
      "source": [
        "## üîë Step 2: Set Up OpenAI API Key\n",
        "If you want to use OpenAI models like GPT-4, you need an API key. Run the code below and enter your key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba2e457",
      "metadata": {
        "id": "fba2e457"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Load API Keys from Colab Secrets\n",
        "# ==================================\n",
        "\n",
        "import os                                  # Used to set environment variables for API keys\n",
        "from google.colab import userdata          # To securely access stored secrets in Colab\n",
        "\n",
        "# Retrieve your stored secrets (API keys)\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')   # OpenAI API key for GPT models\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')   # Google Gemini API key for Gemini models\n",
        "\n",
        "# Set environment variables for the APIs and confirm success\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY   # Set OpenAI key as environment variable\n",
        "    print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API key not found. Please set 'OPENAI_API_KEY' in Colab secrets.\")\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY   # Set Gemini key as environment variable\n",
        "    print(\"‚úÖ Google Gemini API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Google Gemini API key not found. Please set 'GEMINI_API_KEY' in Colab secrets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27zoTSlqMFr",
      "metadata": {
        "id": "e27zoTSlqMFr"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">ü§ñ LLM Connection Check (via LangChain)</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">\n",
        "    This step tests if <strong>LangChain</strong> can communicate with your selected language model‚Äîsuch as OpenAI, Gemini, or others.<br>\n",
        "    All messaging with the LLM is handled by LangChain, giving you a flexible and unified workflow.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 18px; margin-bottom: 22px; border: 1px solid #0055d4;\">\n",
        "  <ol style=\"line-height: 1.7; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li><strong>We call the language model through LangChain</strong> ‚Äî not directly. LangChain manages the connection and delivers the response. üåê</li>\n",
        "    <li>If you get a valid reply, your setup is good to go for any LLM provider! üéØ</li>\n",
        "  </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; border-radius: 12px; padding: 18px; border: 1px solid #0055d4; margin-bottom: 22px;\">\n",
        "  <h2 style=\"color: #0055d4; font-size: 22px; margin-top: 0; margin-bottom: 10px;\">üîπ How <code>.invoke()</code> Works</h2>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 12px;\">\n",
        "    The <code>.invoke()</code> method is the main way to send prompts (messages) to a language model in LangChain.<br>\n",
        "    You wrap your input in a <code>HumanMessage</code> (or <code>SystemMessage</code>, etc.), then pass it as a list.<br>\n",
        "    LangChain sends the message to the LLM and returns a response object.\n",
        "  </p>\n",
        "  <pre style=\"background: #f7faff; border-radius: 8px; padding: 12px; font-size: 15px; border: 1px solid #cce0ff;\">\n",
        "response = llm.invoke([HumanMessage(content=\"Summarize LangChain in one sentence.\")])\n",
        "print(response.content)  # ‚¨ÖÔ∏è This gives you the model's reply\n",
        "  </pre>\n",
        "  <p style=\"font-size: 15px; margin: 0;\">\n",
        "    <strong>Tip:</strong> This workflow is the same for all LLM providers supported by LangChain!\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HpUBxiUZqLqU",
      "metadata": {
        "id": "HpUBxiUZqLqU"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# üåü LangChain Connection Test: OpenAI (Generalized)\n",
        "# =========================================================\n",
        "\n",
        "# üö© Check available OpenAI models at: https://platform.openai.com/docs/models/\n",
        "#   (Recommended: 'gpt-3.5-turbo', 'gpt-4o', 'gpt-4-turbo', etc.)\n",
        "\n",
        "# 1Ô∏è‚É£ Import the correct ChatOpenAI class (from the updated package!)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# 2Ô∏è‚É£ Set your desired model (update to any available OpenAI chat model as needed)\n",
        "OPENAI_MODEL = \"gpt-3.5-turbo\"    # <-- Change this string to any OpenAI chat model you have access to\n",
        "\n",
        "# 3Ô∏è‚É£ Initialize the LLM connection\n",
        "llm = ChatOpenAI(\n",
        "    model=OPENAI_MODEL,\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£ Define your prompt\n",
        "prompt = \"What is LangChain in one short sentence?\"\n",
        "\n",
        "# 5Ô∏è‚É£ Invoke the model (send the prompt as a HumanMessage)\n",
        "response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "# 6Ô∏è‚É£ Display the response\n",
        "print(\"üîπ OpenAI Response:\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================\n",
        "# üßê Inspecting the LLM Response Object\n",
        "# =======================================\n",
        "\n",
        "# üè∑Ô∏è Print the type of the response object\n",
        "print(\"üîñ Type of response:\", type(response))\n",
        "\n",
        "# üß© Print the raw response object (see all attributes/methods)\n",
        "print(\"\\nüîé Raw response object:\\n\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "_81DG3631GhY"
      },
      "id": "_81DG3631GhY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49B7fPC_sM6g",
      "metadata": {
        "id": "49B7fPC_sM6g"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Reading the LLM's Response**\n",
        "# ============================================\n",
        "\n",
        "# Replace '-----' in the placeholders with the correct method or key to retrieve the requested information.\n",
        "\n",
        "# Task 1: Get the main content from the LLM response\n",
        "response_text = response.-----  # Extract the main response text (e.g., \"content\" for ChatOpenAI)\n",
        "\n",
        "# Task 2: Get the number of prompt tokens used\n",
        "prompt_tokens = response.-----  # Extract the number of tokens used in the input prompt\n",
        "\n",
        "# Task 3: Get the number of response tokens generated\n",
        "response_tokens = response.-----  # Extract the number of tokens used in the generated response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ng1KX4E_xXqH",
      "metadata": {
        "id": "Ng1KX4E_xXqH"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 22px 28px 16px 28px; border-radius: 12px; margin-bottom: 18px; text-align: center;\">\n",
        "  <h2 style=\"font-size: 26px; margin-bottom: 8px;\">üí¨ Multi-Turn Conversation in LangChain</h2>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 0;\">\n",
        "    A <strong>multi-turn conversation</strong> allows an AI to retain context across multiple exchanges, making interactions more natural and intelligent.<br>\n",
        "    Instead of treating each query independently, the AI builds on previous inputs, improving coherence and accuracy. <span style=\"font-size: 22px;\">ü§ù</span>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 18px; margin-bottom: 22px; border: 1px solid #0055d4;\">\n",
        "  <h3 style=\"color: #0055d4; font-size: 20px; margin-top: 0;\">‚ú® Why Use Multi-Turn Conversations?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0; padding-left: 20px; line-height: 1.7;\">\n",
        "    <li><strong>Context Retention</strong> ‚Äì AI remembers past interactions, leading to more relevant responses.</li>\n",
        "    <li><strong>Realistic Dialogue</strong> ‚Äì Mimics human conversations, making chatbots more engaging.</li>\n",
        "    <li><strong>Improved Accuracy</strong> ‚Äì Responses are refined based on earlier exchanges.</li>\n",
        "    <li><strong>Scalable Design</strong> ‚Äì Supports long-form discussions without losing context.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15px; margin-top: 12px;\">\n",
        "    <em>This approach is ideal for applications like <b>financial advisors, chatbots, research assistants</b>, and other AI-driven services that require ongoing, dynamic conversations.</em>\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VPScknqNr28z",
      "metadata": {
        "id": "VPScknqNr28z"
      },
      "outputs": [],
      "source": [
        "# üåü Multi-Turn Conversation Example: Coffee History (Concise Responses)\n",
        "# =======================================================================\n",
        "\n",
        "# Import message classes: SystemMessage (AI behavior), HumanMessage (user input), AIMessage (AI responses)\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Instruct the AI to act as a coffee historian and reply in one short sentence\n",
        "messages = [\n",
        "   SystemMessage(content=\"You are a coffee historian. Provide concise, one-sentence answers.\"),\n",
        "   HumanMessage(content=\"What is the origin of coffee?\"),\n",
        "   AIMessage(content=\"Coffee was first discovered in the Kaffa region of Ethiopia in the 9th century.\"),\n",
        "   HumanMessage(content=\"How did coffee spread beyond Ethiopia?\"),\n",
        "   AIMessage(content=\"It traveled via trade routes into Yemen and then throughout the Ottoman Empire.\"),\n",
        "   HumanMessage(content=\"When did coffee reach Europe?\")\n",
        "]\n",
        "\n",
        "# Send the structured messages to your initialized LangChain model\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)  # Expect a one-sentence response on coffee reaching Europe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jhpvSXq7yCv_",
      "metadata": {
        "id": "jhpvSXq7yCv_"
      },
      "outputs": [],
      "source": [
        "# ‚úã **Hands-On: Completing a Multi-Turn Conversation (Travel Assistant)**\n",
        "#==========================================================================\n",
        "\n",
        "# üìå Task Instructions:\n",
        "# - Below is a conversation with a **travel assistant** AI.\n",
        "# - Fill in the last `HumanMessage` with a relevant travel-related question.\n",
        "# - Complete the placeholder `response = ----(----)` to correctly call the LLM.\n",
        "\n",
        "# Define a conversation with missing parts\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful travel assistant, providing recommendations for destinations and travel tips.\"),\n",
        "    HumanMessage(content=\"What are some must-visit places in Japan?\"),\n",
        "    AIMessage(content=\"Some must-visit places in Japan include Tokyo, Kyoto, and Osaka. Each city offers unique cultural and historical experiences.\"),\n",
        "    HumanMessage(content=\"-----\")  #  Task: Fill in a relevant follow-up question\n",
        "]\n",
        "\n",
        "# üîß Task: Complete the function to generate a response from the LLM\n",
        "response = ----(----)  # Call the LLM correctly using the messages\n",
        "\n",
        "print(response)  # Display the AI's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 18px;\"> <h2 style=\"margin: 0 0 10px 0;\">üîπ What is a Multi-LLM Model?</h2> <p style=\"margin: 0;\"> A <strong>Multi-LLM model</strong> is a setup where you can send the same question or conversation to multiple large language models (LLMs)‚Äîfor example, OpenAI's GPT-4 and Google's Gemini‚Äîat the same time. This allows you to compare their answers, pick the best response, or even blend their strengths for more reliable results. </p> </div> <div style=\"background: #f0f5ff; border-radius: 12px; padding: 16px; border: 1px solid #0055d4;\"> <h3 style=\"color: #0055d4; margin: 0 0 8px 0;\">Why Does LangChain Make This Easy?</h3> <ul style=\"padding-left: 20px; margin: 0; font-size: 16px;\"> <li><b>Unified Interface:</b> LangChain lets you connect to many LLM providers (OpenAI, Google, Hugging Face, and more) using the same simple code.</li> <li><b>Modular Chaining:</b> You can send the same messages or workflows to any model without rewriting your logic for each one.</li> <li><b>Rapid Experimentation:</b> Instantly compare outputs, performance, or reliability from different models‚Äîhelping you choose the right LLM for your app or research.</li> </ul> </div> <div style=\"background: #fff3f3; border-radius: 12px; padding: 12px; border: 1px solid #ff6b6b; color: #d32f2f;\"> <b>In short:</b> <i>LangChain empowers you to use, compare, and combine multiple AI models in one unified workflow‚Äîmaking your LLM projects more flexible and future-proof.</i> </div>"
      ],
      "metadata": {
        "id": "Kg0J6tUbHOk5"
      },
      "id": "Kg0J6tUbHOk5"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ú® Multi-LLM Comparison: Gemini vs. ChatGPT (Coffee in 2050)\n",
        "# ==========================================================================\n",
        "\n",
        "# üìù Imports explained:\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage   # For chat message formatting (system/user/AI)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI             # Google's Gemini LLM via LangChain\n",
        "from langchain.chat_models import ChatOpenAI                          # OpenAI models (ChatGPT, GPT-4) via LangChain\n",
        "import pandas as pd                                                   # For tabular storage and display\n",
        "\n",
        "# üü¶ System and User Setup: Future Coffee Culture\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a fun futurist AI barista. Predict and explain what coffee culture will look like in the year 2050.\"),\n",
        "    HumanMessage(content=\"Describe the biggest change in how people enjoy coffee in 2050 in few sentnene.\"),\n",
        "]\n",
        "\n",
        "# ‚òï Initialize LLMs (update model names as needed)\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7)\n",
        "chatgpt_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# ü§ñ Generate responses from both models\n",
        "gemini_response = gemini_llm.invoke(messages)\n",
        "chatgpt_response = chatgpt_llm.invoke(messages)\n",
        "\n",
        "# üìä Store responses in a DataFrame for easy comparison and future use\n",
        "df = pd.DataFrame({\n",
        "    \"Model\": [\"Gemini\", \"ChatGPT\"],\n",
        "    \"Response\": [gemini_response.content, chatgpt_response.content]\n",
        "})\n",
        "\n",
        "# üñ®Ô∏è Print the DataFrame as a table (plain text)\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "xBAmR3BNGzqZ"
      },
      "id": "xBAmR3BNGzqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# üìä LLM Math Reasoning Challenge: Gemini vs. ChatGPT (Lab Style)\n",
        "# =============================================================\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt   # For plotting response times\n",
        "import pandas as pd               # For tabular storage and display\n",
        "\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# üìù Math Problem Prompt: Can LLMs Reason Like Mathematicians?\n",
        "# -------------------------------------------------------------\n",
        "prompt = (\n",
        "    \"Solve for x in the equation: 3x^3 + 5x^2 - 12x + 7 = 21. \"\n",
        "    \"Highlight answers by ---[ x1=  ] ---\"\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# ü§ñ Initialize LLMs: Gemini & ChatGPT (Low Temp for Accuracy)\n",
        "# -------------------------------------------------------------\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3)\n",
        "chatgpt_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# ‚è±Ô∏è Get LLM Responses & Measure Response Time\n",
        "# -------------------------------------------------------------\n",
        "start_gemini = time.time()\n",
        "gemini_response = gemini_llm.invoke([HumanMessage(content=prompt)])\n",
        "end_gemini = time.time()\n",
        "gemini_time = end_gemini - start_gemini\n",
        "\n",
        "start_chatgpt = time.time()\n",
        "chatgpt_response = chatgpt_llm.invoke([HumanMessage(content=prompt)])\n",
        "end_chatgpt = time.time()\n",
        "chatgpt_time = end_chatgpt - start_chatgpt\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# üìã Store Model Outputs & Timing in a Table\n",
        "# -------------------------------------------------------------\n",
        "responses = pd.DataFrame({\n",
        "    \"Model\": [\"Gemini\", \"ChatGPT\"],\n",
        "    \"Response\": [gemini_response.content, chatgpt_response.content],\n",
        "    \"Response Time (s)\": [gemini_time, chatgpt_time]\n",
        "})\n",
        "\n",
        "# üñ•Ô∏è Display table for easy side-by-side review\n",
        "display(responses)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# üìà Visualize: Response Time Comparison (Brand Colors)\n",
        "# -------------------------------------------------------------\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(\n",
        "    responses[\"Model\"],\n",
        "    responses[\"Response Time (s)\"],\n",
        "    color=[\"#00E3FF\", \"#10A37F\"]  # Gemini blue, OpenAI green\n",
        ")\n",
        "plt.ylabel(\"Response Time (seconds)\", fontsize=12)\n",
        "plt.title(\"LLM Math Problem Response Time\", fontsize=14, color=\"#0055d4\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# üßê Notes & Analysis:\n",
        "#   - The equation 3x^3 + 5x^2 - 12x + 7 = 21 has three real solutions:\n",
        "#         x = -1,   x ‚âà 1.852,   x ‚âà -2.519\n",
        "#   - Review the 'Response' column above: Does the LLM show clear reasoning?\n",
        "#   - Did the model provide a step-by-step solution, not just the final answer?\n",
        "# -------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "zG6lB7FONaGG"
      },
      "id": "zG6lB7FONaGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üßë‚Äçüíª Hands-On: Exploring LLM Prompt Engineering & Model Choice\n",
        "# ============================================================\n",
        "\n",
        "\"\"\"\n",
        "Task:\n",
        "1. Choose a different math equation (e.g., a different cubic, a system of equations, or even a word problem).\n",
        "2. Experiment with your prompt style! For example:\n",
        "    - Ask the LLM to \"show every step\"\n",
        "    - Use ReAct: \"First, reflect on what is needed, then solve, then verify your solution\"\n",
        "    - Ask for self-evaluation: \"Explain why your answer makes sense\"\n",
        "3. Try at least TWO different LLMs (e.g., Gemini, ChatGPT, or another you have access to).\n",
        "4. Record response content and timing in a DataFrame.\n",
        "5. At the end, **write down your observations**:\n",
        "    - Did the prompt style or LLM change the accuracy or reasoning?\n",
        "    - Which model did better? Was ReAct or self-checking effective?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "yPe3I6ldR6WI"
      },
      "id": "yPe3I6ldR6WI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üîé Gemini Model Explorer: List Top 10 Newest Models\n",
        "# ============================================================\n",
        "\n",
        "# Import the Gemini model listing function\n",
        "from google.generativeai import list_models\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üöÄ Fetch & Display: Newest Gemini Models (Top 10)\n",
        "# ------------------------------------------------------------\n",
        "gemini_data = []\n",
        "try:\n",
        "    model_list = list(list_models())\n",
        "    # üóÇÔ∏è Sort by model name in reverse (often, higher = newer)\n",
        "    model_list_sorted = sorted(model_list, key=lambda m: m.name, reverse=True)\n",
        "    for m in model_list_sorted[:10]:  # Show only the top 10\n",
        "        gemini_data.append({\n",
        "            \"Model Name\": m.name,\n",
        "            \"Supported Methods\": \", \".join(m.supported_generation_methods)\n",
        "        })\n",
        "    df_gemini = pd.DataFrame(gemini_data)\n",
        "\n",
        "    print(\"üîµ === Top 10 Newest Gemini Models ===\")\n",
        "    display(df_gemini)\n",
        "\n",
        "    if not gemini_data:\n",
        "        print(\"‚ö†Ô∏è No Gemini models found. Your API key may not have access, or Gemini is not available in your region/account.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error:\", e)\n"
      ],
      "metadata": {
        "id": "oABOMIFcSjbF"
      },
      "id": "oABOMIFcSjbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üîµ OpenAI Model Explorer: List Newest Models & Metadata\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# üóùÔ∏è Load OpenAI API Key from Environment\n",
        "# ------------------------------------------------------------\n",
        "openai_key = os.environ.get(\"OPENAI_API_KEY\", None)\n",
        "model_data = []\n",
        "\n",
        "if openai_key:\n",
        "    try:\n",
        "        # ü§ñ Connect to OpenAI and retrieve models\n",
        "        client = OpenAI(api_key=openai_key)\n",
        "        # üìã Sort models by name/id in reverse (newest first)\n",
        "        models = sorted(client.models.list().data, key=lambda m: m.id, reverse=True)\n",
        "        for m in models[:15]:  # Show up to 15 models\n",
        "            model_data.append({\n",
        "                \"Model Name\": m.id,\n",
        "                \"Owned By\": getattr(m, \"owned_by\", \"\"),\n",
        "                \"Created\": getattr(m, \"created\", \"\"),\n",
        "                \"Object\": getattr(m, \"object\", \"\")\n",
        "                # Note: Purpose/Description not present in API!\n",
        "            })\n",
        "        df = pd.DataFrame(model_data)\n",
        "        print(\"üîµ === Newest OpenAI Models: Metadata ===\")\n",
        "        display(df)\n",
        "        if not model_data:\n",
        "            print(\"‚ö†Ô∏è No OpenAI models found. Your API key may not have access, or your account is limited.\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error checking OpenAI API key:\", str(e))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No OpenAI API key found in environment.\")\n"
      ],
      "metadata": {
        "id": "U9wnZ102LU9V"
      },
      "id": "U9wnZ102LU9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 24px; border-radius: 12px; margin-bottom: 20px;\">\n",
        "  <h2 style=\"margin-top:0; font-size: 26px;\">ü§ó Introduction: Hugging Face & Classic LLMs</h2>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    <strong>Hugging Face</strong> is an open-source platform that hosts thousands of ready-to-use machine learning models, making it easy to experiment with and deploy powerful language models locally or in the cloud.<br>\n",
        "    In this section, we'll compare two popular Hugging Face models:\n",
        "  </p>\n",
        "  <ul style=\"font-size: 16px; margin: 18px 0 8px 18px;\">\n",
        "    <li><span style=\"font-size: 20px;\">ü§ñ</span> <b>GPT-2</b> ‚Äì One of the first large language models to generate human-like text. It set the stage for the modern LLM boom, and remains a classic benchmark.</li>\n",
        "    <li><span style=\"font-size: 20px;\">‚ö°</span> <b>DistilGPT2</b> ‚Äì A smaller, faster, and more efficient version of GPT-2. It offers nearly the same capabilities but with lighter resource requirements, making it ideal for quick prototyping.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15px;\">\n",
        "    We'll see how each model responds to creative prompts and compare their outputs side by side!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "AO3XoMtuOB23"
      },
      "id": "AO3XoMtuOB23"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üõ†Ô∏è Setup: Install & Import Required Packages for HF Pipelines\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q --upgrade langchain langchain-huggingface transformers\n",
        "\n",
        "from langchain_huggingface import HuggingFacePipeline   # Wraps Hugging Face models for LangChain\n",
        "from transformers import pipeline                       # Builds local text-generation pipelines\n",
        "from langchain.schema import HumanMessage               # Formats chat/user messages for LLMs\n"
      ],
      "metadata": {
        "id": "fHxdumvtNrn5"
      },
      "id": "fHxdumvtNrn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# ü§ó‚òï Compare Local Hugging Face LLMs: GPT-2 vs. DistilGPT2\n",
        "# =============================================================\n",
        "\n",
        "import pandas as pd  # For tabular output\n",
        "\n",
        "model_ids = [\n",
        "    (\"ü§ñ GPT-2\", \"gpt2\"),\n",
        "    (\"‚ö° DistilGPT2\", \"distilgpt2\"),\n",
        "]\n",
        "\n",
        "prompt = \"What will be the future of AI in 2050?!\"\n",
        "\n",
        "responses = []\n",
        "\n",
        "for model_name, model_id in model_ids:\n",
        "    # 'pipeline' builds a ready-to-use text-generation model from Hugging Face with a single command\n",
        "    hf_pipe = pipeline(\"text-generation\", model=model_id, max_new_tokens=250)\n",
        "    llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    responses.append({\"Model\": model_name, \"Response\": response.strip()})\n",
        "\n",
        "# Save and display the DataFrame of responses\n",
        "df_response = pd.DataFrame(responses)\n",
        "print(\"üîµ Model responses to the prompt:\")\n",
        "display(df_response)\n"
      ],
      "metadata": {
        "id": "lncdmRGjAXp0"
      },
      "id": "lncdmRGjAXp0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# üß™ Hands-On: Getting Better Answers from DistilGPT2\n",
        "# =============================================================\n",
        "\n",
        "# Background:\n",
        "# DistilGPT2 is a smaller, faster version of GPT-2.\n",
        "# While it's efficient, it often just repeats the prompt or gives very generic answers.\n",
        "# This happens because it isn't specifically trained to follow instructions or answer questions directly.\n",
        "\n",
        "# Your Task:\n",
        "# 1. Try using DistilGPT2 for an open-ended question (e.g., \"What will be the future of AI in 2050?\").\n",
        "# 2. Observe if it repeats the prompt or gives an unhelpful answer.\n",
        "\n",
        "# To improve its output, experiment with these techniques:\n",
        "#   - Make your prompt longer or more explicit (e.g., \"Q: What is the future of AI in 2050?\\nA:\").\n",
        "#   - Add clear instructions or a separator (e.g., \"Answer in 2 sentences: ...\").\n",
        "#   - Enable sampling and set temperature higher (e.g., do_sample=True, temperature=0.8).\n",
        "#   - Try using the full GPT-2 model for comparison.\n",
        "\n",
        "# For each approach:\n",
        "# - Record what happens. Does the model's output improve? When does it still repeat or ignore the prompt?\n",
        "# - Compare your best DistilGPT2 output to GPT-2‚Äôs response.\n",
        "# - Summarize your observations and submit them as your exercise report.\n"
      ],
      "metadata": {
        "id": "eYaR5rHMR_4s"
      },
      "id": "eYaR5rHMR_4s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; line-height: 1.6; color: #333;\">\n",
        "  <h2 style=\"color: #0055d4; border-bottom: 2px solid #0055d4; padding-bottom: 8px;\">\n",
        "    The Leap from GPT-2 to Today‚Äôs Super-Models\n",
        "  </h2>\n",
        "  <p>\n",
        "    Back in 2019, <strong>GPT-2</strong> shook the world with its 1.5 billion parameters (and even a distilled 82 million version) by generating surprisingly coherent paragraphs from web-scraped text. It proved that transformers could model language at scale‚Äîbut still struggled with long-form reasoning, factual accuracy, and open-ended prompts.\n",
        "  </p>\n",
        "  <p>\n",
        "    Its lighter sibling, <strong>DistilGPT2</strong>, was optimized for speed and efficiency but often echoes the prompt or produces very basic completions, especially for creative or complex questions. This is a common limitation with small, ‚Äúvanilla‚Äù language models that haven‚Äôt been tuned for instruction or dialogue.\n",
        "  </p>\n",
        "  <p>\n",
        "    Fast forward to today: modern LLMs boast tens or hundreds of billions of parameters, trained on diverse, multilingual data and fine-tuned with human feedback. They not only write essays and debug code, but also follow instructions, explain reasoning, and adapt to specialized domains. The leap from ‚Äúsometimes impressive, often generic‚Äù to ‚Äúgenuinely useful and insightful‚Äù has happened in just a few years‚Äîa testament to the rapid evolution of generative AI.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "gqpheBxXLhL7"
      },
      "id": "gqpheBxXLhL7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #f0f5ff 0%, #e6f0ff 100%); border-radius: 12px; padding: 25px; margin-bottom: 25px; border: 1px solid #0055d4;\">\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 26px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">ü¶Ö Falcon: High-Performance Open-Source Large Language Model</h2>\n",
        "  <p style=\"font-size: 16px; color: #222; line-height: 1.7; margin-bottom: 18px;\">\n",
        "    <b>Falcon</b> is a state-of-the-art, open-source large language model (LLM) developed by the <a href=\"https://www.tii.ae/\" target=\"_blank\" style=\"color:#0055d4;text-decoration:underline;\"><b>Technology Innovation Institute (TII)</b></a> in Abu Dhabi. Falcon stands out for its high efficiency and performance, and is widely adopted in both research and industry.\n",
        "  </p>\n",
        "  <ul style=\"margin-bottom: 18px; font-size: 16px;\">\n",
        "    <li>üöÄ Available in multiple sizes: <b>Falcon-7B</b> and <b>Falcon-40B</b></li>\n",
        "    <li>‚ö° Optimized for low memory usage and fast inference</li>\n",
        "    <li>üõ†Ô∏è Supports a broad range of NLP tasks (text generation, summarization, chat, and more)</li>\n",
        "    <li>üåê Fully open-source with community support on Hugging Face</li>\n",
        "  </ul>\n",
        "  <div style=\"margin-bottom: 10px; font-size: 16px;\">\n",
        "    <b>Useful Resources:</b>\n",
        "    <ul>\n",
        "      <li><a href=\"https://huggingface.co/tiiuae/falcon-7b\" target=\"_blank\" style=\"color:#0055d4;\">Falcon-7B Model Card (Hugging Face)</a></li>\n",
        "      <li><a href=\"https://github.com/tiiuae/falcon-llm\" target=\"_blank\" style=\"color:#0055d4;\">Falcon GitHub Repository</a></li>\n",
        "    </ul>\n",
        "  </div>\n",
        "  <div style=\"background: #e6f0ff; border-radius: 8px; padding: 14px 18px; margin-top: 16px; border-left: 5px solid #0055d4;\">\n",
        "    <b>Why Falcon?</b> <br>\n",
        "    Falcon models are a powerful choice for building efficient, scalable AI applications, and serve as a strong open-source alternative to proprietary LLMs.\n",
        "  </div>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "GpUYslkAVzK6"
      },
      "id": "GpUYslkAVzK6"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# =============================================================\n",
        "# ü¶æ Falcon-7B: Local Model Loading & Pipeline Initialization\n",
        "# =============================================================\n",
        "\n",
        "- Loads the Falcon-7B large language model and tokenizer using Hugging Face `transformers`.\n",
        "- Sets up a text-generation pipeline that runs efficiently on your Colab or local GPU (using bfloat16 precision and auto device selection).\n",
        "- This pipeline lets you generate human-like text locally‚Äîno internet or external API needed once the model is downloaded.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Specify the model name (Falcon-7B)\n",
        "model = \"tiiuae/falcon-7b\"\n",
        "\n",
        "# Load the tokenizer for the Falcon-7B model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# Build a text-generation pipeline for efficient local inference\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",           # Task: generate text\n",
        "    model=model,                 # Model to use\n",
        "    tokenizer=tokenizer,         # Tokenizer to use\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency (if supported)\n",
        "    device_map=\"auto\",           # Automatically use GPU if available\n",
        ")\n"
      ],
      "metadata": {
        "id": "8RPdlrhzU63V"
      },
      "id": "8RPdlrhzU63V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ‚ö°Ô∏è How Does This Cell Work?\n",
        ">\n",
        "> - The code imports ü§ó `transformers` and loads the Falcon-7B model directly into your Colab session.\n",
        "> - Downloading and setting up the model **can take several minutes** (due to the large size and initial processing).\n",
        "> - **Falcon-7B is an open-source model**: you don't need any API key, and all text generation happens locally on your Colab GPU‚Äîno outside cloud service required!\n"
      ],
      "metadata": {
        "id": "SbAb1p6WXXSk"
      },
      "id": "SbAb1p6WXXSk"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# =============================================================\n",
        "# ‚òïü¶æ Falcon-7B CalmMindBot: AI Multi-Turn Wellness Demo\n",
        "# =============================================================\n",
        "\"\"\"\n",
        "\n",
        "prompt = (\n",
        "    \"CalmMindBot is a compassionate AI assistant who supports people with stress and emotional wellness.\"\n",
        "    \"\\nUser: Hi CalmMindBot, I‚Äôve been feeling overwhelmed lately. What‚Äôs a simple way to relax?\"\n",
        "    \"\\nCalmMindBot:\"\n",
        ")\n",
        "\n",
        "sequences = text_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=600,        # Allow for several conversation turns\n",
        "    do_sample=True,\n",
        "    top_k=15,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    full_text = seq['generated_text']\n",
        "    # Split at each role's tag (User: or CalmMindBot:)\n",
        "    import re\n",
        "    dialogue_lines = re.split(r'(User:|CalmMindBot:)', full_text)\n",
        "    # The split leaves ['', 'User:', '...', 'CalmMindBot:', '...', ...]\n",
        "    # So we iterate in steps of 2, skipping any empty initial\n",
        "    for i in range(1, len(dialogue_lines), 2):\n",
        "        speaker = dialogue_lines[i].strip()\n",
        "        content = dialogue_lines[i+1].strip()\n",
        "        if speaker == \"User:\":\n",
        "            print(f\"üë§ User: {content}\\n\")\n",
        "        elif speaker == \"CalmMindBot:\":\n",
        "            print(f\"ü§ñ CalmMindBot: {content}\\n\")\n"
      ],
      "metadata": {
        "id": "tUk6h8yRYt-N"
      },
      "id": "tUk6h8yRYt-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ü§î How Does a Transformer Model Like Falcon-7B Generate Text?\n",
        ">\n",
        "> - The `transformers` library loads the Falcon-7B model‚Äôs weights into your computer or Colab GPU.\n",
        "> - When you give it a prompt, the model **processes each word (token) step by step**, multiplying learned weights (from its neural network) to predict the most likely next word.\n",
        "> - This process repeats, one token at a time, until the model completes its response‚Äîcreating answers that seem natural and relevant.\n",
        ">\n",
        "> In short: you provide a starting message, and the model uses its ‚Äúlearned math‚Äù (weights) to generate a reply, token by token!\n"
      ],
      "metadata": {
        "id": "pBLVEb4_YB7l"
      },
      "id": "pBLVEb4_YB7l"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úã **Hands-On: Querying Falcon-7B with LangChain**\n",
        "# Replace the placeholders to:\n",
        "# 1Ô∏è‚É£ Initialize the Hugging Face model correctly.\n",
        "# 2Ô∏è‚É£ Use the correct method to query the model.\n",
        "# 3Ô∏è‚É£ Store the response in the right variable.\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ‚úÖ Step 1: Initialize the Falcon model\n",
        "llm_falcon = ----- (repo_id=\"tiiuae/falcon-7b-instruct\")  # üîß Replace '-----' with the correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define the question\n",
        "question = \"How will AI impact the job market in the next decade?\"\n",
        "\n",
        "# ‚úÖ Step 3: Query the model\n",
        "response_falcon = llm_falcon.----- (question)  # üîß Replace '-----' with the correct method\n",
        "\n",
        "# ‚úÖ Display the response\n",
        "print(\"üîπ Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "NcHL_zVXDoA0"
      },
      "id": "NcHL_zVXDoA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You have successfully completed the **Introduction to LangChain** lab. üöÄ  \n",
        "We hope you found it insightful and are excited to explore more!  \n",
        "\n",
        "üí° **Next Steps:**  \n",
        "- Try using different LLMs in LangChain.  \n",
        "- Experiment with structured vs. plain text prompts.  \n",
        "- Explore advanced features like memory and chains.\n",
        "\n",
        "Happy Coding! üíª‚ú®  \n"
      ],
      "metadata": {
        "id": "BqzZxacjPK6k"
      },
      "id": "BqzZxacjPK6k"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eu2mJRbHp5b7"
      },
      "id": "eu2mJRbHp5b7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}