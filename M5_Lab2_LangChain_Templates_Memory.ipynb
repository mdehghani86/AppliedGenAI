{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 27px; letter-spacing: 0.5px;\">\n",
        "    üß† LangChain Lab 2: Prompt Templates &amp; Memory\n",
        "  </h2>\n",
        "  <p style=\"font-size: 17px; margin-bottom: 8px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Instructor:</span> Prof. Dehghani\n",
        "  </p>\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 12px;\">Lab Overview</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 14px;\">\n",
        "    In this lab, you'll enhance your interactions with LLMs by using <b>Prompt Templates</b> and <b>Memory</b> features in LangChain.<br>\n",
        "    You'll learn to create <b>structured prompts dynamically</b> and maintain <b>conversation history</b> across multiple turns.\n",
        "  </p>\n",
        "  <hr style=\"border: 1px solid #3f77d4; margin: 16px 0;\">\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">üéØ What You'll Learn</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 10px 22px; line-height: 1.8;\">\n",
        "    <li>üîπ <b>Prompt Templates</b> ‚Äì Format inputs dynamically for LLMs.</li>\n",
        "    <li>üîπ <b>Memory in LangChain</b> ‚Äì Maintain context in multi-turn conversations.</li>\n",
        "    <li>üîπ <b>Hands-on exercises</b> ‚Äì Reinforce concepts with practical coding tasks.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15.5px; margin-top: 8px;\">\n",
        "    By the end, you'll be able to structure prompts effectively and implement conversational memory in LangChain applications. üöÄ\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚öôÔ∏è Install essential packages"
      ],
      "metadata": {
        "id": "8k2xSdP5989g"
      },
      "id": "8k2xSdP5989g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd"
      },
      "outputs": [],
      "source": [
        "#‚öôÔ∏è Install essential packages for LangChain with OpenAI & Gemini support\n",
        "\n",
        "!pip -q install -U \\\n",
        "  langchain \\\n",
        "  langchain-core \\\n",
        "  langchain-community \\\n",
        "  langchain-openai \\\n",
        "  langchain-google-genai \\\n",
        "  langchain-classic \\\n",
        "  openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîë Step 2: Set Up OpenAI API Key"
      ],
      "metadata": {
        "id": "C9B4mXQr93u8"
      },
      "id": "C9B4mXQr93u8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8eaa18-e547-41f3-de12-c63890877639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded successfully!\n",
            "‚úÖ Google Gemini API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è Load API Keys from Colab Secrets\n",
        "# ==================================\n",
        "\n",
        "import os                                  # Used to set environment variables for API keys\n",
        "from google.colab import userdata          # To securely access stored secrets in Colab\n",
        "\n",
        "# Retrieve your stored secrets (API keys)\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')   # OpenAI API key for GPT models\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')   # Google Gemini API key for Gemini models\n",
        "\n",
        "# Set environment variables for the APIs and confirm success\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY   # Set OpenAI key as environment variable\n",
        "    print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API key not found. Please set 'OPENAI_API_KEY' in Colab secrets.\")\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY   # Set Gemini key as environment variable\n",
        "    print(\"‚úÖ Google Gemini API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Google Gemini API key not found. Please set 'GEMINI_API_KEY' in Colab secrets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5047273f",
      "metadata": {
        "id": "5047273f"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 25px; letter-spacing: 0.5px;\">üìù Prompt Templates in LangChain</h2>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 8px;\">üîπ What are Prompt Templates?</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 12px;\">\n",
        "    Prompt Templates let you <b>dynamically format prompts</b> by inserting variables, making interactions with LLMs more flexible and reusable.<br>\n",
        "    Instead of writing static text, you can use placeholders that are filled in with real values at runtime.\n",
        "  </p>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 8px;\">üîπ Why Use Prompt Templates?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.8;\">\n",
        "    <li>‚úÖ <b>Reusability</b> ‚Äì No need for repetitive prompts.</li>\n",
        "    <li>‚úÖ <b>Dynamic Inputs</b> ‚Äì Easily personalize prompts with new user data.</li>\n",
        "    <li>‚úÖ <b>Consistency</b> ‚Äì Keeps your prompt formatting structured and reliable.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"font-size: 17px; margin-bottom: 8px;\">üìå Example Usage</h3>\n",
        "  <div style=\"background: rgba(255,255,255,0.08); border-radius: 7px; padding: 11px 16px; margin-bottom: 6px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Static prompt:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of AI in healthcare?\"\n",
        "    </span>\n",
        "  </div>\n",
        "  <div style=\"background: rgba(255,255,255,0.08); border-radius: 7px; padding: 11px 16px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Dynamic prompt with a template:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of {technology} in {industry}?\"\n",
        "    </span><br>\n",
        "    <span style=\"font-size: 15px;\">If <b>{technology} = \"AI\"</b> and <b>{industry} = \"healthcare\"</b>, the prompt becomes:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of AI in healthcare?\"\n",
        "    </span>\n",
        "  </div>\n",
        "\n",
        "  <p style=\"font-size: 16px; margin-top: 14px;\">üöÄ Let's get started with the first example!</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da3cc1a",
      "metadata": {
        "id": "7da3cc1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4504e527-5526-42fb-a450-d60c93ae1f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Generated Prompt: What are the benefits of Drones in SupplyChain in 1 sentence?\n",
            "üîπ LLM Response: Drones in supply chain improve efficiency, reduce delivery time and costs, and enhance accessibility in difficult terrains.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# üéØ Using Prompt Templates with OpenAI (GPT-4)\n",
        "# ==================================================\n",
        "\"\"\"\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI   # ‚úÖ NEW import for ChatOpenAI\n",
        "\n",
        "# Step 1: Define a prompt template with variables\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],\n",
        "    template=\"What are the benefits of {technology} in {industry} in 1 sentence?\"\n",
        ")\n",
        "\n",
        "# Step 2: Format the prompt with specific values\n",
        "formatted_prompt = prompt_template.format(technology=\"Drones\", industry=\"SupplyChain\")\n",
        "\n",
        "# Step 3: Initialize the OpenAI LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Step 4: Generate the response\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "# Step 5: Display results\n",
        "print(\"üîπ Generated Prompt:\", formatted_prompt)\n",
        "print(\"üîπ LLM Response:\", response_ChatGPT.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbb5ec1",
      "metadata": {
        "id": "6cbb5ec1"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# ‚úã **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# ==================================================\n",
        "\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2Ô∏è‚É£ Ensure the Prompt Template correctly replaces {topic} and {context}.\n",
        "# 3Ô∏è‚É£ Run the code and verify GPT-4 generates a response.\n",
        "\n",
        "# ‚úÖ Step 1: Define a Prompt Template\n",
        "prompt_template = -----(\n",
        "    input_variables=[\"-----\", \"context\"],  # Fill in the missing variable name\n",
        "    template=\"How does {topic} impact {context} in a few words?\"  # Structure of the prompt\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 2: Format the prompt with actual values\n",
        "formatted_prompt = prompt_template.----- (topic=\"Machine Learning\", -----=\"business analytics\")\n",
        "\n",
        "# ‚úÖ Step 3: Generate a response using OpenAI (GPT-4)\n",
        "llm_ChatGPT = llm.----- (model_name=\"gpt-4\")  # Initialize the ChatGPT model\n",
        "response_ChatGPT = llm_ChatGPT.invoke(-----)  # Fill in the correct variable for invoke\n",
        "\n",
        "# ‚úÖ Step 4: Display results\n",
        "print(\"üîπ **Generated Prompt:**\", formatted_prompt)\n",
        "print(\"üîπ **LLM Response:**\", response_ChatGPT.-----)  # Extract response content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# üîÑ Using Prompt Templates in a Loop\n",
        "# ==================================================\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Step 1Ô∏è‚É£: Define a prompt template with variables\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],\n",
        "    template=\"In one sentence, how does {technology} impact {industry} in 1 sentence?\"\n",
        ")\n",
        "\n",
        "# Step 2Ô∏è‚É£: Initialize the OpenAI LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Step 3Ô∏è‚É£: Define input values for the loop\n",
        "input_data = [\n",
        "    {\"technology\": \"AI\", \"industry\": \"education\"},\n",
        "    {\"technology\": \"Blockchain\", \"industry\": \"finance\"},\n",
        "    {\"technology\": \"5G\", \"industry\": \"telecommunications\"},\n",
        "]\n",
        "\n",
        "# Step 4Ô∏è‚É£: Loop through inputs, format the prompt, and generate a response\n",
        "for data in input_data:\n",
        "    formatted_prompt = prompt_template.format(**data)\n",
        "    response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "    # Step 5Ô∏è‚É£: Display results in a clear, modern format\n",
        "    print(f\"üîπ Prompt: {formatted_prompt}\")\n",
        "    print(f\"üí° Response: {response_ChatGPT.content}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "MQE2mEke-6_w"
      },
      "id": "MQE2mEke-6_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #1a386e 0%, #377ce8 100%); color: white; padding: 24px 26px 16px 26px; border-radius: 12px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h3 style=\"margin-top: 0; font-size: 21px;\">üîó Wrapping Up: Why Prompt Templates Matter</h3>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    Just as you wouldn't rewrite a whole menu for every customer in a coffee shop, you don't need to create a new prompt for every question you ask an LLM.\n",
        "    <br><br>\n",
        "    <b>Prompt templates</b> give you a reusable, flexible, and structured way to interact with language models‚Äîmaking your code cleaner, your queries more consistent, and your applications easier to scale.\n",
        "    <br><br>\n",
        "    Whether you're building a chatbot, automating business tasks, or analyzing data, prompt templates are an essential tool in your GenAI toolkit!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "B9jXv_aeFukI"
      },
      "id": "B9jXv_aeFukI"
    },
    {
      "cell_type": "markdown",
      "id": "3c9bdf18",
      "metadata": {
        "id": "3c9bdf18"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 27px; letter-spacing: 0.5px;\">\n",
        "    üß† Understanding Memory in LangChain\n",
        "  </h2>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">üîπ What is Memory in LangChain?</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 13px;\">\n",
        "    By default, LLMs <b>do not remember past interactions</b>.<br>\n",
        "    LangChain <b>Memory</b> allows an AI model to <b>retain context</b> across multiple turns, enabling more natural, conversational interactions.\n",
        "  </p>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">üîπ Why Use Memory?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.8;\">\n",
        "    <li>‚úÖ <b>Maintains conversation history</b> ‚Äì AI can recall previous exchanges.</li>\n",
        "    <li>‚úÖ <b>Improves response coherence</b> ‚Äì Reduces redundant user re-explanations.</li>\n",
        "    <li>‚úÖ <b>Essential for chatbots &amp; agents</b> ‚Äì Allows multi-turn dialogue without loss of context.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">üîπ Types of Memory in LangChain</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.7;\">\n",
        "    <li>1Ô∏è‚É£ <b>ConversationBufferMemory</b> ‚Äì Stores messages in a buffer (basic memory).</li>\n",
        "    <li>2Ô∏è‚É£ <b>ConversationSummaryMemory</b> ‚Äì Summarizes past interactions instead of storing all messages.</li>\n",
        "    <li>3Ô∏è‚É£ <b>ConversationBufferWindowMemory</b> ‚Äì Retains only the last N interactions for efficiency.</li>\n",
        "    <li>4Ô∏è‚É£ <b>Vector-based Memory</b> ‚Äì Uses embeddings for advanced retrieval of past conversations.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"font-size: 17px; margin-bottom: 8px;\">üöÄ What We'll Do in This Lab</h3>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    We‚Äôll start with <b>ConversationBufferMemory</b>, which allows an LLM to <b>recall past messages</b> and interact in a more natural, memory-enhanced way.<br>\n",
        "    <br>\n",
        "    <span style=\"color: #a5d8ff;\">Note:</span> When using a conversation chain with memory, you‚Äôll use the <b><code>predict()</code></b> method instead of <code>invoke()</code>. This lets the AI maintain and use context across multiple turns.<br>\n",
        "    <br>\n",
        "    Let's get started! üëá\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225bb6e6",
      "metadata": {
        "id": "225bb6e6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# üí¨üß† Memory Matters: AnniversaryBot Demo (Stateless vs. Memory)\n",
        "# ==================================================\n",
        "#\n",
        "# This demo shows how LangChain's memory feature allows an AI assistant to remember\n",
        "# details‚Äîusing the example of a user telling the bot their anniversary date.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "\n",
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ Version WITHOUT Memory (Stateless)\n",
        "# -------------------------------\n",
        "llm_stateless = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "print(\"\\n========== WITHOUT MEMORY (Stateless LLM) ==========\")\n",
        "print(\"üë©‚Äç‚ù§Ô∏è‚Äçüë® User: Our anniversary is October 5th. Please remember that!\")\n",
        "response1 = llm_stateless.invoke(\"Our anniversary is October 5th. Please remember that!\")\n",
        "print(\"ü§ñ AnniversaryBot:\", response1.content)\n",
        "\n",
        "print(\"üë©‚Äç‚ù§Ô∏è‚Äçüë® User: When is our anniversary?\")\n",
        "response2 = llm_stateless.invoke(\"When is our anniversary?\")\n",
        "print(\"ü§ñ AnniversaryBot:\", response2.content)\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Version WITH Memory\n",
        "# -------------------------------\n",
        "memory = ConversationBufferMemory()\n",
        "llm_with_mem = ChatOpenAI(model_name=\"gpt-4\")\n",
        "conversation_with_mem = ConversationChain(llm=llm_with_mem, memory=memory)\n",
        "\n",
        "print(\"\\n========== WITH MEMORY ==========\")\n",
        "print(\"üë©‚Äç‚ù§Ô∏è‚Äçüë® User: Our anniversary is October 5th. Please remember that!\")\n",
        "response3 = conversation_with_mem.predict(input=\"Our anniversary is October 5th. Please remember that!\")\n",
        "print(\"ü§ñ AnniversaryBot:\", response3)\n",
        "\n",
        "print(\"üë©‚Äç‚ù§Ô∏è‚Äçüë® User: When is our anniversary?\")\n",
        "response4 = conversation_with_mem.predict(input=\"When is our anniversary?\")\n",
        "print(\"ü§ñ AnniversaryBot:\", response4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc20167",
      "metadata": {
        "id": "9dc20167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "77164a19-cabf-403f-8b88-0bb17aef14f2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (ipython-input-1930312552.py, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1930312552.py\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    llm_ChatGPT = ----- (model_name=\"gpt-4\")  # Initialize ChatGPT model\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# ‚úã Hands-On: Using Memory with OpenAI (Beer Game - Supply Chain Predictions)\n",
        "# ==================================================\n",
        "#\n",
        "# üìå Task Instructions:\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2Ô∏è‚É£ Ensure the AI remembers previous demand data and predicts future order quantities.\n",
        "# 3Ô∏è‚É£ Run the code and check if ChatGPT maintains context for supply chain decisions.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "# ‚úÖ Step 1: Initialize Memory\n",
        "memory = -----()  # Initialize the correct memory class\n",
        "\n",
        "# ‚úÖ Step 2: Initialize ChatGPT with Memory\n",
        "llm_ChatGPT = ----- (model_name=\"gpt-4\")  # Initialize ChatGPT model\n",
        "conversation = ----- (llm=llm_ChatGPT, memory=memory)  # Attach memory to conversation\n",
        "\n",
        "# ‚úÖ Step 3: Run Multiple Interactions\n",
        "print(\"\\nüí¨ **Retailer:** Last week, the customer demand was 200 units. What should I order this week?\")\n",
        "response_ChatGPT = conversation.----- (input=\"Last week, the customer demand was 200 units. What should I order this week?\")  # Call the correct method\n",
        "print(\"ü§ñ **ChatGPT:**\", response_ChatGPT)\n",
        "\n",
        "print(\"\\nüí¨ **Retailer:** If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "response_ChatGPT = conversation.----- (input=\"If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "print(\"ü§ñ **ChatGPT:**\", response_ChatGPT)\n",
        "\n",
        "print(\"\\nüí¨ **Retailer:** What was the demand I mentioned last week?\")\n",
        "response_ChatGPT = conversation.----- (input=\"What was the demand I mentioned last week?\")\n",
        "print(\"ü§ñ **ChatGPT:**\", response_ChatGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using 'Summarized Conversation' Example\n"
      ],
      "metadata": {
        "id": "3ihDbUh4E5Sn"
      },
      "id": "3ihDbUh4E5Sn"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üé§ **Using Memory in LangChain: Job Interview Prep**\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a job interview practice session.\n",
        "# It uses ConversationSummaryMemory to retain key points from previous exchanges\n",
        "# rather than storing the full conversation history.\n",
        "\n",
        "# ‚úÖ Import required classes\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationSummaryMemory  # Summarized conversation memory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "\n",
        "# ‚úÖ Step 1: Initialize Memory\n",
        "# This memory will maintain a **summarized** version of the conversation.\n",
        "memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=\"gpt-4\"))\n",
        "\n",
        "# ‚úÖ Step 2: Initialize ChatGPT with Memory\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")  # Using GPT-4 model\n",
        "\n",
        "# ‚úÖ Step 3: Initialize Conversation Chain\n",
        "# The model will summarize key details from the job interview practice.\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "# ‚úÖ Step 4: Conduct the Interview Simulation\n",
        "\n",
        "print(\"\\nüí¨ **User:** Can you ask me a common interview question?\")\n",
        "response = conversation.predict(input=\"Can you ask me a common interview question?\")\n",
        "print(\"ü§ñ **ChatGPT:**\", response)\n",
        "\n",
        "# ‚úÖ Check memory after first interaction\n",
        "print(\"\\nüìú **Memory Summary After 1st Question:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nüí¨ **User:** My biggest strength is adaptability and problem-solving.\")\n",
        "response = conversation.predict(input=\"My biggest strength is adaptability and problem-solving.\")\n",
        "print(\"ü§ñ **ChatGPT:**\", response)\n",
        "\n",
        "# ‚úÖ Check memory after user shares strength\n",
        "print(\"\\nüìú **Memory Summary After Strength Response:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nüí¨ **User:** My biggest weakness is that I sometimes overthink decisions.\")\n",
        "response = conversation.predict(input=\"My biggest weakness is that I sometimes overthink decisions.\")\n",
        "print(\"ü§ñ **ChatGPT:**\", response)\n",
        "\n",
        "# ‚úÖ Check memory after user shares weakness\n",
        "print(\"\\nüìú **Memory Summary After Weakness Response:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nüí¨ **User:** Can you summarize what we discussed so far?\")\n",
        "response = conversation.predict(input=\"Can you summarize what we discussed so far?\")\n",
        "print(\"ü§ñ **ChatGPT:**\", response)\n",
        "\n",
        "# ‚úÖ Final Memory Check\n",
        "print(\"\\nüìú **Final Memory Summary:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n"
      ],
      "metadata": {
        "id": "PGhKAmB-E3vm"
      },
      "id": "PGhKAmB-E3vm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üç∫ **LangChain Beer Game: Comparing Memory Types\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a Beer Game ordering process over 6 weeks.\n",
        "# It uses:\n",
        "# 1Ô∏è‚É£ ConversationBufferMemory (Tracks full history)\n",
        "# 2Ô∏è‚É£ ConversationBufferWindowMemory (Tracks only last 3 orders)\n",
        "#\n",
        "# The AI predicts the next order quantity based on past interactions.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "import pandas as pd\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# ‚úÖ Step 1: Initialize Memory Types\n",
        "buffer_memory = ConversationBufferMemory(return_messages=True)  # Stores entire history\n",
        "window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)  # Stores last 3 interactions\n",
        "\n",
        "# ‚úÖ Step 2: Initialize Chat Model (NEW!)\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# ‚úÖ Step 3: Define a Prompt Template\n",
        "beer_game_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are managing a supply chain for a beer distribution system.\n",
        "    Orders fluctuate at first but stabilize later.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on past orders, what should be the next order quantity?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# ‚úÖ Step 4: Define Processing Pipelines\n",
        "buffer_chain = beer_game_template | llm\n",
        "window_chain = beer_game_template | llm\n",
        "\n",
        "# ‚úÖ Step 5: Define Order Fluctuations (First 3 weeks volatile, last 3 weeks stable)\n",
        "weekly_orders = [20, 50, 10, 25, 30, 30]  # Example fluctuations\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# ‚úÖ Step 6: Run the Simulation\n",
        "for week in range(1, len(weekly_orders) + 1):\n",
        "    prev_orders = \", \".join(map(str, weekly_orders[:week]))  # Orders seen so far\n",
        "    context = f\"Week {week}: The previous orders were {prev_orders}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "    window_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "\n",
        "    # Get AI predictions using RunnableSequence\n",
        "    buffer_prediction = buffer_chain.invoke({\"context\": context})\n",
        "    window_prediction = window_chain.invoke({\"context\": context})\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# ‚úÖ Step 7: Display Results in a Table\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(weekly_orders) + 1)),\n",
        "    \"Actual Orders\": weekly_orders,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n"
      ],
      "metadata": {
        "id": "uFTXdRyVFUNC"
      },
      "id": "uFTXdRyVFUNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "dRm1ZzStWDLa"
      },
      "id": "dRm1ZzStWDLa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Save the table to an Excel file\n",
        "df.to_excel(\"beer_game_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# ‚úÖ Print confirmation message\n",
        "print(\"Data saved to 'beer_game_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "tlLyE0DYB6Qt"
      },
      "id": "tlLyE0DYB6Qt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå **Assignment: AI Stock Market Trend Prediction with Memory**\n",
        "\n",
        "## **Objective**\n",
        "In this assignment, you will use AI to predict stock market trends based on historical stock prices. You will compare how different memory types affect AI's ability to track and predict future trends.\n",
        "\n",
        "## **Tasks**\n",
        "1. **Initialize memory types** (`ConversationBufferMemory` and `ConversationBufferWindowMemory`).\n",
        "2. **Define the AI model** (GPT-4 or another suitable model).\n",
        "3. **Complete the prompt template** to guide AI predictions.\n",
        "4. **Process stock price data** and use memory to store past trends.\n",
        "5. **Retrieve and analyze stored memory** after each step.\n",
        "6. **Invoke the AI model correctly** to generate predictions.\n",
        "7. **Save results to an Excel file** for analysis.\n",
        "\n",
        "## **Expected Outcome**\n",
        "You will observe how AI predictions change when it has full history vs. limited memory. This will help you understand the impact of memory in AI-based forecasting.\n",
        "\n",
        "üöÄ **Complete the placeholders and run the script to generate insights!** üöÄ\n"
      ],
      "metadata": {
        "id": "r9Y518a4MeVJ"
      },
      "id": "r9Y518a4MeVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚úã **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# üìà **AI Assignment: Stock Market Trend Prediction with Memory**\n",
        "# ==================================================\n",
        "#\n",
        "# üîπ In this assignment, you will use AI to predict stock market trends.\n",
        "# üîπ You will compare how different memory types affect AI's ability to track stock price movements.\n",
        "# üîπ Complete the placeholders (----) to make the script functional.\n",
        "#\n",
        "# üìå **Your Tasks:**\n",
        "# 1Ô∏è‚É£ Initialize the correct memory types.\n",
        "# 2Ô∏è‚É£ Define the AI model.\n",
        "# 3Ô∏è‚É£ Complete the template prompt.\n",
        "# 4Ô∏è‚É£ Use memory correctly when processing stock data.\n",
        "# 5Ô∏è‚É£ Ensure correct invocation of AI for predictions.\n",
        "# 6Ô∏è‚É£ Retrieve and analyze stored memory.\n",
        "# 7Ô∏è‚É£ Save results in an Excel file.\n",
        "\n",
        "# ‚úÖ Import required libraries\n",
        "import pandas as pd\n",
        "from langchain_classic.memory import ConversationBufferMemory, ConversationBufferWindowMemory  # Import appropriate memory classes\n",
        "from langchain_openai import ChatOpenAI  # Import ChatGPT model\n",
        "from langchain_core.prompts import PromptTemplate  # Import PromptTemplate\n",
        "\n",
        "\n",
        "# ‚úÖ Step 1: Initialize Memory Types\n",
        "buffer_memory = ----  # Stores full stock history\n",
        "window_memory = ----  # Stores last 3 stock movements\n",
        "\n",
        "# ‚úÖ Step 2: Initialize Chat Model\n",
        "llm = ----  # Define the AI model (GPT-4 or another model)\n",
        "\n",
        "# ‚úÖ Step 3: Define a Prompt Template\n",
        "stock_prediction_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI financial analyst predicting stock market trends.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on this stock price history, what will be the next trend (Up, Down, or Stable)?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 4: Define Processing Pipelines\n",
        "buffer_chain = ----  # Define how memory connects to AI\n",
        "window_chain = ----  # Define how memory connects to AI with windowed memory\n",
        "\n",
        "# ‚úÖ Step 5: Define Stock Price Data (Fluctuations in the first weeks, then stabilizing)\n",
        "stock_prices = [120, 125, 110, 130, 128, 129]  # Example price movements\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# ‚úÖ Step 6: Run the Prediction Simulation\n",
        "for week in range(1, len(stock_prices) + 1):\n",
        "    prev_prices = \", \".join(map(str, stock_prices[:week]))  # Stocks seen so far\n",
        "    context = f\"Week {week}: The previous stock prices were {prev_prices}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.----  # Store context in buffer memory\n",
        "    window_memory.----  # Store context in windowed memory\n",
        "\n",
        "    # Get AI predictions\n",
        "    buffer_prediction = buffer_chain.----  # Invoke AI for buffer memory\n",
        "    window_prediction = window_chain.----  # Invoke AI for window memory\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# ‚úÖ Step 7: Save Results in an Excel File\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(stock_prices) + 1)),\n",
        "    \"Stock Price\": stock_prices,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n",
        "\n",
        "df.to_excel(\"stock_market_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# ‚úÖ Print confirmation message\n",
        "print(\"Assignment completed! Data saved to 'stock_market_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "TGapDKV0HJWg"
      },
      "id": "TGapDKV0HJWg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}