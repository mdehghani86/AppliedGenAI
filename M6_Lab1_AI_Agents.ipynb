{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/M6_Lab1_AI_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 20px; margin-bottom: 15px; text-align: center; border-radius: 10px;\">\n",
        "<h1 style=\"font-size: 28px; margin-bottom: 8px;\">LangChain: Agents and Chains</h1>\n",
        "<div style=\"background: white; color: #0055d4; padding: 4px 12px; border-radius: 10px; font-size: 13px; display: inline-block; margin-bottom: 8px;\">Prof. Dehghani</div>\n",
        "<p style=\"margin: 0; font-size: 14px;\">m.dehghani@northeastern.edu</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 10px; padding: 15px; margin-bottom: 15px; border: 1px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 20px; padding-bottom: 8px; border-bottom: 2px solid #0055d4;\">Lab Overview</h2>\n",
        "<p style=\"line-height: 1.6; font-size: 15px; margin-bottom: 10px;\">This lab focuses on automating multi-step reasoning and decision-making in LangChain using <strong>Chains & Agents</strong>. You'll learn how to connect multiple components, dynamically execute logic, and use LLMs to make decisions.</p>\n",
        "\n",
        "<div style=\"margin-top: 15px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h3 style=\"color: #0055d4; font-size: 16px; margin-bottom: 10px;\">Learning Objectives</h3>\n",
        "<div style=\"background: white; border-radius: 8px; padding: 12px; margin-bottom: 8px; border: 1px solid #e0e0e0;\">\n",
        "<span style=\"color: #0055d4; font-weight: bold;\">1.</span> Chains in LangChain â€” Connect multiple LLM calls in a sequence\n",
        "</div>\n",
        "<div style=\"background: white; border-radius: 8px; padding: 12px; margin-bottom: 8px; border: 1px solid #e0e0e0;\">\n",
        "<span style=\"color: #0055d4; font-weight: bold;\">2.</span> Agents & Tools â€” Create AI-driven agents that reason & act dynamically\n",
        "</div>\n",
        "<div style=\"background: white; border-radius: 8px; padding: 12px; margin-bottom: 8px; border: 1px solid #e0e0e0;\">\n",
        "<span style=\"color: #0055d4; font-weight: bold;\">3.</span> Hands-on Implementation â€” Apply concepts through practical coding exercises\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<p style=\"margin-top: 12px; font-size: 14px; color: #666;\">Upon completion, you'll be equipped to build AI workflows using structured pipelines and autonomous decision-making.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d663e9-5452-4893-8af4-ca59197cd7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/63.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/438.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m430.1/438.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.3/438.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# âš™ï¸ Installing Required Libraries (Quiet Mode)\n",
        "# ==================================================\n",
        "!pip install -q --upgrade langchain  # Core framework for LLMs\n",
        "!pip install -q --upgrade langchain-community  # Community LLMs, tools, memory, etc.\n",
        "!pip install -q --upgrade openai  # OpenAI API (always use latest unless you have a reason to pin)\n",
        "!pip install -q --upgrade langchain-openai # Install the OpenAI integration for LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32324c8-1e8b-43af-8386-093e25443457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Essential libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# ğŸ Importing Essential Python Libraries for LangChain + OpenAI\n",
        "# ==================================================\n",
        "\n",
        "import os  # System: Manage environment variables (e.g., API keys)\n",
        "\n",
        "import ipywidgets as widgets  # Jupyter: For interactive input controls in notebooks\n",
        "from IPython.display import clear_output, display  # Jupyter: Output management and display tools\n",
        "\n",
        "from langchain_openai import ChatOpenAI  # LangChain: OpenAI chat model wrapper\n",
        "from langchain.memory import ConversationBufferMemory  # LangChain: Stores conversational history for chatbots\n",
        "from langchain.prompts import PromptTemplate  # LangChain: Create prompt templates for LLMs\n",
        "\n",
        "print(\"âœ… Essential libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d4e47a7",
      "metadata": {
        "id": "2d4e47a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "1cf60141f87f4e148b2d776a930ed63b",
            "774358387c6d4866b01b9d86d507c22c",
            "9d17791427664c4d91452a25353c92af",
            "448a270731b84cb8bbafb6333f1cde3e",
            "31637c3db0f64a40a144fa435ab6299a",
            "a7ac7c6cb70a4841a15b51e783d485d5"
          ]
        },
        "outputId": "fd114a8c-30c0-4cbc-8a96-23dd4900592a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='ğŸ”‘ OpenAI Key:', placeholder='Enter your OpenAI API Key')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cf60141f87f4e148b2d776a930ed63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='âœ… Set API Key', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "448a270731b84cb8bbafb6333f1cde3e"
            }
          },
          "metadata": {}
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"margin-bottom: 25px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #e0e0e0;\">Understanding Chains in LangChain</h2>\n",
        "\n",
        "<p style=\"line-height: 1.6; font-size: 16px; color: #666;\">One of the core features of LangChain is its ability to <strong>create chains</strong>, allowing us to sequence multiple tasks together. Instead of manually handling each step, chains automate workflows by linking components such as prompts, LLMs, memory, and tools.</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f8f9fa; border-radius: 10px; padding: 20px; margin-bottom: 20px;\">\n",
        "<h3 style=\"color: #0055d4; margin-top: 0; font-size: 18px; margin-bottom: 15px;\">Why Use Chains in LangChain?</h3>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "<strong style=\"color: #0055d4;\">Automate multi-step processes</strong> â€” No need to manually pass outputs between steps\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "<strong style=\"color: #0055d4;\">Create structured AI pipelines</strong> â€” Chain together LLMs, retrievers, memory, and tools\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "<strong style=\"color: #0055d4;\">Enable decision-making AI agents</strong> â€” Chains help LLMs interact with external tools to retrieve and process information dynamically\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-bottom: 25px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 22px; margin-bottom: 15px;\">Using the Pipe (|) Operator for Cleaner Chaining</h2>\n",
        "\n",
        "<p style=\"line-height: 1.6; font-size: 16px; color: #666; margin-bottom: 15px;\">LangChain provides a simplified way to create chains using the <strong>pipe (|) operator</strong>, which allows direct data flow between components.</p>\n",
        "\n",
        "<h3 style=\"color: #0055d4; font-size: 18px; margin-bottom: 10px;\">Example: Two Ways to Process Input with an LLM</h3>\n",
        "\n",
        "<div style=\"display: flex; gap: 10px; margin-bottom: 15px;\">\n",
        "<div style=\"flex: 1; background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">Without Pipe (|)</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">Manually format the input, then pass it to the LLM</span>\n",
        "</div>\n",
        "\n",
        "<div style=\"flex: 1; background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">With Pipe (|)</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">Directly chain them together for automatic execution</span>\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<p style=\"font-size: 15px; color: #666;\">Let's compare both approaches in the next code cells!</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "F7lwDEVbbG1v"
      },
      "id": "F7lwDEVbbG1v"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ğŸ”¹ **Comparing Two Methods: Manual Execution vs. Pipe (`|`) Operator**\n",
        "# ==================================================\n",
        "# This example demonstrates two ways to process an input with an LLM:\n",
        "#\n",
        "# 1ï¸âƒ£ **Without Pipe (`|`)** â†’ Manually format the prompt and pass it to the LLM.\n",
        "# 2ï¸âƒ£ **With Pipe (`|`)** â†’ Use LangChainâ€™s `|` operator to create a streamlined sequence.\n",
        "#\n",
        "# Using the `|` operator allows for **cleaner, automatic execution**, reducing code complexity.\n",
        "\n",
        "\n",
        "# âœ… Define a prompt template with a World Cup theme\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"event\"],\n",
        "    template=\"\"\"\n",
        "    You are a legendary sports analyst with deep knowledge of World Cup history.\n",
        "    Fans eagerly await your expert take on the most iconic moments.\n",
        "\n",
        "    Analyze this legendary World Cup event in max ~20 simple words: {event}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# âœ… Initialize the LLM (GPT-4)\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "# ==================================================\n",
        "# âŒ Without Using the Pipe (`|`) - More Manual Steps\n",
        "# ==================================================\n",
        "# 1ï¸âƒ£ Manually format the prompt\n",
        "formatted_prompt = prompt.format(event=\"Zidane's 2006 World Cup final red card\")\n",
        "\n",
        "# 2ï¸âƒ£ Pass it to the LLM manually (Fixed: Removed unnecessary dictionary)\n",
        "response_without_pipe = llm.invoke(formatted_prompt)\n",
        "\n",
        "# âœ… Print the response\n",
        "print(\"âŒ Without Pipe Response:\", response_without_pipe.content)\n",
        "\n",
        "# ==================================================\n",
        "# âœ… Using the Pipe (`|`) - Cleaner and Automatic Execution\n",
        "# ==================================================\n",
        "# 1ï¸âƒ£ Directly chain the prompt and LLM together\n",
        "chain = prompt | llm  # This creates a RunnableSequence\n",
        "\n",
        "# 2ï¸âƒ£ Run the chain with an input in one step (Fixed: Using correct format)\n",
        "response_with_pipe = chain.invoke({\"event\": \"Zidane's 2006 World Cup final red card\"})\n",
        "\n",
        "# âœ… Print the response\n",
        "print(\"âœ… With Pipe Response:\", response_with_pipe.content)\n"
      ],
      "metadata": {
        "id": "qboSX5pObOmo"
      },
      "id": "qboSX5pObOmo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ‹ Hands-On: Single Prompt Chain"
      ],
      "metadata": {
        "id": "jS5bBlCKjE4H"
      },
      "id": "jS5bBlCKjE4H"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On 1: Creating a Single Prompt Chain**\n",
        "# ==================================================\n",
        "\n",
        "# ğŸ“Œ **Task Instructions:**\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure the Prompt Template correctly replaces {topic}.\n",
        "# 3ï¸âƒ£ Run the code and verify GPT-4 generates a response.\n",
        "\n",
        "# âœ… Step 1: Import necessary modules\n",
        "from langchain.prompts import -----  # Import the correct class\n",
        "from langchain.chat_models import -----  # Import the correct class\n",
        "\n",
        "# âœ… Step 2: Define a Prompt Template\n",
        "prompt_template = -----(\n",
        "    input_variables=[\"-----\"],  # Placeholder for dynamic input\n",
        "    template=\"Explain {topic} in simple terms using no more than 15 words.\"\n",
        ")\n",
        "\n",
        "# âœ… Step 3: Initialize GPT-4 model\n",
        "llm_ChatGPT = -----(model_name=\"gpt-4\")  # Load GPT-4 model\n",
        "\n",
        "# âœ… Step 4: Create a runnable chain using `|` (pipe operator)\n",
        "chain = prompt_template ----- llm_ChatGPT  # Use the correct operator to chain them\n",
        "\n",
        "# âœ… Step 5: Run the chain with a sample input\n",
        "response = chain.invoke({\"topic\": \"Collective Intelligence\"})\n",
        "\n",
        "# âœ… Step 6: Display results\n",
        "print(\"ğŸ”¹ **Generated Prompt:**\", prompt_template.format(topic=\"Collective Intelligence\"))\n",
        "print(\"ğŸ”¹ **LLM Response:**\", response.-----)  # Extract and display response content\n"
      ],
      "metadata": {
        "id": "3WLVBwwAi6Iu"
      },
      "id": "3WLVBwwAi6Iu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Multi-LLM Pipelines with Chaining  \n",
        "\n",
        "## ğŸ”¹ What is Multi-LLM Chaining?  \n",
        "Multi-LLM chaining is a method where **multiple AI models** collaborate in a **step-by-step sequence** to handle complex tasks efficiently. Instead of a single model doing everything, **each AI is specialized** for a specific function, ensuring **better accuracy, efficiency, and interpretability**.  \n",
        "\n",
        "---\n",
        "\n",
        "## âœ… **Key Benefits of Multi-LLM Chaining**\n",
        "âœ”ï¸ **Task Specialization** â€“ Each model is optimized for a **specific role**, leading to **higher accuracy**.  \n",
        "âœ”ï¸ **Improved Efficiency** â€“ Models **focus on one task at a time**, reducing processing load and response time.  \n",
        "âœ”ï¸ **Scalability** â€“ Easily extendable by adding **more AI models** for deeper analysis.  \n",
        "âœ”ï¸ **Transparency & Interpretability** â€“ Step-by-step outputs **show AI reasoning**, making results easier to trust.  \n",
        "âœ”ï¸ **Error Reduction** â€“ If an early step **detects errors**, later models can **correct them** for a refined output.  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ Real-World Applications of Multi-LLM Chaining\n",
        "ğŸ”¹ **Legal AI:** One model extracts case details, another predicts legal outcomes.  \n",
        "ğŸ”¹ **Customer Support:** One model classifies sentiment, another generates a response.  \n",
        "ğŸ”¹ **Content Writing:** One model creates content, another summarizes or edits it.  \n",
        "ğŸ”¹ **Fake News Detection:** One model extracts claims, another fact-checks them.  \n",
        "\n",
        "By chaining **specialized AI models**, we can **enhance AI reasoning**, improve accuracy, and build more **intelligent, structured workflows** across different industries. ğŸŒŸ  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¥ Example: Medical Diagnosis Chain  \n",
        "In a medical AI pipeline:  \n",
        "1ï¸âƒ£ **GPT-4 Turbo** â†’ Analyzes symptoms and suggests possible conditions.  \n",
        "2ï¸âƒ£ **GPT-4-01** â†’ Evaluates the list and selects the **most likely condition** with reasoning.  \n",
        "\n",
        "This structured approach **breaks down decision-making into logical steps**, leading to **more reliable outputs**.\n"
      ],
      "metadata": {
        "id": "wvyg7LSSgrVi"
      },
      "id": "wvyg7LSSgrVi"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 1: Define Two LLMs\n",
        "# ================================\n",
        "\n",
        "# ğŸ”¹ First LLM (GPT-4 Turbo) - Medical AI: Finds Possible Conditions\n",
        "llm_medical = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)\n",
        "\n",
        "# ğŸ”¹ Second LLM (GPT-4-01) - Reasoning AI: Selects Best Condition & Justifies\n",
        "llm_reasoning = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0.0)\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 2: Create Prompt Templates\n",
        "# ================================\n",
        "\n",
        "# ğŸ”¹ Prompt for Medical AI (Find Possible Conditions)\n",
        "prompt_medical = PromptTemplate(\n",
        "    input_variables=[\"symptoms\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI medical assistant. Based on the symptoms provided, suggest up to 3 possible conditions.\n",
        "\n",
        "    Symptoms: {symptoms}\n",
        "\n",
        "    Respond in a **comma-separated list** (e.g., \"Flu, COVID-19, Pneumonia\").\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ğŸ”¹ Prompt for Reasoning AI (Pick the Most Likely Condition)\n",
        "prompt_reasoning = PromptTemplate(\n",
        "    input_variables=[\"conditions\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI doctor. Based on the possible conditions listed, pick the **most probable** one and provide a **brief reason**.\n",
        "\n",
        "    Possible conditions: {conditions}\n",
        "\n",
        "    Respond with **only the best condition + reasoning in â‰¤15 words**.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 3: Create the Chain (Using `|` Operator)\n",
        "# ================================\n",
        "\n",
        "# ğŸ”¹ Chain Explanation:\n",
        "# 1ï¸âƒ£ GPT-4 Turbo gets symptoms â†’ outputs possible conditions\n",
        "# 2ï¸âƒ£ The conditions are passed automatically to GPT-4-01\n",
        "# 3ï¸âƒ£ GPT-4-01 picks **one most likely condition** & gives a short reason\n",
        "\n",
        "medical_chain = (\n",
        "    prompt_medical  # Step 1: Format symptoms into a medical prompt\n",
        "    | llm_medical   # Step 2: Use GPT-4 Turbo to find possible conditions\n",
        "    | (lambda x: {\"conditions\": x.content})  # Step 3: Extract conditions as input for next LLM\n",
        "    | prompt_reasoning  # Step 4: Format the conditions for reasoning AI\n",
        "    | llm_reasoning  # Step 5: Use GPT-4-01 to pick the best condition\n",
        ")"
      ],
      "metadata": {
        "id": "iIMmsdZFgxXZ"
      },
      "id": "iIMmsdZFgxXZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Test Case 1: Common Cold Symptoms\n",
        "symptoms_input = \"runny nose, sneezing, mild headache\"\n",
        "\n",
        "response = medical_chain.invoke({\"symptoms\": symptoms_input})\n",
        "\n",
        "print(\"ğŸ” Possible Conditions (GPT-4 Turbo):\", response.content)\n"
      ],
      "metadata": {
        "id": "MCeLejCTMqOz"
      },
      "id": "MCeLejCTMqOz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Test Case 2: Severe Flu-like Symptoms\n",
        "symptoms_input = \"fever, chills, muscle pain, fatigue\"\n",
        "\n",
        "response = medical_chain.invoke({\"symptoms\": symptoms_input})\n",
        "\n",
        "print(\"ğŸ” Possible Conditions (GPT-4 Turbo):\", response.content)\n"
      ],
      "metadata": {
        "id": "sip86GtOMwdE"
      },
      "id": "sip86GtOMwdE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Test Case 3: Stomach-related Symptoms\n",
        "symptoms_input = \"nausea, vomiting, diarrhea, stomach cramps\"\n",
        "\n",
        "response = medical_chain.invoke({\"symptoms\": symptoms_input})\n",
        "\n",
        "print(\"ğŸ” Possible Conditions (GPT-4 Turbo):\", response.content)\n"
      ],
      "metadata": {
        "id": "wzJu_UAgMyyC"
      },
      "id": "wzJu_UAgMyyC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ‹ Hands-On Exercise: Customer Review Analysis & Automated Response"
      ],
      "metadata": {
        "id": "dMppSO10jOgf"
      },
      "id": "dMppSO10jOgf"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On: Customer Review Sentiment & AI Response**\n",
        "# ==================================================\n",
        "# ğŸ“Œ **Task Instructions:**\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (`-----`) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure both Prompt Templates correctly embed {review} and {sentiment}.\n",
        "# 3ï¸âƒ£ Run the code and verify the AI-generated sentiment & response.\n",
        "\n",
        "# âœ… Step 1: Import necessary modules\n",
        "\n",
        "# âœ… Step 2: Initialize two different LLMs\n",
        "llm_sentiment = -----(\"gpt-4-turbo\")  # GPT-4 Turbo for sentiment analysis\n",
        "llm_response = -----(\"gpt-4-1106-preview\")  # GPT-4-01 for response generation\n",
        "\n",
        "# âœ… Step 3: Define the first Prompt Template (Sentiment Analysis)\n",
        "sentiment_prompt = -----(\n",
        "    input_variables=[\"-----\"],  # Define the input variable for review input\n",
        "    template=\"\"\"\n",
        "    You are an AI assistant analyzing customer sentiment.\n",
        "\n",
        "    Review: {review}\n",
        "\n",
        "    Respond with either \"Positive\", \"Neutral\", or \"Negative\".\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# âœ… Step 4: Define the second Prompt Template (Automated Response)\n",
        "response_prompt = -----(\n",
        "    input_variables=[\"-----\"],  # Define the input variable for sentiment classification\n",
        "    template=\"\"\"\n",
        "    You are an AI customer support agent. Based on the sentiment, generate a short, polite response.\n",
        "\n",
        "    Sentiment: {sentiment}\n",
        "\n",
        "    Response (â‰¤ 15 words):\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# âœ… Step 5: Create Runnable Chains using the `|` operator\n",
        "sentiment_chain = ----- | -----  # Chain the sentiment prompt with GPT-4 Turbo\n",
        "response_chain = ----- | -----  # Chain the response prompt with GPT-4-01\n",
        "\n",
        "# âœ… Step 6: Run the pipeline\n",
        "\n",
        "# Step 6.1: Sample customer review input\n",
        "customer_review = \"The product is amazing! Great quality and fast delivery. Will buy again.\"\n",
        "\n",
        "# Step 6.2: Analyze sentiment using GPT-4 Turbo\n",
        "sentiment_result = sentiment_chain.-----({\"review\": customer_review})  # Call the function to invoke the model\n",
        "\n",
        "# Step 6.3: Generate AI response using GPT-4-01\n",
        "response_text = response_chain.-----({\"sentiment\": sentiment_result.-----})  # Ensure correct content extraction\n",
        "\n",
        "# âœ… Step 7: Display results\n",
        "print(\"ğŸ” **Detected Sentiment:**\", sentiment_result.-----)  # Extract response content\n",
        "print(\"\\nğŸ’¬ **AI Response:**\", response_text.-----)  # Extract AI-generated response\n",
        "\n",
        "\n",
        "#+++++++ Example Output +++++++#\n",
        "#ğŸ” **Detected Sentiment:** Positive\n",
        "#ğŸ’¬ **AI Response:** Thank you for your kind words! Weâ€™re glad you loved it! ğŸ˜Š\n"
      ],
      "metadata": {
        "id": "WV19WC9ujSna"
      },
      "id": "WV19WC9ujSna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ‹ Hands-On Exercise: Customer Review Analysis & Automated Response (Merged Chain)"
      ],
      "metadata": {
        "id": "szagChUdO841"
      },
      "id": "szagChUdO841"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On: Merged Chain for Customer Review Sentiment & AI Response**\n",
        "# ==================================================\n",
        "# ğŸ“Œ **Task Instructions:**\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (`-----`) to complete the chain.\n",
        "# 2ï¸âƒ£ Ensure sentiment analysis flows correctly into the response generation.\n",
        "# 3ï¸âƒ£ Run the code and verify AI-generated responses.\n",
        "\n",
        "# âœ… The `PromptTemplate` and LLMs (`llm_sentiment` and `llm_response`) are already defined.\n",
        "\n",
        "# âœ… Step 5: Create a Single Runnable Chain (Merged Approach)\n",
        "full_chain = (\n",
        "    -----  # Step 1: Start with the sentiment prompt\n",
        "    | -----  # Step 2: Pass it to the sentiment analysis LLM\n",
        "    | (lambda x: {\"sentiment\": x.content})  # Step 3: Extract sentiment result\n",
        "    | -----  # Step 4: Format sentiment into response prompt\n",
        "    | -----  # Step 5: Pass it to the response generation LLM\n",
        ")\n",
        "\n",
        "# âœ… Step 6: Run the pipeline\n",
        "\n",
        "# Sample customer review input\n",
        "customer_review = \"The product is amazing! Great quality and fast delivery. Will buy again.\"\n",
        "\n",
        "# Run the full pipeline in one step\n",
        "response = full_chain.invoke({\"review\": customer_review})\n",
        "\n",
        "# âœ… Step 7: Display results\n",
        "print(\"ğŸ’¬ **AI Response:**\", response.content)  # Extract AI-generated response\n"
      ],
      "metadata": {
        "id": "RwRSZ76pO5JH"
      },
      "id": "RwRSZ76pO5JH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– AI Agents: Beyond Basic LLM Chaining  \n",
        "\n",
        "## ğŸ”¹ What are AI Agents?  \n",
        "Unlike simple LLM pipelines where models work in a **fixed sequence**, **AI agents** are **more flexible** and can **make decisions dynamically**. They can interact with **external tools, APIs, memory, and reasoning frameworks** to **adapt their responses** based on the situation.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” **LLM Chaining vs. AI Agents**\n",
        "| Feature           | LLM Chaining                     | AI Agents                      |\n",
        "|------------------|--------------------------------|--------------------------------|\n",
        "| **Execution**    | Fixed sequence of steps        | Dynamic, adaptive behavior    |\n",
        "| **Decision-Making** | Follows predefined logic       | Can reason and choose actions |\n",
        "| **Interactivity** | Limited to internal logic      | Can use APIs, databases, tools |\n",
        "| **Memory**       | No long-term state             | Can remember previous actions |\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… **Why Use AI Agents?**\n",
        "âœ”ï¸ **Decision-Making** â€“ Agents **select actions** dynamically instead of just following a script.  \n",
        "âœ”ï¸ **Tool Integration** â€“ Can access **APIs, databases, and external tools** like search engines or calculators.  \n",
        "âœ”ï¸ **Memory** â€“ Stores past interactions to **improve responses over time**.  \n",
        "âœ”ï¸ **Multi-Step Reasoning** â€“ Can **plan**, execute, and refine responses based on feedback.  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ Real-World Applications of AI Agents\n",
        "ğŸ”¹ **Customer Support AI:** Detects user needs, queries databases, and provides **real-time support**.  \n",
        "ğŸ”¹ **Financial AI:** Analyzes market trends, retrieves **live stock prices**, and recommends investments.  \n",
        "ğŸ”¹ **Research Assistants:** Searches the web, **extracts insights**, and summarizes articles.  \n",
        "ğŸ”¹ **Automated Workflow Agents:** Interact with **multiple APIs** to execute complex business tasks.  \n",
        "\n",
        "---\n",
        "\n",
        "## âš¡ Next Steps: Building an AI Agent  \n",
        "Now, letâ€™s **create an AI agent** that can **reason, interact with tools, and make decisions** dynamically! ğŸš€  \n"
      ],
      "metadata": {
        "id": "sYopzX5PSmNR"
      },
      "id": "sYopzX5PSmNR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§® Lab: Building a Math Solver Agent  \n",
        "\n",
        "In this lab, we create an **AI agent** that can solve **math problems dynamically** using a **calculator tool**. The agent **decides** whether to perform calculations itself or use external tools, showcasing **reasoning and tool integration** in LangChain.  \n",
        "\n",
        "The agent is executed using **`.run()`**, which allows it to process user queries and decide on actions dynamically.  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¹ What is a Tool in LangChain?  \n",
        "A **tool** in LangChain is an **external function** that an agent can call to perform **specialized tasks** beyond just text generation. Tools help **extend AI capabilities**, allowing it to:  \n",
        "âœ”ï¸ **Perform calculations** (e.g., a calculator tool)  \n",
        "âœ”ï¸ **Query APIs** (e.g., fetch stock prices, weather updates)  \n",
        "âœ”ï¸ **Search databases** (e.g., retrieve company records)  \n",
        "\n",
        "In this lab, we use a **calculator tool** to enable precise **math computation**, ensuring accurate results instead of relying solely on an LLMâ€™s reasoning.  "
      ],
      "metadata": {
        "id": "s-JOOTTym4Iu"
      },
      "id": "s-JOOTTym4Iu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ—ï¸ Build the Math Solver Agent\n",
        "# ================================\n",
        "# âœ… This cell initializes the agent with a reasoning LLM and a calculator tool.\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI  # This imports the OpenAI-powered chatbot model for conversational AI\n",
        "from langchain.agents import initialize_agent, AgentType  # This is used to initialize an agent with a specific type\n",
        "from langchain.tools import Tool  # Tool class is used to define custom tools that the agent can use\n",
        "import operator  # Importing the operator module to perform mathematical and logical operations\n",
        "\n",
        "# âœ… Step 1: Define the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)\n",
        "\n",
        "import operator  # Importing the operator module for mathematical operations\n",
        "\n",
        "# âœ… Step 2: Create a Calculator Tool\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Securely evaluates a mathematical expression and returns the computed result.\n",
        "\n",
        "    This function allows basic arithmetic operations while restricting access to\n",
        "    potentially dangerous built-in functions.\n",
        "\n",
        "    Supported operations:Addition (+), Subtraction (-), Multiplication (*)\n",
        "    - Division (/), Exponentiation (**), Floor Division (//), Modulus (%)\n",
        "\n",
        "    Parameters:\n",
        "    expression (str): A valid mathematical expression in string format\n",
        "                      (e.g., \"5 + 3\", \"10 * 2\", \"8 ** 2\").\n",
        "\n",
        "    Returns:\n",
        "    str: The computed result as a string, or an error message if the input is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # âœ… Securely evaluate the mathematical expression\n",
        "        # - `eval()` computes the arithmetic operation safely.\n",
        "        # - `{\"__builtins__\": {}}` removes access to all built-in functions, preventing security risks.\n",
        "        # - `operator.__dict__` limits the allowed operations to those defined in the operator module.\n",
        "        result = eval(expression, {\"__builtins__\": {}}, operator.__dict__)\n",
        "\n",
        "        # âœ… Convert the result to a string before returning it\n",
        "        return str(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        # âœ… Handle errors gracefully, such as:\n",
        "        # - Invalid mathematical expressions (e.g., \"five plus three\" instead of \"5 + 3\")\n",
        "        # - Unsupported operations\n",
        "        # - Division by zero\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "calculator = Tool(\n",
        "    name=\"Calculator\",\n",
        "    func=calculator_tool,\n",
        "    description=\"Use this tool to perform basic arithmetic calculations.\"\n",
        ")\n",
        "\n",
        "# âœ… Step 3: Initialize the Math Solver Agent\n",
        "math_solver_agent = initialize_agent(\n",
        "    tools=[calculator],\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "hlWQ9rtoSnLc"
      },
      "id": "hlWQ9rtoSnLc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”¹ Why Do We Need an LLM for the Math Solver Agent?  \n",
        "\n",
        "The **LLM (GPT-4 Turbo)** is essential for enabling **natural language understanding and reasoning** in the **Math Solver Agent**.  \n",
        "\n",
        "### **âœ… Why Keep the LLM?**  \n",
        "âœ” Interprets **human-like queries** (e.g., `\"Multiply 10 by 3\"` â†’ `\"10 * 3\"`)  \n",
        "âœ” Decides **when to use the calculator tool**  \n",
        "âœ” Provides **error handling and explanations** in natural language  \n",
        "âœ” Responds to **unsupported queries** instead of failing  \n",
        "\n",
        "### **âŒ What Happens Without It?**  \n",
        "ğŸ”¹ Only strict expressions (e.g., `\"5 + 3\"`) work, no **language understanding**  \n",
        "ğŸ”¹ The tool **cannot reason** or decide how to handle a query  \n",
        "ğŸ”¹ The agent **becomes a basic calculator**, losing flexibility  \n",
        "\n",
        "ğŸš€Using the LLM makes the agent **more intelligent, flexible, and user-friendly** beyond just executing math operations.  \n"
      ],
      "metadata": {
        "id": "gaySwvBeiTlY"
      },
      "id": "gaySwvBeiTlY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ§ª Test the Math Solver Agent\n",
        "# ================================\n",
        "# âœ… This cell runs the agent with different math problems.\n",
        "\n",
        "# ğŸ”¹ Test Cases (Increasing Difficulty):\n",
        "test_cases = [\n",
        "    \"What is 42 * (8 + 3)?\",  # Simple arithmetic\n",
        "    \"Solve for x: 3x + 5 = 20.\",  # Equation solving\n",
        "    \"Integrate (3x^2 + 2x - 5) dx.\",  # Advanced calculus (integration)\n",
        "]\n",
        "\n",
        "# ğŸ”¹ Run the agent on each test case one by one\n",
        "for query in test_cases:\n",
        "    response = math_solver_agent.run(query)\n",
        "    print(f\"ğŸ§® Query: {query}\")\n",
        "    print(f\"ğŸ“¢ Response: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "hOe8c9LemiCs"
      },
      "id": "hOe8c9LemiCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¡ Did You Realize? Your Agent is Smarter Than You Think!\n",
        "\n",
        "### â“ Why does the Math Solver Agent understand `\"Multiply 10 by 3\"` even though there is no prompt?  \n",
        "âœ… The agent uses **`ZERO_SHOT_REACT_DESCRIPTION`**, which **automatically prompts the LLM** behind the scenes.  \n",
        "\n",
        "### â“ How does the LLM convert `\"Multiply 10 by 3\"` into `\"10 * 3\"`?  \n",
        "âœ… LangChain **asks the LLM to interpret the query** and map words to mathematical symbols before calling the calculator tool.  \n",
        "\n",
        "### â“ What happens if we remove the LLM?  \n",
        "âœ… The agent **wonâ€™t understand** `\"Multiply 10 by 3\"`, and only exact expressions like `\"10 * 3\"` will work.  \n",
        "\n",
        "### â“ Does the definition of a tool get passed to the LLM?  \n",
        "âœ… Yes! The agent **passes the toolâ€™s description** to the LLM so it knows when and how to use it.  \n",
        "\n",
        "### â“ Can an agent have multiple tools?  \n",
        "âœ… Yes! An agent can use **multiple tools**, and the LLM will decide which one to call based on the query."
      ],
      "metadata": {
        "id": "UXqFE7nxjjlY"
      },
      "id": "UXqFE7nxjjlY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# âœ‹ Hands-On: Build a Unit Conversion Agent\n",
        "# ================================\n",
        "# ğŸ“Œ **Task Instructions:**\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (`-----`) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure the tool and agent are correctly initialized.\n",
        "# 3ï¸âƒ£ Test the agent by providing a conversion query.\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI  # Importing OpenAI-powered LLM\n",
        "from langchain.agents import initialize_agent, AgentType  # For agent initialization\n",
        "from langchain.tools import Tool  # Tool class to define custom tools for the agent\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 1: Define the LLM\n",
        "# ================================\n",
        "# ğŸ”¹ Fill in the placeholder to define the LLM\n",
        "llm = -----(\"gpt-4-turbo\", temperature=0.0)\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 2: Create a Unit Conversion Tool\n",
        "# ================================\n",
        "def unit_conversion_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Handles unit conversions for basic metrics like length, weight, and temperature.\n",
        "\n",
        "    Supported conversions:\n",
        "    - Miles to Kilometers\n",
        "    - Kilograms to Pounds\n",
        "    - Celsius to Fahrenheit\n",
        "\n",
        "    The function extracts the numerical value from the query string, performs the\n",
        "    specified conversion, and returns the result as a formatted string.\n",
        "\n",
        "    Returns:\n",
        "    str: The conversion result or an error message if the query is unsupported or invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if \"miles to kilometers\" in query.lower():\n",
        "            miles = float(query.split()[0])\n",
        "            return f\"{miles} miles is {miles * 1.60934:.2f} kilometers.\"\n",
        "        elif \"kilograms to pounds\" in query.lower():\n",
        "            kg = float(query.split()[0])\n",
        "            return f\"{kg} kilograms is {kg * 2.20462:.2f} pounds.\"\n",
        "        elif \"celsius to fahrenheit\" in query.lower():\n",
        "            celsius = float(query.split()[0])\n",
        "            return f\"{celsius}Â°C is {celsius * 9/5 + 32:.2f}Â°F.\"\n",
        "        else:\n",
        "            return (\n",
        "                \"Unsupported conversion. Supported conversions:\\n\"\n",
        "                \"- Miles to Kilometers\\n\"\n",
        "                \"- Kilograms to Pounds\\n\"\n",
        "                \"- Celsius to Fahrenheit\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        return f\"Error: Unable to process the query. Details: {str(e)}\"\n",
        "\n",
        "\n",
        "# ğŸ”¹ Fill in the placeholder to define the tool\n",
        "unit_converter = Tool(\n",
        "    name=\"Unit Converter\",\n",
        "    func=-----,  # Function to handle unit conversions\n",
        "    description=\"Use this tool to convert units like miles to kilometers, kilograms to pounds, and Celsius to Fahrenheit.\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 3: Initialize the Unit Conversion Agent\n",
        "# ================================\n",
        "# ğŸ”¹ Fill in the placeholder to initialize the agent\n",
        "unit_conversion_agent = initialize_agent(\n",
        "    tools=[-----],  # Tool for unit conversion\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 4: Test the Unit Conversion Agent\n",
        "# ================================\n",
        "# ğŸ”¹ Example query for conversion\n",
        "query = \"5 miles to kilometers\"\n",
        "\n",
        "# ğŸ”¹ Run the agent with the query\n",
        "response = unit_conversion_agent.run(query)\n",
        "\n",
        "# ğŸ”¹ Print the response\n",
        "print(\"ğŸ”„ Unit Conversion Agent Response:\\n\", response)\n"
      ],
      "metadata": {
        "id": "P_YFRx0LxisN"
      },
      "id": "P_YFRx0LxisN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ˆ Stock Market Agent with Simplified Analytics\n",
        "\n",
        "This lab demonstrates how to create a **Stock Market Agent** that retrieves and analyzes AAPL stock data. The agent calculates the **latest close price**, **price changes**, and determines the **trend direction** (upwards or downwards) using historical data. By leveraging LangChain tools and an LLM, the agent provides concise and actionable insights into stock performance.\n"
      ],
      "metadata": {
        "id": "BaXtG9OwxsTp"
      },
      "id": "BaXtG9OwxsTp"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ“Š The MarketMaster Agent: Smart Stock Analytics & Advice\n",
        "# ================================\n",
        "# âœ… This AI-powered stock market agent helps investors by providing:\n",
        "# - ğŸ“ˆ Moving Average Calculation: Detects stock trends over different timeframes.\n",
        "# - ğŸ’¡ Investment Advice: Uses stock price movements to generate buy/sell/hold recommendations.\n",
        "# - ğŸ§  AI Reasoning: Understands queries and determines the best tool to use.\n",
        "#\n",
        "# ğŸš€ **How It Works:**\n",
        "# - The agent reads historical AAPL stock data.\n",
        "# - It calculates key **market insights** such as trend direction and price changes.\n",
        "# - The AI-powered agent **analyzes** stock movement and **suggests potential investment actions**.\n",
        "\n",
        "import pandas as pd  # For handling stock data\n",
        "from langchain.chat_models import ChatOpenAI  # LLM for reasoning and investment advice\n",
        "from langchain.agents import initialize_agent, AgentType  # LangChain agent framework\n",
        "from langchain.tools import Tool  # To register custom tools\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 1: Load AAPL Stock Data\n",
        "# ================================\n",
        "csv_url = \"https://www.dropbox.com/scl/fi/ysqxvj39gx2bkl4husg3y/HistoricalData_1739205046441.csv?rlkey=36q4rjpvvmwt9fyf3fftxwvb3&dl=1\"\n",
        "stock_data = pd.read_csv(csv_url)\n",
        "\n",
        "# ğŸ”¹ Format \"Close/Last\" column (remove \"$\" sign and convert to float)\n",
        "stock_data['Close/Last'] = stock_data['Close/Last'].str.replace('$', '').astype(float)\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 2: Define the Moving Average Tool\n",
        "# ================================\n",
        "def calculate_moving_average(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Computes the moving average for the last N days.\n",
        "\n",
        "    How It Works:\n",
        "    - Extracts the number of days (e.g., \"7-day moving average\").\n",
        "    - Calculates the moving average using historical closing prices.\n",
        "    - Helps determine whether the market is **bullish (uptrend) or bearish (downtrend)**.\n",
        "\n",
        "    Expected Query Format: \"Calculate the 7-day moving average\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract the number of days from the query\n",
        "        days = int(query.split()[2].replace(\"-day\", \"\"))  # Extracts \"7\" from \"7-day moving average\"\n",
        "\n",
        "        # Ensure we have enough data\n",
        "        if len(stock_data) < days:\n",
        "            return f\"Error: Not enough data to calculate a {days}-day moving average.\"\n",
        "\n",
        "        # Compute moving average for the last N days\n",
        "        moving_avg = stock_data['Close/Last'].tail(days).mean()\n",
        "\n",
        "        return f\"The {days}-day moving average for AAPL is ${moving_avg:.2f}.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ğŸ”¹ Register Moving Average Tool\n",
        "moving_avg_tool = Tool(\n",
        "    name=\"Moving Average Calculator\",\n",
        "    func=calculate_moving_average,\n",
        "    description=(\n",
        "        \"Calculates the moving average for the last N days based on AAPL closing prices. \"\n",
        "        \"This indicator is commonly used to detect **bullish (uptrend) or bearish (downtrend) market conditions**.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 3: Define the Investment Advice Tool\n",
        "# ================================\n",
        "def generate_investment_advice(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Provides a buy/sell/hold recommendation based on stock trends.\n",
        "\n",
        "    How It Works:\n",
        "    - Analyzes price trends (upward/downward).\n",
        "    - Checks moving average (short-term vs. long-term trends).\n",
        "    - Uses the LLM to generate a concise investment recommendation.\n",
        "\n",
        "    Expected Query Format: \"Should I invest in AAPL?\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get latest stock data\n",
        "        latest_entry = stock_data.iloc[-1]\n",
        "        previous_entry = stock_data.iloc[-2]\n",
        "\n",
        "        # Calculate price change percentage\n",
        "        price_change = latest_entry['Close/Last'] - previous_entry['Close/Last']\n",
        "        price_change_percent = (price_change / previous_entry['Close/Last']) * 100\n",
        "\n",
        "        # Determine trend direction\n",
        "        trend = \"upward ğŸ“ˆ\" if price_change > 0 else \"downward ğŸ“‰\"\n",
        "\n",
        "        # Generate investment advice prompt\n",
        "        advice_prompt = (\n",
        "            f\"AAPL Stock Analysis:\\n\"\n",
        "            f\"- Latest Close Price: ${latest_entry['Close/Last']}\\n\"\n",
        "            f\"- Price Change: {price_change:.2f} ({price_change_percent:.2f}%)\\n\"\n",
        "            f\"- Trend: {trend}\\n\\n\"\n",
        "            f\"Based on this, provide a brief investment recommendation (buy, sell, or hold).\"\n",
        "        )\n",
        "\n",
        "        # Use LLM to generate investment advice\n",
        "        return llm_investment.predict(advice_prompt)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ğŸ”¹ Register Investment Advice Tool\n",
        "investment_tool = Tool(\n",
        "    name=\"Investment Advice Generator\",\n",
        "    func=generate_investment_advice,\n",
        "    description=\"Analyzes AAPL trends and provides buy/sell/hold investment advice.\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 4: Initialize the MarketMaster Agent\n",
        "# ================================\n",
        "\n",
        "# ğŸ”¹ Define two LLMs for different tasks\n",
        "llm_analysis = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)  # For reasoning & analysis\n",
        "llm_investment = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)  # For investment recommendations\n",
        "\n",
        "# ğŸ”¹ Initialize the agent with both tools\n",
        "marketmaster_agent = initialize_agent(\n",
        "    tools=[moving_avg_tool, investment_tool],  # Includes both tools\n",
        "    llm=llm_analysis,  # LLM for reasoning\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Allows decision-making\n",
        "    verbose=True  # Logs agent reasoning\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 5: Run the MarketMaster Agent\n",
        "# ================================\n",
        "\n",
        "# ğŸ”¹ Example Queries\n",
        "query1 = \"Calculate the 7-day moving average for AAPL.\"\n",
        "query2 = \"Should I invest in AAPL?\"\n",
        "\n",
        "# ğŸ”¹ Run the agent with both queries\n",
        "response1 = marketmaster_agent.run(query1)\n",
        "response2 = marketmaster_agent.run(query2)\n",
        "\n",
        "# ğŸ”¹ Print Responses\n",
        "print(\"ğŸ“Š Moving Average Response:\\n\", response1)\n",
        "print(\"\\nğŸ’¡ Investment Advice Response:\\n\", response2)\n"
      ],
      "metadata": {
        "id": "-VXDJT4zoSSD"
      },
      "id": "-VXDJT4zoSSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ“ˆ Chaining Analytics and Graphing Agents\n",
        "# ================================\n",
        "# âœ… This cell demonstrates chaining two agents: one for analytics and another for graphing.\n",
        "\n",
        "import matplotlib.pyplot as plt  # For graphing stock price trends\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 1: Define the Graphing Tool\n",
        "# ================================\n",
        "def generate_stock_graph(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a stock price trend graph for AAPL over the last 10 days.\n",
        "    \"\"\"\n",
        "    # Extract the last 10 days of data\n",
        "    recent_data = stock_data.tail(10)\n",
        "    dates = recent_data['Date']\n",
        "    close_prices = recent_data['Close/Last']\n",
        "\n",
        "    # Create the graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(dates, close_prices, marker='o', linestyle='-', color='blue')\n",
        "    plt.title(\"AAPL Stock Price Trend (Last 10 Days)\", fontsize=14)\n",
        "    plt.xlabel(\"Date\", fontsize=12)\n",
        "    plt.ylabel(\"Close Price ($)\", fontsize=12)\n",
        "    plt.xticks(rotation=45, fontsize=10)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"aapl_stock_trend.png\")  # Save the graph as an image\n",
        "    plt.close()\n",
        "\n",
        "    return \"ğŸ“Š Stock trend graph generated successfully: 'aapl_stock_trend.png'\"\n",
        "\n",
        "# ğŸ”¹ Register the graphing tool\n",
        "graph_tool = Tool(\n",
        "    name=\"Graph Generator\",\n",
        "    func=generate_stock_graph,\n",
        "    description=\"Generates a graph of AAPL's stock price trend over the last 10 days.\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# âœ… Step 2: Chain the Agents\n",
        "# ================================\n",
        "\n",
        "# ğŸ”¹ Analytics Agent (from previous step)\n",
        "analytics_agent = stock_agent  # This uses the existing analytics agent\n",
        "\n",
        "# ğŸ”¹ Graphing Agent\n",
        "llm_graph = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)\n",
        "graphing_agent = initialize_agent(\n",
        "    tools=[graph_tool],  # The graphing tool generates the stock graph\n",
        "    llm=llm_graph,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ğŸ”¹ Chain Execution: Run analytics first, then graphing\n",
        "query = \"Provide the latest stock data for AAPL.\"\n",
        "\n",
        "# Step 1: Run the analytics agent\n",
        "analytics_response = analytics_agent.run(query)\n",
        "print(\"ğŸ“ˆ Analytics Agent Response:\\n\", analytics_response)\n",
        "\n",
        "# Step 2: Run the graphing agent\n",
        "graph_response = graphing_agent.run(\"Generate a graph for AAPL stock trend.\")\n",
        "print(\"\\nğŸ“Š Graphing Agent Response:\\n\", graph_response)\n"
      ],
      "metadata": {
        "id": "ZxaXayThu8L8"
      },
      "id": "ZxaXayThu8L8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgCEO_sawwO2"
      },
      "id": "KgCEO_sawwO2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1cf60141f87f4e148b2d776a930ed63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "ğŸ”‘ OpenAI Key:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_774358387c6d4866b01b9d86d507c22c",
            "placeholder": "Enter your OpenAI API Key",
            "style": "IPY_MODEL_9d17791427664c4d91452a25353c92af",
            "value": ""
          }
        },
        "774358387c6d4866b01b9d86d507c22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d17791427664c4d91452a25353c92af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "448a270731b84cb8bbafb6333f1cde3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "âœ… Set API Key",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_31637c3db0f64a40a144fa435ab6299a",
            "style": "IPY_MODEL_a7ac7c6cb70a4841a15b51e783d485d5",
            "tooltip": ""
          }
        },
        "31637c3db0f64a40a144fa435ab6299a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ac7c6cb70a4841a15b51e783d485d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}