{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/AppliedGenAI/blob/main/M8_Lab1_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "<div style=\"background: linear-gradient(90deg, #eef4fc 0%, #ddeafe 100%); border-radius: 16px; box-shadow: 0 6px 24px rgba(0,85,212,0.08); padding: 34px 36px 32px 36px; max-width: 820px; margin: 30px auto 36px auto; font-family: 'Segoe UI', Arial, sans-serif; color: #152033;\">\n",
        "\n",
        "  <h1 style=\"color: #0055d4; font-size: 2.3rem; margin-bottom: 10px; letter-spacing: -1px;\">\n",
        "    üîó LangChain Lab 5: Retrieval-Augmented Generation (RAG)\n",
        "  </h1>\n",
        "  <div style=\"font-size: 1.07rem; color: #0055d4; margin-bottom: 22px;\">\n",
        "    Prof. Dehghani <span style=\"color:#2d3c66; font-size:1rem;\">(m.dehghani@northeastern.edu)</span>\n",
        "  </div>\n",
        "\n",
        "  <h2 style=\"margin-top:18px; color:#1b2e5b;\">üìñ Introduction to RAG</h2>\n",
        "  <h3 style=\"font-weight:600; color:#284ab6; margin-top:8px; font-size:1.12rem;\">\n",
        "    üîπ What is Retrieval-Augmented Generation (RAG)?\n",
        "  </h3>\n",
        "  <div style=\"margin-bottom:12px;\">\n",
        "    <b>Retrieval-Augmented Generation (RAG)</b> is a technique that enhances Large Language Models (LLMs) by retrieving external knowledge before generating a response. Instead of relying solely on a model's pretrained knowledge, RAG fetches relevant <b>documents, database entries, or structured data</b> to improve accuracy.\n",
        "  </div>\n",
        "\n",
        "  <h3 style=\"font-weight:600; color:#284ab6; font-size:1.12rem;\">\n",
        "    üöÄ Why Use RAG?\n",
        "  </h3>\n",
        "  <ul style=\"margin: 0 0 8px 18px; padding: 0;\">\n",
        "    <li><b>‚úÖ Limited Knowledge</b> ‚Äì LLMs can‚Äôt update their training data dynamically.</li>\n",
        "    <li><b>‚úÖ Hallucinations</b> ‚Äì Models sometimes generate incorrect or fabricated information.</li>\n",
        "    <li><b>‚úÖ Domain-Specific Needs</b> ‚Äì For specialized fields like <b>finance, law, or medicine</b>, retrieval ensures better accuracy.</li>\n",
        "  </ul>\n",
        "  <div style=\"margin-bottom:16px;\">\n",
        "    <b>RAG solves these issues by combining retrieval and generation,</b> allowing models to fetch relevant knowledge on demand.\n",
        "  </div>\n",
        "\n",
        "  <h3 style=\"font-weight:600; color:#284ab6; font-size:1.12rem;\">\n",
        "    üõ†Ô∏è How Does RAG Work?\n",
        "  </h3>\n",
        "  <ol style=\"margin: 0 0 12px 25px;\">\n",
        "    <li><b>Retrieval:</b> The system <b>searches for relevant information</b> in a knowledge source (e.g., vector database, documents).</li>\n",
        "    <li><b>Generation:</b> The retrieved information is <b>passed as context</b> to an LLM, which generates a response based on both its knowledge and the retrieved data.</li>\n",
        "  </ol>\n",
        "  <div style=\"margin-bottom:18px;\">\n",
        "    <span style=\"background: #e5f0ff; border-radius: 7px; padding: 4px 10px;\">\n",
        "      üìå <b>Example Use Case:</b> A chatbot answering questions about <b>company policies</b> can use RAG to pull up official policy documents instead of relying only on pre-trained responses.\n",
        "    </span>\n",
        "  </div>\n",
        "\n",
        "  <h3 style=\"font-weight:600; color:#284ab6; font-size:1.12rem;\">\n",
        "    üî¨ Comparison: Traditional LLM vs. RAG\n",
        "  </h3>\n",
        "  <table style=\"width: 97%; background: #f8fbff; border-collapse: collapse; font-size:1rem; box-shadow: 0 1px 6px rgba(44,70,169,0.07); margin-bottom:22px;\">\n",
        "    <tr style=\"background:#d9e6fb;\">\n",
        "      <th style=\"padding:7px 10px; border:1px solid #cee2fa; text-align:left;\">Feature</th>\n",
        "      <th style=\"padding:7px 10px; border:1px solid #cee2fa; text-align:left;\">Traditional LLM</th>\n",
        "      <th style=\"padding:7px 10px; border:1px solid #cee2fa; text-align:left;\">RAG-Enhanced LLM</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Knowledge Source</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Fixed (Training Data)</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Dynamic (Retrieval + LLM)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Updates</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Requires Retraining</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Can Fetch New Information</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Risk of Hallucinations</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">High</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Reduced</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Domain-Specific Adaptability</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Limited</td>\n",
        "      <td style=\"padding:7px 10px; border:1px solid #cee2fa;\">Highly Adaptable</td>\n",
        "    </tr>\n",
        "  </table>\n",
        "\n",
        "  <h3 style=\"font-weight:600; color:#284ab6; font-size:1.12rem;\">\n",
        "    üèóÔ∏è Next Step: Setting Up the Environment\n",
        "  </h3>\n",
        "  <div>\n",
        "    In the next section, we will <b>install required libraries</b> and set up our workspace for building a <b>RAG pipeline in Google Colab</b>.\n",
        "  </div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üìå Installing Required Libraries\n",
        "# ==================================================\n",
        "!pip install langchain langchain-community  # Core LangChain framework & community package\n",
        "!pip install openai==0.28  # OpenAI API package (version 0.28) for GPT models\n",
        "\n",
        "# Additional libraries for RAG (FAISS, ChromaDB, Tokenization, and Unstructured Data Processing)\n",
        "!pip install faiss-cpu chromadb tiktoken unstructured\n",
        "\n",
        "!pip install \"unstructured[pdf]\" pypdf pdfminer.six\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üìå Importing Required Libraries for LangChain RAG Lab\n",
        "# ==================================================\n",
        "\n",
        "# ‚úÖ System & Environment Setup\n",
        "import os  # For setting environment variables, such as API keys\n",
        "\n",
        "# ‚úÖ Jupyter & Colab Utilities\n",
        "import ipywidgets as widgets  # For creating interactive input widgets\n",
        "from IPython.display import clear_output, display  # For managing notebook outputs\n",
        "\n",
        "# ‚úÖ OpenAI API\n",
        "import openai  # Direct interaction with OpenAI API (useful for API-based calls)\n",
        "\n",
        "# ‚úÖ LangChain Core Components\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI chat models (GPT)\n",
        "from langchain.llms import OpenAI  # OpenAI LLM wrapper\n",
        "from langchain.prompts import PromptTemplate  # Structured prompt templates\n",
        "from langchain.memory import ConversationBufferMemory  # Maintaining conversation history\n",
        "\n",
        "# ‚úÖ RAG-Specific LangChain Imports\n",
        "from langchain.vectorstores import FAISS  # FAISS for fast retrieval\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings  # OpenAI embeddings for vector search\n",
        "from langchain.chains import RetrievalQA  # Prebuilt RAG pipeline in LangChain\n",
        "from langchain.document_loaders import TextLoader  # Loading documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splitting text into chunks\n",
        "\n",
        "# ‚úÖ Confirmation message\n",
        "print(\"‚úÖ All required libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4e47a7",
      "metadata": {
        "id": "2d4e47a7"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üîë OpenAI API Key Setup from Colab Secrets\n",
        "# ==================================================\n",
        "\n",
        "# ‚úÖ Retrieve OpenAI API Key from Colab's Secret Storage\n",
        "try:\n",
        "    from google.colab import userdata  # Import Colab's secret storage\n",
        "    openai_key = userdata.get('OpenAI_Key')  # Retrieve key from Colab Secrets\n",
        "\n",
        "    if openai_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        print(\"‚úÖ OpenAI API Key has been set successfully from Colab Secrets!\")\n",
        "    else:\n",
        "        print(\"‚ùå OpenAI API Key not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving OpenAI API Key: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Case Study: OpenAI's Marketing Strategy ‚Äì RAG vs. Non-RAG\n",
        "\n",
        "## üéØ Objective\n",
        "This case study evaluates how **Retrieval-Augmented Generation (RAG)** improves AI-generated responses for a business use case. We analyze OpenAI's **marketing strategy**, first using a standard LLM (**without RAG**) and then incorporating **retrieved external data (RAG)** to enhance the answer.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Approach\n",
        "We structured our experiment in **four key steps**:\n",
        "\n",
        "1. **Non-RAG Query:**  \n",
        "   - Asked OpenAI's GPT model: *\"What is OpenAI's marketing strategy?\"*  \n",
        "   - The model relied **only on its pretrained knowledge**, potentially outdated.  \n",
        "\n",
        "2. **Loading External Knowledge:**  \n",
        "   - Uploaded a **marketing-related PDF from Dropbox** to provide fresh, structured data.  \n",
        "   - Split the document into **smaller retrievable text chunks** using LangChain.  \n",
        "\n",
        "3. **Embedding & Retrieval:**  \n",
        "   - Converted document chunks into **vector embeddings** (FAISS).  \n",
        "   - Set up a **retriever** to fetch relevant context dynamically.  \n",
        "\n",
        "4. **RAG-Based Query:**  \n",
        "   - Asked the same question, but now the model **retrieved** relevant document excerpts.  \n",
        "   - AI generated a **more informed and factually grounded** response.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìä Comparison: Non-RAG vs. RAG Responses\n",
        "\n",
        "| Feature              | ‚ùå **Without RAG** (LLM Only) | ‚úÖ **With RAG** (Retrieved Data) |\n",
        "|----------------------|-----------------------------|----------------------------------|\n",
        "| **Knowledge Source** | Trained model (static)      | External documents (dynamic)    |\n",
        "| **Response Quality** | General & vague            | Specific & data-backed          |\n",
        "| **Up-to-date Info**  | Limited                     | Can retrieve recent data        |\n",
        "| **Risk of Hallucination** | Higher                | Reduced                         |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "O-6iyAWy1t02"
      },
      "id": "O-6iyAWy1t02"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚ùå No RAG: Ask OpenAI About Its Marketing Strategy\n",
        "# ==================================================\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# ‚úÖ Initialize OpenAI Model\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# ‚úÖ Test Query Without RAG\n",
        "query = \"What is OpenAI's marketing strategy in 3 bullets?\"\n",
        "response = llm.invoke(query)  # Corrected to use .invoke()\n",
        "\n",
        "# ‚úÖ Display Result\n",
        "print(\"ü§ñ OpenAI's Marketing Strategy (No RAG):\")\n",
        "print(response.content)  # Corrected to use .content\n"
      ],
      "metadata": {
        "id": "QZewsGvez13I"
      },
      "id": "QZewsGvez13I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "# üìå Comparing `RecursiveCharacterTextSplitter` vs. `CharacterTextSplitter`\n",
        "\n",
        "## üîç Why Use a Text Splitter?\n",
        "Large documents must be broken into smaller, manageable chunks for efficient **retrieval in RAG pipelines**. LangChain provides different text splitters for this purpose.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° `CharacterTextSplitter`\n",
        "**Basic, fast, but limited control.**  \n",
        "‚úÖ Splits text based on a **fixed character limit** (e.g., 500 characters).  \n",
        "‚úÖ Doesn't consider **logical sentence breaks**‚Äîmay split words in half.  \n",
        "‚úÖ **Good for simple text division** without deep structure.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ `RecursiveCharacterTextSplitter`\n",
        "**More advanced & structure-aware.**  \n",
        "‚úÖ Attempts to **split text at logical breakpoints** (e.g., paragraphs, sentences).  \n",
        "‚úÖ Uses a **fallback mechanism**: tries to split by **paragraphs > sentences > words** if possible.  \n",
        "‚úÖ **Better for structured documents** like PDFs, articles, or books.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Summary Table\n",
        "\n",
        "| Feature                        | `CharacterTextSplitter` | `RecursiveCharacterTextSplitter` |\n",
        "|--------------------------------|-------------------------|----------------------------------|\n",
        "| **Splitting Logic**             | Fixed character count   | Tries paragraphs ‚Üí sentences ‚Üí words |\n",
        "| **Maintains Logical Flow?**     | ‚ùå No                   | ‚úÖ Yes |\n",
        "| **Best for PDFs & Long Texts?** | ‚ùå No                   | ‚úÖ Yes |\n",
        "| **Computational Efficiency**    | ‚úÖ Faster               | ‚ö†Ô∏è Slightly slower |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Which One Should You Use?**\n",
        "- **For simple splitting** (e.g., short plain text) ‚Üí Use `CharacterTextSplitter`.  \n",
        "- **For structured documents** (PDFs, articles, books) ‚Üí Use `RecursiveCharacterTextSplitter`.  \n",
        "- **If unsure** ‚Üí Default to `RecursiveCharacterTextSplitter` for better retrieval quality.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "1Hc73ijx_Nrz"
      },
      "id": "1Hc73ijx_Nrz"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üìÇ Download & Load PDF from Dropbox for RAG\n",
        "# ==================================================\n",
        "# This cell fixes the issue with loading PDFs from a Dropbox URL.\n",
        "# Instead of using `UnstructuredURLLoader` (which requires extra dependencies),\n",
        "# we:\n",
        "# ‚úÖ Step 1: Download the PDF from Dropbox and save it locally.\n",
        "# ‚úÖ Step 2: Use `PyPDFLoader` to extract text from the PDF.\n",
        "# ‚úÖ Step 3: Split the extracted text into small chunks for retrieval.\n",
        "# ‚úÖ Step 4: Preview the first few chunks to verify the content.\n",
        "\n",
        "import requests  # Library for downloading files\n",
        "from langchain.document_loaders import PyPDFLoader  # Stable PDF loader for LangChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into smaller parts\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 1: Download the PDF from Dropbox and Save Locally\n",
        "# ==================================================\n",
        "dropbox_url = \"https://www.dropbox.com/scl/fi/wvvef7qxrq36czo4poquc/pdf.pdf?rlkey=yp0sn2f60bjumn7m943hh3o4u&dl=1\"\n",
        "pdf_path = \"/content/document.pdf\"  # Local path to save the downloaded file\n",
        "\n",
        "# Download the file from Dropbox\n",
        "response = requests.get(dropbox_url)\n",
        "with open(pdf_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"‚úÖ PDF Downloaded Successfully!\")\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 2: Load the PDF Using `PyPDFLoader`\n",
        "# ==================================================\n",
        "# PyPDFLoader extracts the text content from the entire PDF document.\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()  # Loads the text into a LangChain-compatible format\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 3: Split Text into Chunks for Better Retrieval\n",
        "# ==================================================\n",
        "# Large text blocks make retrieval inefficient, so we split the text into smaller pieces.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)  # Splits the document into multiple parts\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 4: Preview the First Few Chunks to Ensure Proper Splitting\n",
        "# ==================================================\n",
        "print(f\"‚úÖ Total Chunks: {len(docs)}\")  # Shows how many text chunks were created\n",
        "\n",
        "# Display the first few chunks to verify extraction\n",
        "for i in range(min(3, len(docs))):  # Avoid index error if the document is too short\n",
        "    print(f\"\\nüìú Chunk {i+1}: {docs[i].page_content[:300]}...\")  # Display first 300 chars of each chunk\n"
      ],
      "metadata": {
        "id": "UTtajJuA0FCb"
      },
      "id": "UTtajJuA0FCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Vector Databases & FAISS: Efficient Retrieval in RAG\n",
        "\n",
        "\"\"\"\n",
        "# üîç What Are Vector Databases?\n",
        "Vector databases store and search **high-dimensional embeddings**, allowing AI to find **similar text chunks efficiently**. They are essential for **Retrieval-Augmented Generation (RAG)**, where AI retrieves relevant context before generating responses.\n",
        "\n",
        "## üöÄ Why Use a Vector Database?\n",
        "- üîé **Fast similarity search** for large datasets.\n",
        "- üìñ **Improves accuracy** in AI-generated responses.\n",
        "- ‚ö° **Optimized for large-scale AI applications** (chatbots, search engines, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ FAISS: A Popular Choice\n",
        "FAISS (**Facebook AI Similarity Search**) is an **open-source, fast, and efficient** vector database optimized for **local similarity search**. It‚Äôs widely used for:\n",
        "\n",
        "‚úÖ **Low-latency text retrieval**  \n",
        "‚úÖ **Handling millions of vectors efficiently**  \n",
        "‚úÖ **Offline or on-device AI applications**  \n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Other Vector Database Options\n",
        "| Vector DB  | Best For | Key Features |\n",
        "|------------|---------|--------------|\n",
        "| **FAISS**  | Local, Fast Search | ‚úÖ No server required, efficient indexing |\n",
        "| **ChromaDB** | Simple RAG Pipelines | ‚úÖ Lightweight, native LangChain support |\n",
        "| **Pinecone** | Scalable Cloud Search | ‚úÖ Fully managed, real-time retrieval |\n",
        "| **Weaviate** | Hybrid Search (Text + Metadata) | ‚úÖ Graph-based, AI-powered filtering |\n",
        "\n",
        "### ‚úÖ Choosing the Right One:\n",
        "- **For local, fast retrieval** ‚Üí Use **FAISS**.  \n",
        "- **For easy cloud-based search** ‚Üí Use **Pinecone**.  \n",
        "- **For AI-driven search & metadata filtering** ‚Üí Use **Weaviate**.  \n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "i6sGnvE9AH3_"
      },
      "id": "i6sGnvE9AH3_"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîç Step 3: Convert Text Chunks to Embeddings (FAISS Vector Database)\n",
        "# ==================================================\n",
        "# This step:\n",
        "# ‚úÖ Converts each text chunk into vector embeddings using OpenAI Embeddings.\n",
        "# ‚úÖ Stores the embeddings in FAISS, a fast and efficient vector search database.\n",
        "# ‚úÖ Confirms successful embedding of document chunks.\n",
        "\n",
        "from langchain.vectorstores import FAISS  # FAISS for fast similarity search\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings  # OpenAI's embedding model\n",
        "\n",
        "# ‚úÖ Step 1: Initialize OpenAI Embeddings\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# ‚úÖ Step 2: Convert Text Chunks into Vector Embeddings and Store in FAISS\n",
        "vector_db = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "\n",
        "# ‚úÖ Step 3: Confirm database is ready\n",
        "print(f\"‚úÖ {len(docs)} document chunks embedded successfully!\")\n"
      ],
      "metadata": {
        "id": "6OnpkpGe1NHB"
      },
      "id": "6OnpkpGe1NHB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîç Step 4: Querying the Vector Database (Retrieval + Generation)\n",
        "# ==================================================\n",
        "# This step:\n",
        "# ‚úÖ Creates a retriever to fetch relevant document chunks.\n",
        "# ‚úÖ Uses OpenAI's LLM to generate an answer based on retrieved content.\n",
        "# ‚úÖ Compares RAG-based response with the non-RAG response.\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ‚úÖ Step 1: Create a Retriever (Finds Relevant Chunks)\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "# ‚úÖ Step 2: Create a Retrieval-Augmented Generation (RAG) Chain\n",
        "rag_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "\n",
        "# ‚úÖ Step 3: Ask the same question, but now with RAG retrieval\n",
        "query = \"What is OpenAI's marketing strategy in 3 bullets ?\"\n",
        "response_rag = rag_chain.run(query)\n",
        "\n",
        "# ‚úÖ Step 4: Display RAG-Based Response\n",
        "print(\"\\nüîç RAG-Based Response (With Retrieval):\")\n",
        "print(response_rag)\n"
      ],
      "metadata": {
        "id": "M75QIXhX1X5J"
      },
      "id": "M75QIXhX1X5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Loading External Data: HTML & CSV in LangChain\n",
        "\n",
        "\"\"\"\n",
        "# üîç Loading External Data in LangChain\n",
        "LangChain allows us to **ingest external data** from sources like **HTML (web pages)** and **CSV files (structured data)** for retrieval-based AI applications.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Loading HTML Data (Web Scraping)\n",
        "We can extract text from websites using `HTMLLoader`:\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import HTMLLoader\n",
        "\n",
        "# Load a Wikipedia page (example)\n",
        "loader = HTMLLoader(\"https://en.wikipedia.org/wiki/Renewable_energy\")\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "cTX-PTnGAtwj"
      },
      "id": "cTX-PTnGAtwj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úã**Hands-On: RAG with HTML Data**\n"
      ],
      "metadata": {
        "id": "jS5bBlCKjE4H"
      },
      "id": "jS5bBlCKjE4H"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **üìå Hands-On Task: Load Wikipedia Data on Renewable Energy**\n",
        "```python\n",
        "# ==================================================\n",
        "# ‚úã **Hands-On: Load & Retrieve Renewable Energy Info from Wikipedia**\n",
        "# ==================================================\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (`-----`) to complete the process.\n",
        "# 2Ô∏è‚É£ Use `HTMLLoader` to load Wikipedia data.\n",
        "# 3Ô∏è‚É£ Split text into retrievable chunks.\n",
        "# 4Ô∏è‚É£ Convert chunks into vector embeddings using FAISS.\n",
        "# 5Ô∏è‚É£ Use retrieval to answer a question about renewable energy.\n",
        "\n",
        "from langchain.document_loaders import HTMLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 1: Load Wikipedia Page on Renewable Energy\n",
        "# ==================================================\n",
        "wiki_url = \"https://en.wikipedia.org/wiki/Renewable_energy\"\n",
        "loader = -----  # Load HTML from Wikipedia\n",
        "documents = -----  # Extract text from the page\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 2: Split Text into Chunks\n",
        "# ==================================================\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = -----  # Split extracted text into smaller chunks\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 3: Convert Chunks to Embeddings & Store in FAISS\n",
        "# ==================================================\n",
        "embedding_model = -----  # Use OpenAIEmbeddings or another model\n",
        "vector_db = -----  # Convert docs into vector embeddings and store in FAISS\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 4: Create a Retriever to Fetch Relevant Information\n",
        "# ==================================================\n",
        "retriever = -----  # Convert FAISS vector store into a retriever\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 5: Ask AI a Question About Renewable Energy\n",
        "# ==================================================\n",
        "rag_chain = RetrievalQA.from_chain_type(-----, retriever=retriever)  # Define the RAG pipeline\n",
        "\n",
        "query = \"What are the main types of renewable energy sources?\"\n",
        "response_rag = rag_chain.run(query)\n",
        "\n",
        "# ‚úÖ Step 6: Display Retrieved Answer\n",
        "print(\"\\nüåç üîã AI Answer on Renewable Energy:\")\n",
        "print(response_rag)\n"
      ],
      "metadata": {
        "id": "wMfwf0xUz1Zq"
      },
      "id": "wMfwf0xUz1Zq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üìå Loading CSV Data in LangChain for AI Retrieval\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# üîç Why Load CSV Data in LangChain?\n",
        "CSV files store **structured data** such as financial reports, survey responses, or product catalogs. With LangChain, we can:\n",
        "- üìä **Extract relevant data** for AI analysis.\n",
        "- üîç **Retrieve context-specific insights** using vector embeddings.\n",
        "- ü§ñ **Improve AI-generated responses** by grounding answers in real data.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How to Load CSV Data\n",
        "LangChain provides the `CSVLoader` to extract text from CSV files.\n",
        "\n",
        "### üîπ Example:\n",
        "```python\n",
        "from langchain.document_loaders import CSVLoader\n",
        "\n",
        "# Load a sample CSV file\n",
        "loader = CSVLoader(file_path=\"data.csv\")\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "7TSID1pEPdQV"
      },
      "id": "7TSID1pEPdQV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ‚úã** Hands-On: No RAG vs. RAG with CSV Data**\n",
        "\n"
      ],
      "metadata": {
        "id": "lAD5Qlm9Qb0R"
      },
      "id": "lAD5Qlm9Qb0R"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6VZH8mO8J7ck"
      },
      "id": "6VZH8mO8J7ck"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚úã **Hands-On: AI Insights from CSV Data (No RAG vs. RAG)**\n",
        "# ==================================================\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ First, run the **No RAG version**, where AI generates a response **without external data**.\n",
        "# 2Ô∏è‚É£ Then, fill in the **placeholders (`-----`)** to complete the **RAG version**, which retrieves information from a CSV dataset.\n",
        "# 3Ô∏è‚É£ Use any **structured dataset (e.g., market trends, financial reports, healthcare statistics, etc.)**.\n",
        "# 4Ô∏è‚É£ Compare AI‚Äôs responses **before and after retrieval**.\n",
        "\n",
        "import pandas as pd\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 1: Upload CSV Data in Google Colab\n",
        "# ==================================================\n",
        "# üîπ Instructions:\n",
        "# 1. Click the **folder icon** üìÇ in the left sidebar.\n",
        "# 2. Click the **Upload** button.\n",
        "# 3. Upload your **CSV file** (e.g., `your_dataset.csv`).\n",
        "# 4. Make sure the filename below matches your uploaded file.\n",
        "\n",
        "csv_path = \"/content/your_dataset.csv\"  # Update with your actual file name\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 2: Define the Question (Both Versions)\n",
        "# ==================================================\n",
        "query = \"What are the key insights from the dataset?\"  # Generalized question\n",
        "\n",
        "# ==================================================\n",
        "# ‚ùå No RAG: Ask AI Without External Data\n",
        "# ==================================================\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "response_no_rag = llm.invoke(query)\n",
        "\n",
        "print(\"\\nüöÄ **AI Response Without RAG:**\")\n",
        "print(response_no_rag.content)  # Model response without retrieval\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 3: Load CSV Data for RAG\n",
        "# ==================================================\n",
        "loader = -----  # Use `CSVLoader` to load data\n",
        "documents = -----  # Extract documents from CSV\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 4: Split CSV Data into Chunks for Retrieval\n",
        "# ==================================================\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = -----  # Split CSV data into smaller retrievable chunks\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 5: Convert Chunks to Embeddings & Store in FAISS\n",
        "# ==================================================\n",
        "embedding_model = -----  # Use OpenAIEmbeddings or an alternative\n",
        "vector_db = -----  # Convert docs into vector embeddings and store in FAISS\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 6: Create a Retriever and AI Pipeline for RAG\n",
        "# ==================================================\n",
        "retriever = -----  # Convert FAISS vector store into a retriever\n",
        "rag_chain = RetrievalQA.from_chain_type(-----, retriever=retriever)  # Define the RAG pipeline\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 7: Ask the Same Question Using RAG\n",
        "# ==================================================\n",
        "response_rag = rag_chain.run(query)\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Step 8: Print Model Responses (Compare No RAG vs. RAG)\n",
        "# ==================================================\n",
        "print(\"\\nüöÄ **AI Response Without RAG:**\")\n",
        "print(response_no_rag.content)  # Response without retrieval\n",
        "\n",
        "print(\"\\nüìä **AI Response With RAG (CSV Data Used):**\")\n",
        "print(response_rag)  # Response with retrieved CSV insights\n"
      ],
      "metadata": {
        "id": "3WLVBwwAi6Iu"
      },
      "id": "3WLVBwwAi6Iu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}